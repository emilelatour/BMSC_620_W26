{
  "hash": "2e8132e39253576e36f848d77902336a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Random Variables and Binomial Distribution\"\nsubtitle: \"Textbook Sections 3.1–3.2\"\nauthor: \"Emile Latour, Nicky Wakim, Meike Niederhausen\"\ndate: \"2026-01-26\"\ndate-format: long\nformat:\n  revealjs:\n    theme: \"../../assets/css/reveal-bmsc620_v5.scss\"\n    slide-number: true\n    show-slide-number: all\n    width: 1955\n    height: 1100\n    footer: \"BMSC 620 | Random Variables & Binomial Distribution\"\n    html-math-method: mathjax\n    chalkboard: true\n    header-includes: |\n      <style>\n      #wrap {\n        width: 1650px;\n        height: 900px;\n        margin: 0 auto;\n        overflow: hidden;\n        border: 1px solid #999;\n        border-radius: 8px;\n      }\n      #frame {\n        width: 1650px;\n        height: 900px;\n        border: 0;\n        zoom: 1.25;\n        -moz-transform: scale(1.25);\n        -moz-transform-origin: 0 0;\n      }\n      /* Make selected tables bigger in RevealJS slides (robust to flextable output) */\n      .reveal .tbl-big {\n        font-size: 28px !important;\n        /* center the output block inside the column */\n        display: flex;\n        justify-content: center;\n      \n        /* scale from center */\n        transform: scale(1.6);\n        transform-origin: top center;\n      }\n      .reveal .tbl-big table,\n      .reveal .tbl-big .flextable,\n      .reveal .tbl-big .flextable table {\n        font-size: 28px !important;\n      }\n      .reveal .tbl-big td,\n      .reveal .tbl-big th {\n        padding: 6px 10px !important;\n      }\n      </style>\nexecute:\n  echo: true\n  warning: false\n  message: false\n  freeze: auto\n---\n\n\n\n\n\n## Learning Objectives\n\nBy the end of today's class, you should be able to:\n\n1.  Define random variables and distinguish between discrete and continuous random variables\n2.  Calculate the expected value (mean) and variance of discrete random variables\n3.  Calculate the expected value and variance of linear combinations of random variables\n4.  Identify when the Binomial distribution is appropriate and calculate probabilities using it\n\n## Roadmap for Today\n\n**Part 1: Random Variables (Section 3.1)**\n\n-   What are random variables?\n-   Expected value and variance\n-   Linear combinations of random variables\n\n**Part 2: Binomial Distribution (Section 3.2)**\n\n-   Bernoulli distribution\n-   Binomial distribution\n-   Calculating probabilities with the Binomial\n\n# Part 1: Random Variables\n\nSection 3.1\n\n## Motivation: From Data to Probability Models\n\nSo far we've worked with:\n\n-   **Data points**: $x_1, x_2, x_3, \\ldots, x_n$ (observed values)\n-   **Sample statistics**: mean, variance, standard deviation\n\nNow we're moving to:\n\n-   **Random variables**: mathematical models for uncertain outcomes\n-   **Probability distributions**: theoretical descriptions of random phenomena\n-   **Parameters**: population-level quantities (like $\\mu$ and $\\sigma$)\n\n## What is a Random Variable?\n\n::: {.callout-note icon=\"false\"}\n## Definition: Random Variable\n\nA **random variable (r.v.)** assigns numerical values to the outcomes of a random phenomenon.\n:::\n\n**Notation:**\n\n-   We use capital letters like $X$, $Y$, $Z$ for random variables\n-   We use lowercase letters like $x$, $y$, $z$ for specific values\n\n**Key idea:** A random variable connects random outcomes to numbers and probabilities\n\n## Example: Rolling a Die\n\nSuppose you roll a fair 6-sided die. Let $X$ be the outcome of the roll.\n\n**Question 1:** What is the probability distribution of $X$?\n\n. . .\n\n| $x$      | 1   | 2   | 3   | 4   | 5   | 6   |\n|----------|-----|-----|-----|-----|-----|-----|\n| $P(X=x)$ | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 |\n\n. . .\n\nNote:\n\n-   Each outcome has equal probability (fair die)\n-   Probabilities sum to 1: $\\sum_{x=1}^6 P(X=x) = 1$\n-   All outcomes are disjoint (mutually exclusive)\n\n## Probability Distributions: Quick Review\n\n::: {.callout-tip icon=\"false\"}\n## Rules for a Probability Distribution\n\nA probability distribution must satisfy three rules:\n\n1.  The outcomes must be **disjoint** (mutually exclusive)\n2.  Each probability must be **between 0 and 1**\n3.  The probabilities must **sum to 1**\n:::\n\nThese are the same rules we learned in our probability unit!\n\n## Discrete vs. Continuous Random Variables\n\n::::::: columns\n:::: {.column width=\"48%\"}\n::: {.callout-important icon=\"false\"}\n## Discrete Random Variable\n\nA **discrete r.v.** takes on:\n\n-   A finite number of values, OR\n-   A countably infinite number of values\n\n**Examples:**\n\n-   Number of heads in 10 coin flips\n-   Number of students in a class\n-   Number of COVID cases per day\n:::\n::::\n\n:::: {.column width=\"48%\"}\n::: {.callout-warning icon=\"false\"}\n## Continuous Random Variable\n\nA **continuous r.v.** can take:\n\n-   Any real value in an interval\n-   Any value in a union of intervals\n\n**Examples:**\n\n-   Height\n-   Blood pressure\n-   Time until an event occurs\n:::\n::::\n:::::::\n\n::: fragment\n**Today's focus:** Discrete random variables (continuous coming soon!)\n:::\n\n## Expected Value (Mean)\n\nThe **expected value** of a random variable is its long-run average value.\n\n::: {.callout-note icon=\"false\"}\n## Definition: Expected Value of a Discrete R.V.\n\nIf $X$ takes on outcomes $x_1, \\ldots, x_k$ with probabilities $P(X=x_1), \\ldots, P(X=x_k)$, then:\n\n$$E(X) = \\mu = \\sum_{i=1}^k x_i \\cdot P(X=x_i)$$\n\nThe expected value is a **weighted average** where weights are probabilities.\n:::\n\n**Notation:** $E(X)$ or $\\mu$ (mu)\n\n## Example: Expected Value of Rolling a Die\n\n**Question 2:** What is the expected outcome when rolling a fair die?\n\n. . .\n\n$$\\begin{aligned}\nE(X) &= \\sum_{x=1}^6 x \\cdot P(X=x) \\\\\n&= 1 ( \\frac{1}{6}) + 2(\\frac{1}{6}) + 3(\\frac{1}{6}) + 4(\\frac{1}{6}) + 5(\\frac{1}{6}) + 6 (\\frac{1}{6}) \\\\\n&= \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} \\\\\n&= \\frac{21}{6} = 3.5\n\\end{aligned}$$\n\n. . .\n\n**Interpretation:** If you rolled the die many times, the average would approach 3.5\n\nNote: The expected value doesn't have to be a possible outcome!\n\n## Example: Unfair Die\n\n**Question 3:** What if the die is not fair?\n\n| $x$      | 1    | 2    | 3    | 4    | 5    | 6    |\n|----------|------|------|------|------|------|------|\n| $P(X=x)$ | 1/9 | 1/9 | 1/9 | 1/9 | 2/9 | 3/9 |\n\n. . .\n\n$$\\begin{aligned}\nE(X) &= 1 ( \\frac{1}{9}) + 2(\\frac{1}{9}) + 3(\\frac{1}{9}) + 4(\\frac{1}{9}) + 5(\\frac{2}{9}) + 6 (\\frac{3}{9}) \\\\\n&= 0.11 + 0.22 + 0.33 + 0.33 + 1.11 + 2.0 \\\\\n&= 4.22\n\\end{aligned}$$\n\n. . .\n\nThe die is \"loaded\" toward higher values (mean is 4.22 vs. 3.5 for fair die)\n\n## Variance and Standard Deviation\n\nJust like with data, we measure spread with variance and standard deviation.\n\n::: {.callout-note icon=\"false\"}\n## Definition: Variance of a Discrete R.V.\n\nIf $X$ has expected value $\\mu = E(X)$, then:\n\n$$\\text{Var}(X) = \\sigma^2 = \\sum_{i=1}^k (x_i - \\mu)^2 \\cdot P(X=x_i)$$\n\nThe **standard deviation** is: $SD(X) = \\sigma = \\sqrt{\\text{Var}(X)}$\n:::\n\n**Key idea:** Squared deviations from the mean, weighted by probabilities\n\n## Example: Variance of a Fair Die\n\nWe found $E(X) = 3.5$. What is $\\text{Var}(X)$?\n\n. . .\n\n$$\\begin{aligned}\n\\text{Var}(X) &= \\sum_{x=1}^6 (x - 3.5)^2 \\cdot P(X=x) \\\\\n&= (1-3.5)^2 \\cdot \\frac{1}{6} + (2-3.5)^2 \\cdot \\frac{1}{6} + \\cdots + (6-3.5)^2 \\cdot \\frac{1}{6} \\\\\n&= \\frac{6.25 + 2.25 + 0.25 + 0.25 + 2.25 + 6.25}{6} \\\\\n&= \\frac{17.5}{6} \\approx 2.92\n\\end{aligned}$$\n\n. . .\n\nStandard deviation: $SD(X) = \\sqrt{2.92} \\approx 1.71$\n\n## Pause for a Quick Example\n\n**Apgar scores for newborns**\n\nJust after birth, each child is rated on the Apgar scale, with scores determined by color, heart rate, reflex irritability, muscle tone, and respiratory effort.\n\n\\\n\nLet $X$ = Apgar score. Historical data suggests the following probability distribution:\n\n| $x$      | 0     | 1     | 2     | 3     | 4    | 5    | 6    | 7    | 8    | 9    | 10   |\n|----------|-------|-------|-------|-------|------|------|------|------|------|------|------|\n| $P(X=x)$ | 0.002 | 0.001 | 0.002 | 0.005 | 0.02 | 0.04 | 0.18 | 0.37 | 0.25 | 0.12 | 0.01 |\n\n\\\n\n**Questions:**\n\n1.  What is $E(X)$?\n2.  What is $\\text{Var}(X)$?\n\n## Solution: Apgar Score Example (1/2)\n\n\\\n\n| $x$      | 0     | 1     | 2     | 3     | 4    | 5    | 6    | 7    | 8    | 9    | 10   |\n|----------|-------|-------|-------|-------|------|------|------|------|------|------|------|\n| $P(X=x)$ | 0.002 | 0.001 | 0.002 | 0.005 | 0.02 | 0.04 | 0.18 | 0.37 | 0.25 | 0.12 | 0.01 |\n\n\\\n\n**1. Expected value:**\n\n$$\\begin{aligned}\nE(X) = \\mu &= 0(0.002) + 1(0.001) + 2(0.002) + 3(0.005) \\dots + 8(0.25) + 9(0.12) + 10(0.01) \\\\\n&= 0 + 0.001 + 0.004 + 0.015 + \\dots + 2.00 + 1.08 + 0.10 \\\\\n&= 7.17\n\\end{aligned}$$\n\nThe average Apgar score is about 7.2.\n\n## Solution: Apgar Score Example (2/2)\n\n\\\n\n| $x$      | 0     | 1     | 2     | 3     | 4    | 5    | 6    | 7    | 8    | 9    | 10   |\n|----------|-------|-------|-------|-------|------|------|------|------|------|------|------|\n| $P(X=x)$ | 0.002 | 0.001 | 0.002 | 0.005 | 0.02 | 0.04 | 0.18 | 0.37 | 0.25 | 0.12 | 0.01 |\n\n\\\n\n**2. Variance:**\n\n$$\\begin{aligned}\n\\text{Var}(X) &= \\sum_{x=0}^{10} (x - 7.17)^2 \\cdot P(X=x) \\\\\n&= (0-7.17)^2(0.002) + (1-7.17)^2(0.001) + \\cdots + (10-7.17)^2(0.01) \\\\\n&= 0.103 + 0.038 + 0.053 + 0.087 + 0.201 + 0.188 \\\\\n&\\quad + 0.246 + 0.011 + 0.171 + 0.402 + 0.080 \\\\\n&= 1.58\n\\end{aligned}$$\n\n. . .\n\n**Standard deviation:** $SD(X) = \\sqrt{1.58} \\approx 1.26$\n\n**Interpretation:** Most babies score between 6 and 9 (within 1 SD of the mean).\n\n## Linear Combinations of Random Variables\n\nOften in research, we combine multiple measurements into a composite score.\n\n::: {.callout-note icon=\"false\"}\n## Definition: Linear Combination\n\nIf $X$ and $Y$ are random variables and $a$ and $b$ are constants:\n\n$$aX + bY$$\n\nis a **linear combination** of the random variables.\n:::\n\n**Examples in biomedical research:**\n\n-   Composite health scores (sum of multiple domains)\n-   Weighted risk scores\n-   Total costs (sum of different service types)\n\n## Expected Value of Linear Combinations\n\n::: {.callout-important icon=\"false\"}\n## Theorem: Expected Value of Linear Combinations\n\nIf $X$ and $Y$ are random variables and $a$ and $b$ are constants:\n\n$$E(aX + bY) = aE(X) + bE(Y)$$\n\nand\n\n$$E(aX + b) = aE(X) + b$$\n:::\n\n**Key properties:**\n\n-   Expectation is **linear** - it distributes over addition\n-   Works whether or not $X$ and $Y$ are independent\n-   Constants factor out: $E(cX) = cE(X)$\n\n**This is intuitive:** The average of a sum is the sum of the averages!\n\n## Example: Clinical Trial Health Score (Setup)\n\nIn a clinical trial, researchers measure three patient-reported outcomes (PROs):\n\n-   **Pain score** $(X_1)$: Scale 0-10, where 0 = no pain\n    -   $E(X_1) = 4$, $SD(X_1) = 2$\n-   **Mobility score** $(X_2)$: Scale 0-10, where 10 = full mobility\n    -   $E(X_2) = 6$, $SD(X_2) = 1.5$\n-   **Quality of life score** $(X_3)$: Scale 0-10, where 10 = excellent\n    -   $E(X_3) = 5$, $SD(X_3) = 2.5$\n\nA **composite health score** is calculated as: $$H = X_1 + X_2 + X_3$$\n\nAssume the three scores are **independent** (one doesn't influence the others).\n\n## Example: Expected Composite Score\n\n**Question:** What is the expected composite health score?\n\n. . .\n\n$$\\begin{aligned}\nE(H) &= E(X_1 + X_2 + X_3) \\\\\n&= E(X_1) + E(X_2) + E(X_3) \\\\\n&= 4 + 6 + 5 \\\\\n&= 15\n\\end{aligned}$$\n\n. . .\n\n**Interpretation:** On average, patients have a composite health score of 15 out of 30.\n\n## Variance of Linear Combinations\n\n::: {.callout-important icon=\"false\"}\n## Theorem: Variance of Linear Combinations\n\nIf $X$ and $Y$ are **INDEPENDENT** random variables and $a$ and $b$ are constants:\n\n$$\\text{Var}(aX + bY) = a^2\\text{Var}(X) + b^2\\text{Var}(Y)$$\n:::\n\n**Key differences from expected value:**\n\n-   Requires **independence** of the random variables\n-   Constants are **squared**: $a^2$ and $b^2$ (not just $a$ and $b$)\n-   Variances add, but standard deviations do NOT simply add\n\n**Why squared?** Variance measures squared deviations, so scaling by $a$ scales variance by $a^2$\n\n## Example: Variance of Composite Score\n\n**Question:** What is the variance and standard deviation of the composite health score?\n\nRecall: $H = X_1 + X_2 + X_3$\n\n-   $\\text{Var}(X_1) = 2^2 = 4$\n-   $\\text{Var}(X_2) = 1.5^2 = 2.25$\\\n-   $\\text{Var}(X_3) = 2.5^2 = 6.25$\n\n. . .\n\n$$\\begin{aligned}\n\\text{Var}(H) &= \\text{Var}(X_1 + X_2 + X_3) \\\\\n&= \\text{Var}(X_1) + \\text{Var}(X_2) + \\text{Var}(X_3) \\\\\n&= 4 + 2.25 + 6.25 \\\\\n&= 12.5\n\\end{aligned}$$\n\n. . .\n\n$$SD(H) = \\sqrt{12.5} \\approx 3.54$$\n\n## Example: Weighted Composite Score\n\nNow suppose researchers want to weight the scores differently:\n\n$$H_{\\text{weighted}} = 0.5X_1 + 0.3X_2 + 0.2X_3$$\n\n(Giving more weight to pain and mobility than quality of life)\n\n**Questions:**\n\n1.  What is $E(H_{\\text{weighted}})$?\n2.  What is $\\text{Var}(H_{\\text{weighted}})$?\n\n## Solution: Weighted Composite Score\n\n**1. Expected value:**\n\n$$\\begin{aligned}\nE(H_{\\text{weighted}}) &= E(0.5X_1 + 0.3X_2 + 0.2X_3) \\\\\n&= 0.5E(X_1) + 0.3E(X_2) + 0.2E(X_3) \\\\\n&= 0.5(4) + 0.3(6) + 0.2(5) \\\\\n&= 2 + 1.8 + 1.0 \\\\\n&= 4.8\n\\end{aligned}$$\n\n. . .\n\n**2. Variance:**\n\n$$\\begin{aligned}\n\\text{Var}(H_{\\text{weighted}}) &= (0.5)^2\\text{Var}(X_1) + (0.3)^2\\text{Var}(X_2) + (0.2)^2\\text{Var}(X_3) \\\\\n&= 0.25(4) + 0.09(2.25) + 0.04(6.25) \\\\\n&= 1.0 + 0.20 + 0.25 \\\\\n&= 1.45\n\\end{aligned}$$\n\n$$SD(H_{\\text{weighted}}) = \\sqrt{1.45} \\approx 1.20$$\n\n## Key Takeaways: Linear Combinations\n\n**Expected value:** $$E(aX + bY) = aE(X) + bE(Y)$$\n\n-   Linear property\n-   No independence needed\n-   Constants factor out\n\n**Variance:** $$\\text{Var}(aX + bY) = a^2\\text{Var}(X) + b^2\\text{Var}(Y)$$\n\n-   Requires independence\n-   Constants are squared\n-   Standard deviations don't simply add\n\n\\\n\n**Why this matters:** Many biomedical measures are composite scores!\n\n# Part 2: Binomial Distribution\n\nSection 3.2\n\n## Binary Outcomes\n\nMany situations involve **two possible outcomes**:\n\n-   Flipping a coin: heads or tails\n-   Medical test: positive or negative\\\n-   Surgery outcome: success or failure\n-   Manufacturing: defective or non-defective\n\nWe call these **binary** or **Bernoulli** trials:\n\n-   One outcome is called a \"success\" (probability $p$)\n-   The other is called a \"failure\" (probability $q = 1-p$)\n\n## Bernoulli Distribution\n\n::: {.callout-note icon=\"false\"}\n## Definition: Bernoulli Random Variable\n\nIf $X$ is a random variable that takes:\n\n-   Value 1 with probability $p$ (success)\n-   Value 0 with probability $1-p$ (failure)\n\nThen $X$ is a **Bernoulli random variable**.\n\n**Notation:** $X \\sim \\text{Bernoulli}(p)$ or $X \\sim \\text{Bern}(p)$\n:::\n\n**Parameter:** $p$ is the probability of success (between 0 and 1)\n\n## Bernoulli: Mean and Variance\n\n::: {.callout-important icon=\"false\"}\n## Theorem: Mean and Variance of Bernoulli R.V.\n\nIf $X \\sim \\text{Bernoulli}(p)$, then:\n\n$$E(X) = p$$ $$\\text{Var}(X) = p(1-p)$$\n:::\n\n**Example with diabetes:**\n\n-   Among US adults aged 65-74, about 20.3% have diabetes\n-   Define $X = 1$ if a randomly selected adult has diabetes, $X = 0$ otherwise\n-   $E(X) = 0.203$\n-   $\\text{Var}(X) = 0.203(0.797) = 0.162$\n\n## From Bernoulli to Binomial\n\nWhat if we repeat a Bernoulli trial multiple times?\n\n**Example:** Randomly select 10 adults aged 65-74. How many have diabetes?\n\n-   Each person: Bernoulli(0.203)\n-   Total number with diabetes: Binomial(10, 0.203)\n\n::: {.callout-tip icon=\"false\"}\n## Key Relationship\n\n$$\\text{Binomial}(n, p) = \\text{Sum of } n \\text{ independent Bernoulli}(p) \\text{ trials}$$\n:::\n\n## From Bernoulli to Binomial (continued)\n\n-   The **Bernoulli distribution** is a special case of the Binomial distribution where $n=1$\n\n    -   Specifically: $$\\text{Binomial}(1, p) = \\text{Bernoulli}(p) $$\n\n-   To get a **Binomial distribution**, we simply extend the scenario from a **single** trial to **multiple** independent trials.\n\n    -   If we conduct $n$ independent Bernoulli trials with the same success probability $p$, the total number of successes across these $n$ trials will follow a Binomial distribution\n\n## Binomial Distribution: Definition\n\n::: {.callout-note icon=\"false\"}\n## Definition: Binomial Random Variable\n\n$X$ is a **Binomial random variable** if:\n\n1.  There are $n$ independent trials\n2.  Each trial has two outcomes: success or failure\n3.  Probability of success is $p$ (same for all trials)\n4.  $X$ counts the total number of successes\n\n**Notation:** $X \\sim \\text{Binomial}(n, p)$ or $X \\sim \\text{Binom}(n, p)$\n:::\n\n**Parameters:** $n$ (number of trials) and $p$ (probability of success)\n\n**Possible values:** $X \\in \\{0, 1, 2, \\ldots, n\\}$\n\n## Derivation by example\n\nApproximately 20.3% of US adults aged 65-74 have diabetes.\n\nHere, let's define\n\n-   \"Success\" (S) to be the event that a person has diabetes, then\n-   \"Failure\" (F) is the event that a person does not have diabetes.\n\nAmong US adults aged 65-74:\n\n-   $P(\\text{has diabetes}) = 0.203 = p$\n-   $P(\\text{no diabetes}) = 1 - p = 0.797$\n\n## Define an experiment\n\nSuppose we conduct an experiment in which $n = 3$ US adults aged 65-74 are tested for diabetes.\n\n-   The testing of each adult represents a trial.\n-   The outcome from each trial is either a success (has diabetes) or a failure (does not have diabetes).\n-   The outcome (S/F) of any one trial is not influenced by earlier outcomes nor does it affect later outcomes (independent trials).\n-   The probability of having diabetes ($p = 0.203$) is the same for each adult sampled.\n\n## Possible outcomes\n\nWith 3 trials and 2 possible outcomes (S/F), there are 8 possible arrangements.\n\n|       |       |\n|-------|-------|\n| $FFF$ | $FSS$ |\n| $FFS$ | $SFS$ |\n| $FSF$ | $SSF$ |\n| $SFF$ | $SSS$ |\n\n## Define the rv\n\nLet $X =$ the number of adults that have diabetes among the $n = 3$ sampled.\n\n\\\n\nRe-organize the possible arrangements:\n\n- $X = 0 \\Longleftrightarrow FFF$\n- $X = 1 \\Longleftrightarrow FFS \\cup FSF \\cup SFF$\n- $X = 2 \\Longleftrightarrow FSS \\cup SFS \\cup SSF$\n- $X = 3 \\Longleftrightarrow SSS$\n\n## Compute the probabilities\n\n\\\n\n**Case: $X = 0$ (no one has diabetes)**\n\n$$\\begin{aligned}\nP(X = 0) &= P(FFF) \\\\\n&\\overset{\\text{ind.}}{=} (1 - p)(1 - p)(1 - p) \\\\\n&= (1 - p)^3 = 0.797^3 \\approx 0.506\n\\end{aligned}$$\n\n\nBecause of the general multiplication rule for independent events, we simplified the event $FFF$ into the product of $n = 3$ separate terms.\n\n## Compute the probabilities (continued)\n\n\\\n\n**Case: $X = 1$ (exactly 1 has diabetes)**\n\n- $FFS \\Longrightarrow (1 - p)(1 - p)p = p(1 - p)^2$\n- $FSF \\Longrightarrow (1 - p)p(1 - p) = p(1 - p)^2$\n- $SFF \\Longrightarrow p(1 - p)(1 - p) = p(1 - p)^2$\n\n\\\n\nSubstitute for $p$ and $(1-p)$ then for each $p(1 - p)^2 = (0.203)(0.797)^2 \\approx 0.129$.\n\n\\\n\n$$\\begin{aligned}\nP(X = 1) &= P(FFS \\cup FSF \\cup SFF) \\\\\n&= P(FFS) + P(FSF) + P(SFF) \\\\\n&= 3p(1 - p)^2 \\approx 3(0.129) = 0.387\n\\end{aligned}$$\n\n## Compute the probabilities (continued)\n\n\\\n\n**Case: $X = 2$ (exactly 2 have diabetes)**\n\nSimilar reasoning gives\n\n- $FSS \\Longrightarrow (1 - p)pp = p^2(1 - p)$\n- $SFS \\Longrightarrow p(1 - p)p = p^2(1 - p)$\n- $SSF \\Longrightarrow pp(1 - p) = p^2(1 - p)$\n\n\\\n\nEach of the three parts is approximately $0.033$\n\n\\\n\n$$\\begin{aligned}\nP(X = 2) &= P(FSS \\cup SFS \\cup SSF) \\\\\n&= P(FSS) + P(SFS) + P(SSF) \\\\\n&= 3p^2(1 - p) \\approx 3(0.033) = 0.099\n\\end{aligned}$$\n\n\n## Compute the probabilities (continued)\n\n\\\n\n**Case: $X = 3$ (all 3 have diabetes)**\n\n$$\\begin{aligned}\nP(X = 3) &= P(SSS) \\\\\n&= ppp \\\\\n&= p^3 = (0.203)^3 \\approx 0.008\n\\end{aligned}$$\n\n\n## The Pattern Emerges\n\nLooking at our calculations:\n\n| $X$ | Number of arrangements | Probability |\n|-----|:----------------------:|:-----------:|\n| 0 | 1 | $1 \\times (0.203)^0(0.797)^3 = 0.506$ |\n| 1 | 3 | $3 \\times (0.203)^1(0.797)^2 = 0.387$ |\n| 2 | 3 | $3 \\times (0.203)^2(0.797)^1 = 0.099$ |\n| 3 | 1 | $1 \\times (0.203)^3(0.797)^0 = 0.008$ |\n\n. . .\n\n**Pattern:** \n$$P(X = k) = (\\text{number of arrangements}) \\times p^k(1-p)^{n-k}$$\n\n**Question:** How do we count the number of arrangements for any $n$ and $k$?\n\n## Counting Arrangements: \"n choose k\"\n\nThe number of ways to arrange $k$ successes among $n$ trials is:\n\n$$\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$$\n\nThis is called the **binomial coefficient** or \"n choose k\"\n\n**Reminder:** $n!$ (read \"n factorial\") means $n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1$\n\n- Example: $3! = 3 \\times 2 \\times 1 = 6$\n- Special case: $0! = 1$ (by definition)\n\n. . .\n\n**For our example with $n=3$:**\n\n- $\\binom{3}{0} = \\frac{3!}{0!3!} = \\frac{6}{1 \\times 6} = 1$ ✓\n- $\\binom{3}{1} = \\frac{3!}{1!2!} = \\frac{6}{1 \\times 2} = 3$ ✓\n- $\\binom{3}{2} = \\frac{3!}{2!1!} = \\frac{6}{2 \\times 1} = 3$ ✓\n- $\\binom{3}{3} = \\frac{3!}{3!0!} = \\frac{6}{6 \\times 1} = 1$ ✓\n\n## Binomial distribution\n\nThe final rule is \n\n\n::: {.callout-important icon=\"false\"}\n## Distribution of a Binomial random variable\n\nLet $X$ be the total number of successes in $n$ independent trials, each with probability $p$ of a success. Then probability of observing exactly $k$ successes in $n$ independent trials is \n\n$$P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k},  k= 0, 1, 2, \\dots, n $$\n:::\n\n**Components:**\n\n- $\\binom{n}{k}$: number of ways to arrange $k$ successes in $n$ trials\n- $p^k$: probability of $k$ successes\n- $(1-p)^{n-k}$: probability of $n-k$ failures\n\n## Verify with our example\n\nDoes our formula work for $n=3, p=0.203$?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate P(X=0), P(X=1), P(X=2), P(X=3)\n\nx <- 0:3\n\nprobs <- dbinom(x, size = 3, prob = 0.203)\n\ndata.frame(x = x, probability = round(probs, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  x probability\n1 0       0.506\n2 1       0.387\n3 2       0.099\n4 3       0.008\n```\n\n\n:::\n:::\n\n\n\n. . .\n\nMatches what we calculated by hand! ✓\n\n## Now extend to 10 adults\n\nNow we can easily calculate probabilities for $n=10$ adults:\n\n\\\n\n**Setup:** $X \\sim \\text{Binomial}(n=10, p=0.203)$\n\n\\\n\n**Without the formula:** We'd need to enumerate $2^{10} = 1024$ possible arrangements!\n\n\\\n\n**With the formula:** We can calculate any probability directly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# P(X = 4)\ndbinom(x = 4, size = 10, prob = 0.203)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09140151\n```\n\n\n:::\n:::\n\n\n\n\\\n\n**The binomial formula saves us from tedious counting!**\n\n\n## Binomial: Mean and Variance\n\n::: {.callout-important icon=\"false\"}\n## Theorem: Mean and Variance of Binomial R.V.\n\nIf $X \\sim \\text{Binomial}(n, p)$, then:\n\n$$E(X) = np$$ $$\\text{Var}(X) = np(1-p)$$ $$SD(X) = \\sqrt{np(1-p)}$$\n:::\n\n**Intuition:**\n\n-   If each trial has probability $p$ of success, and we do $n$ trials, we expect $np$ successes on average\n-   The variance formula comes from summing $n$ independent Bernoulli variances\n\n## Example: Diabetes in Older Adults (10 people)\n\n\\\n\nApproximately 20.3% of US adults aged 65-74 have diabetes. Suppose we randomly select 10 adults in this age group (independently).\n\n\\\n\nLet $X$ = number with diabetes among the 10 people\n\n**Setup:** $X \\sim \\text{Binomial}(n=10, p=0.203)$\n\n\\\n\n. . .\n\n**Question 1:** What is the expected value of $X$?\n\n$$E(X) = np = 10(0.203) = 2.03$$\n\n\\\n\n. . .\n\n**Question 2:** What is the standard deviation of $X$?\n\n$$SD(X) = \\sqrt{np(1-p)} = \\sqrt{10(0.203)(0.797)} = \\sqrt{1.618} \\approx 1.27$$\n\n## Example: Exactly 4 with Diabetes\n\n**Question 3:** What is the probability that exactly 4 of the 10 have diabetes?\n\n$$P(X = 4) = \\binom{10}{4} (0.203)^4 (0.797)^6$$\n\n. . .\n\n$$= \\frac{10!}{4! \\cdot 6!} (0.203)^4 (0.797)^6 = 210 \\times 0.0017 \\times 0.256 \\approx 0.091$$\n\n. . .\n\n**In R:**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbinom(x = 4, size = 10, prob = 0.203)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09140151\n```\n\n\n:::\n:::\n\n\n\n\\\n\nThe `d` in `dbinom` stands for \"density\" (probability at a specific value)\n\n## Example: At Most 3 with Diabetes\n\n**Question 4:** What is the probability that at most 3 have diabetes?\n\n$$P(X \\leq 3) = P(X=0) + P(X=1) + P(X=2) + P(X=3)$$\n\n. . .\n\nWe could calculate each term, but there's an easier way!\n\n\\\n\n. . .\n\n**In R:**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npbinom(q = 3, size = 10, prob = 0.203)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8737821\n```\n\n\n:::\n:::\n\n\n\n\\\n\nThe `p` in `pbinom` stands for \"cumulative probability\", $P(X \\leq k)$\n\n\\\n\nResult: About 87% chance that 3 or fewer have diabetes\n\n## Example: At Least 5 with Diabetes\n\n**Question 5:** What is the probability that at least 5 have diabetes?\n\n$$P(X \\geq 5) = P(X=5) + P(X=6) + \\cdots + P(X=10)$$\n\n. . .\n\n**Approach 1:** Use complement rule $$P(X \\geq 5) = 1 - P(X \\leq 4)$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pbinom(q = 4, size = 10, prob = 0.203)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03481642\n```\n\n\n:::\n:::\n\n\n\n\\\n\n. . .\n\n**Approach 2:** Use `lower.tail = FALSE`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npbinom(q = 4, size = 10, prob = 0.203, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03481642\n```\n\n\n:::\n:::\n\n\n\nResult: About 3.4% chance that 5 or more have diabetes\n\n## R Functions for Binomial Distribution\n\n\\\n\n\n| Function | What it does | Example |\n|-----------------------|-----------------------------|---------------------|\n| `dbinom()` | Probability at specific value: $P(X=k)$ | `dbinom(x = 4, size = 10, prob = 0.203)` |\n| `pbinom()` | Cumulative probability: $P(X \\leq k)$ | `pbinom(q = 3, size = 10, prob = 0.203)` |\n| `qbinom()` | Quantile: Find $k$ for given probability | `qbinom(p = 0.5, size = 10, prob = 0.203)` |\n| `rbinom()` | Generate random samples | `rbinom(n = 100, size = 10, prob = 0.203)` |\n\n: {tbl-colwidths=\"\\[20, 40, 50\\]\"}\n\n## Visualizing the Binomial Distribution\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_random_variables_binomial2_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n## Key Takeaways\n\n**Random Variables:**\n\n-   Connect random outcomes to numerical values and probabilities\n-   Expected value: weighted average using probabilities\n-   Variance: measures spread around the expected value\n-   Linear combinations: $E(aX+bY) = aE(X) + bE(Y)$, but $\\text{Var}(aX+bY) = a^2\\text{Var}(X) + b^2\\text{Var}(Y)$ (if independent)\n\n**Binomial Distribution:**\n\n-   Models number of successes in $n$ independent trials\n-   Parameters: $n$ (trials) and $p$ (success probability)\n-   $E(X) = np$, $\\text{Var}(X) = np(1-p)$\n-   Use R functions: `dbinom()`, `pbinom()`, etc.\n\n## Looking Ahead\n\n**Wednesday:**\n\n-   Normal distribution\n-   Central Limit Theorem\n-   Sampling distributions\n\n**Next steps:**\n\n-   Review textbook sections 3.1-3.2\n-   Work on HW 2 (due Tuesday, January 27)\n-   Practice calculating binomial probabilities in R\n\n## Questions?\n\n![](https://media.giphy.com/media/3o7btPCcdNniyf0ArS/giphy.gif){fig-align=\"center\" width=\"400\"}\n",
    "supporting": [
      "09_random_variables_binomial2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}