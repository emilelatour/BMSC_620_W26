{
  "hash": "c25c042e0d92fe4952a606a956b6db7c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Muddy Points\"\nsubtitle: \"Random Variables and Binomial Distribution\"\ndate-modified: \"today\"\nformat:\n  html:\n    toc: true\n    toc-depth: 2\n    code-fold: false\n---\n\n\n\n\n## Overview\n\nThank you for your feedback on Monday's lecture! The pacing feedback was excellent - 69% felt it was \"about right\" and 31% \"slightly too fast,\" which tells me most of you are keeping up well. I'll continue with strategic pauses and check-ins to help everyone stay on track.\n\nSeveral clear themes emerged from your muddy points. Let me address them:\n\n---\n\n## What Do I Need to Memorize/Know How to Do?\n\n**The Question:**\n\n> \"What should we memorize in terms of formulas? Are we supposed to know how to do these formulas by hand, or using the R functions?\"\n\n**The Answer:**\n\nFor this course, I care about **three things**:\n\n### 1. Conceptual understanding\n\nYou should be able to explain, in words:\n\n- What kind of random variable you are working with (discrete vs continuous)\n- When a binomial model is appropriate\n- What the parameters mean (for example: n and p)\n- What a probability, mean, or variance is describing in context\n\nYou do not need to reproduce derivations, but you should understand what the formulas are doing.\n\n### 2. Using R correctly\n\nYou should be comfortable with:\n\n- Choosing the right R function (`dbinom()`, `pbinom()`, etc.)\n- Knowing what each argument represents\n- Interpreting the output in words\n\nFor example:\n\n- \"Exactly k successes\" â†’ `dbinom()`\n- \"At most k successes\" â†’ `pbinom()`\n\nBeing able to explain why you chose a function matters more than the numeric answer itself.\n\n### 3. Explaining your reasoning\n\nOn homework and exams, you should be able to:\n\n- Describe your approach in plain language\n- Explain what distribution you used and why\n- Interpret results in the context of the problem\n\n### What you are **not** expected to do\n\nYou do not need to:\n\n- Memorize probability formulas\n- Calculate probabilities by hand\n- Derive distributions from scratch\n- Memorize every symbol used on slides\n\nFormulas are shown to build understanding and connect to R â€” not because I expect you to reproduce them from memory.\n\n\n\n## Variable Notation Confusion\n\n**The Issue:**\n\n> \"The way variables were switching from X to k was confusing. It would be helpful if there was a legend on each slide for what each variable means.\"\n\n**Why This Happens:**\n\nDifferent contexts use different notation conventions:\n\n- **X** = the random variable itself (e.g., \"number of successes\")\n- **x or k** = a specific value X might take (e.g., \"exactly 3 successes\")\n- **n** = number of trials\n- **p** = probability of success on each trial\n\nIâ€™ll be more explicit about this on slides going forward.\n\n### Common Notation Guide\n\n| Symbol | Meaning | Example |\n|--------|---------|---------|\n| $X$ | Random variable | \"Number of heads in 10 flips\" |\n| $x$ or $k$ | Specific value | \"Exactly 3 heads\" |\n| $n$ | Number of trials | 10 flips |\n| $p$ | Probability of success | 0.5 for fair coin |\n| $\\mu$ | Mean/Expected value | Average outcome |\n| $\\sigma$ | Standard deviation | Spread of outcomes |\n| $\\sigma^2$ | Variance | Squared spread |\n\n---\n\n## Too Many Formulas / Feeling Abstract\n\n**The Concern:**\n\n> \"Felt like there were just a whole lot of formulas... Some of the formulas started to feel a bit abstract.\"\n\n**My Response:**\n\nYou're absolutely right. There *are* a lot of formulas. Here's how to think about them:\n\n### The Core Ideas (Not the Formulas)\n\n**For any distribution, we care about:**\n\n1. **What values can it take?** (support)\n2. **How likely is each value?** (probability/density)\n3. **What's the typical value?** (mean/expected value)\n4. **How spread out is it?** (variance/standard deviation)\n\n**For binomial specifically:**\n\n- \"How many successes in n tries?\" â†’ use `dbinom()` or `pbinom()`\n- That's it. The formulas are just showing *why* R gives you those answers.\n\n### Plain Language Interpretations\n\nI'll make sure to add these after formulas. For example:\n\n**Formula:** $P(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}$\n\n**Plain English:** \"The probability of getting exactly k successes in n trials, when each trial has probability p of success\"\n\n**R Code:** `dbinom(k, size = n, prob = p)`\n\n**Even Plainer:** \"Use this when you want to know: What are the chances of getting *exactly* this many successes?\"\n\n---\n\n## Variance of Linear Combinations\n\n**The Concern:**\n\n> \"Variance of linear combinations was confusing to me, especially if it will need to be applied to real/our data.\"\n\n**My Response:**\n\nThis is a very reasonable concern.\n\nFor this course, variance of linear combinations is **not** something you will be expected to apply directly to real data by hand.\n\nWhat I want you to take away is the **idea**, not the formula:\n\n- When you combine independent random variables, their variances add\n- Standard deviations do not add directly\n- In practice, software (R) handles these calculations for us\n\nYou will **not** be asked to derive or memorize variance formulas for linear combinations.\n\nLater in the course, you will see variance show up in applied settings (for example, through standard errors and uncertainty in estimates), but at that point we will rely on:\n\n- Interpretation\n- Software output\n- Intuition about variability\n\n- **not** manual variance calculations.\n\n**Bottom line:** Understand the concept that variances add, but don't worry about doing these calculations by hand. This topic is covered in introductory probability courses because it is important in fields like finance and engineering, but in this course we focus on understanding the idea rather than doing the calculations by hand.\n\n\n## Clarifying Bernoulli Variance (Conceptual)\n\n**The Question:**\n\n> \"It is a bit unclear where the equation for the variance for the Bernoulli distribution came from. I don't understand how it relates to other variance equations.\"\n\n**My Response:**\n\nThis is a great question.\n\nThe Bernoulli variance formula is **not** a new or special definition of variance. It comes from the same definition of variance we use for any random variable â€” it just simplifies nicely because a Bernoulli variable can only take two values (0 and 1).\n\n**What's important to know:**\n\n- Variance always measures how spread out values are around the mean\n- For a Bernoulli variable:\n  - The mean is $p$\n  - Outcomes are either 0 or 1\n- Because of that simplicity, the general variance definition reduces to the compact result:\n\n$$\\text{Var}(X) = p(1 - p)$$\n\nYou do **not** need to derive this yourself. The key takeaway is the **behavior** of the variance:\n\n- It is largest when outcomes are most uncertain ($p$ near 0.5)\n  - Example: $p = 0.5 \\rightarrow \\text{Var}(X) = 0.5(0.5) = 0.25$\n- It is smaller when outcomes are more predictable ($p$ near 0 or 1)\n  - Example: $p = 0.9 \\rightarrow \\text{Var}(X) = 0.9(0.1) = 0.09$ (less variable)\n  - Example: $p = 0.1 \\rightarrow \\text{Var}(X) = 0.1(0.9) = 0.09$ (less variable)\n\nWe'll continue to rely on this intuition and on R for calculations, rather than algebraic derivations.\n\n\n## R Functions Still Confusing\n\n**The Concern:**\n\n> \"I'm still a little confused on some of the different binomial functions in R, and pbinom's link to p-values.\"\n\n**Let Me Break This Down:**\n\n### The Four Binomial Functions\n```r\n# d = density/probability (exactly)\ndbinom(x = 3, size = 10, prob = 0.5)\n# \"What's P(X = 3)? Probability of EXACTLY 3 successes\"\n\n# p = cumulative probability (at most)\npbinom(q = 3, size = 10, prob = 0.5)\n# \"What's P(X â‰¤ 3)? Probability of 3 OR FEWER successes\"\n\n# q = quantile (what value gives this cumulative probability?)\nqbinom(p = 0.25, size = 10, prob = 0.5)\n# \"What value of x has 25% of the distribution below it?\"\n\n# r = random (generate random samples)\nrbinom(n = 100, size = 10, prob = 0.5)\n# \"Simulate 100 draws from this distribution\"\n```\n\n### Visual Guide\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_random_variables_binomial_muddy_points_files/figure-html/unnamed-chunk-1-1.png){width=768}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_random_variables_binomial_muddy_points_files/figure-html/unnamed-chunk-2-1.png){width=768}\n:::\n:::\n\n\n\n\n### About p-values\n\n**Important:** `pbinom()` calculates cumulative probabilities, but it's not *directly* a p-value.\n\nA **p-value** is \"the probability of getting results as extreme or more extreme than what we observed, *assuming the null hypothesis is true*.\"\n\nWe can *use* `pbinom()` to calculate p-values, but they're not the same thing:\n\n- `pbinom()` = general cumulative probability function\n- p-value = specific probability used for hypothesis testing\n\nWe'll cover this more when we get to hypothesis testing!\n\n---\n\n## Expected Value Calculation Question\n\n**The Question:**\n\n> \"I didn't understand the expected value (mean) calculations. Why does the value of the dice face factor into the equation since it's an equal chance of rolling any value? Additionally, where did the 1/9 value come from?\"\n\n**Let Me Walk Through This:**\n\n### Fair Die Expected Value\n\nFor a fair 6-sided die, each outcome has probability 1/6:\n\n$$E(X) = 1 \\cdot \\frac{1}{6} + 2 \\cdot \\frac{1}{6} + 3 \\cdot \\frac{1}{6} + 4 \\cdot \\frac{1}{6} + 5 \\cdot \\frac{1}{6} + 6 \\cdot \\frac{1}{6}$$\n\n$$E(X) = \\frac{1+2+3+4+5+6}{6} = \\frac{21}{6} = 3.5$$\n\n**Why do the face values matter?**\n\n- Even though each face has equal probability (1/6), the *values* are different (1, 2, 3, 4, 5, 6)\n- We're calculating the *average value* we'd get in the long run\n- Rolling a 6 contributes more to the average than rolling a 1!\n\n### Where did 1/9 come from?\n\nI just made up the probabilities for the rolls with the unfair dice. I aimed to show that the probabilities don't have to be the same for all possible outcomes. \n\n- Rolls of 1, 2, 3, or 4 all had the same probability: $1/9$\n- A roll of 5 had a probability of $2/9$\n- A roll of 6 had a probability of $3/9$\n- Probabilities add to 1. Check: $1/9 + 1/9 + 1/9 + 1/9 + 2/9 + 3/9 = 1$\n\nThe dice is loaded toward higher numbers.\n\n### The big picture for expected value\n\nExpected value is best thought of as a long-run average, not a prediction of what will happen on a single trial.\n\n- It combines all possible values of a random variable\n- Each value is weighted by how likely it is\n- Larger values contribute more to the average than smaller values\n\nThis same idea applies to any random variable (not just dice) and is why expected value shows up throughout statistics.\n\nYou do not need to compute expected values by hand beyond simple examples like this. In practice, weâ€™ll often:\n\n- rely on formulas (for known distributions), or\n- let software do the calculation for us.\n\nThe important skill is understanding what expected value represents, not the mechanics of summing terms.\n\n\n## Questions?\n\nIf anything is still muddy, please:\n\n- Come to office hours\n- Ask in class on Wednesday\n\nThanks for the thoughtful feedback! ðŸŽ²",
    "supporting": [
      "09_random_variables_binomial_muddy_points_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}