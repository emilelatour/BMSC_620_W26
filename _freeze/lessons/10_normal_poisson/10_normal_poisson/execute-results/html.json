{
  "hash": "73fa5284de455b0d3b47da5a28cc2d5f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Normal and Poisson Distributions\"\nsubtitle: \"Textbook Sections 3.3–3.4\"\nauthor: \"Emile Latour, Nicky Wakim, Meike Niederhausen\"\ndate: \"2026-01-28\"\ndate-format: long\nformat:\n  revealjs:\n    theme: \"../../assets/css/reveal-bmsc620_v5.scss\"\n    slide-number: true\n    show-slide-number: all\n    width: 1955\n    height: 1100\n    footer: \"BMSC 620 | Normal + Poisson\"\n    html-math-method: mathjax\n    chalkboard: true\n    header-includes: |\n      <style>\n      #wrap {\n        width: 1650px;\n        height: 900px;\n        margin: 0 auto;\n        overflow: hidden;\n        border: 1px solid #999;\n        border-radius: 8px;\n      }\n      #frame {\n        width: 1650px;\n        height: 900px;\n        border: 0;\n        zoom: 1.25;\n        -moz-transform: scale(1.25);\n        -moz-transform-origin: 0 0;\n      }\n      /* Make selected tables bigger in RevealJS slides (robust to flextable output) */\n      .reveal .tbl-big {\n        font-size: 28px !important;\n        /* center the output block inside the column */\n        display: flex;\n        justify-content: center;\n      \n        /* scale from center */\n        transform: scale(1.6);\n        transform-origin: top center;\n      }\n      .reveal .tbl-big table,\n      .reveal .tbl-big .flextable,\n      .reveal .tbl-big .flextable table {\n        font-size: 28px !important;\n      }\n      .reveal .tbl-big td,\n      .reveal .tbl-big th {\n        padding: 6px 10px !important;\n      }\n      </style>\nexecute:\n  echo: true\n  warning: false\n  message: false\n  freeze: auto\n---\n\n\n\n\n\n\n# Learning Objectives \n\nBy the end of today's lecture, you will be able to:\n\n1. Describe the characteristics of the Normal distribution\n2. Calculate and interpret Z-scores to standardize observations\n3. Use the Empirical Rule (68-95-99.7) to estimate probabilities\n4. Calculate probabilities using `pnorm()` and find percentiles using `qnorm()`\n5. Apply the Normal approximation to the Binomial distribution\n6. Recognize when to use a Poisson distribution and calculate probabilities\n\n## Roadmap for Today\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n**Part 1: Normal Distribution Basics**\n\n- Continuous distributions\n- Normal parameters (μ, σ)\n- Standard Normal distribution\n\n**Part 2: Z-scores & The Empirical Rule**\n\n- Standardization\n- 68-95-99.7 rule\n- Identifying unusual observations\n\n**Part 3: Calculating with R**\n\n- `pnorm()` for probabilities\n- `qnorm()` for percentiles\n- Real-world examples\n:::\n\n::: {.column width=\"50%\"}\n**Part 4: Normal Approximation**\n\n- When Binomial → Normal\n- Conditions: $np \\geq 10$, $n(1-p) \\geq 10$\n- Continuity correction\n\n**Part 5: Poisson Distribution**\n\n- Modeling rare events\n- Rate × time = λ\n- Poisson approximation to Binomial\n\n**Part 6: Wrap-up**\n\n- Summary\n- Next steps\n:::\n::::\n\n## Last time: Discrete vs. Continuous Random Variables\n\n::::::: columns\n:::: {.column width=\"48%\"}\n::: {.callout-important icon=\"false\"}\n## Discrete Random Variable\n\nA **discrete r.v.** takes on:\n\n-   A finite number of values, OR\n-   A countably infinite number of values\n\n**Examples:**\n\n-   Number of heads in 10 coin flips\n-   Number of students in a class\n-   Number of COVID cases per day\n:::\n::::\n\n:::: {.column width=\"48%\"}\n::: {.callout-warning icon=\"false\"}\n## Continuous Random Variable\n\nA **continuous r.v.** can take:\n\n-   Any real value in an interval\n-   Any value in a union of intervals\n\n**Examples:**\n\n-   Height\n-   Blood pressure\n-   Time until an event occurs\n:::\n::::\n:::::::\n\n::: fragment\n**Today's focus:** Continuous random variables\n:::\n\n\n# Continuous random variables\n\n## Continuous rv in general\n\n- The distribution of a continuous rv is governed by a *density function/curve*.\n- Probabilities are calculated as *area* under the curve over an *interval*.\n- Total area beneath the density function/curve is 1.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_normal_poisson_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n\n\n## Probability between *a* and *b*\n\nThe area beneath the density curve/function between two points *a* and *b* represents the probability that the random variable ($X$, say) will assume some value between *a* and *b*\n\nWhen working with continuous random variables, probability is found for\n**intervals of values** rather than **individual values**.\n\n-   The probability that a continuous r.v. $X$ takes on any single\nindividual value is 0\n\n-   That is, $P(X = x) = 0$.\n\n-   Thus, $P(a < X < b)$ is equivalent to $P(a \\leq X \\leq b)$\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_normal_poisson_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n\n## Probability at exactly *a*\n\nFor a **continuous** random variable X:\n\n- Probability comes from **areas under the curve**\n- Areas require **width**\n- A single point has **no width**\n\n\\\n\nSo:\n\n- $P(X = a) = 0$\n- Only intervals have non-zero probability (for example, $P(a ≤ X ≤ b)$)\n\n\\\n**Intuition**: you can shade an interval, but you can't shade a point.\n\n## All the following are equivalent\n\nFor a **continuous** r.v. $X$, all of the following are equivalent:\n\n$$P(a \\leq X \\leq b) = P(a < X \\leq b)$$\n$$= P(a \\leq X < b) = P(a < X < b)$$\n\nThat is, we can safely ignore equality when defining the endpoints of a given interval. **[This is certainly not true for discrete rv]**\n\n# Normal distribution\n\n![Artwork by Allison Horst](/img_slides/horst_normal_not_normal.png){width=\"350px\"}\n\n## Normal distribution\n\n::::: columns\n::: {.column width=\"40%\"}\n-   A random variable X is modeled with a normal distribution:\n\n-   **Shape:** symmetric, unimodal bell curve\n-   **Center:** mean $\\mu$\n-   **Spread (variability):** standard deviation $\\sigma$\n\n\n\n-   Shorthand for a random variable, $X$, that has a Normal\ndistribution: $$X \\sim \\text{Normal}(\\mu, \\sigma)$$\n:::\n\n<!-- ::: {.column width=\"60%\"} -->\n<!-- ![](/img_slides/Normal_examples.png) -->\n<!-- ::: -->\n:::::\n\n-   **Example:** We recorded the high temperature in the past 100 years\nfor today. The mean high is 19°C (66.2°F)\n\n<!-- ## Functional form -->\n## Anatomy of the Normal curve\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_normal_poisson_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n\n<!-- The shape of the density curve is given by -->\n\n<!-- $$ -->\n<!-- f(x;\\mu,\\sigma) -->\n<!-- = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} -->\n<!--   e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}, -->\n<!-- \\quad \\text{for } -\\infty < x < \\infty,\\; -->\n<!-- -\\infty < \\mu < \\infty,\\; -->\n<!-- \\sigma > 0 -->\n<!-- $$ -->\n\nThe **Normal distribution has a closed-form equation**, but for this course:\n\n- You do **not** need to memorize it\n- You do **not** need to compute it by hand\n- What matters is how $\\mu$ and $\\sigma$ shape the curve\n\n## Different means, Same SD\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_normal_poisson_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n::: notes\n- Orange curve: $\\mu = 15$, $\\sigma = 3$\n- Blue curve: $\\mu = 20$, $\\sigma = 3$\n- Same spread (width), different centers\n- The mean determines where the bell curve is centered on the x-axis\n:::\n\n## Same mean, Different SD\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_normal_poisson_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\n\n::: notes\n- Orange curve: $\\mu = 20$, $\\sigma = 2$\n- Blue curve: $\\mu = 20$, $\\sigma = 5$\n- Same center, different spreads\n- Smaller SD = taller, narrower curve (less variability)\n- Larger SD = shorter, wider curve (more variability)\n:::\n\n## Different means, different SD\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_normal_poisson_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n\n\n::: notes    \n- Orange curve: mu = 15, sigma = 2 (centered left, narrow/tall)\n- Blue curve: mu = 24, sigma = 5 (centered right, wide/short)\n- The mean controls the horizontal location (center)\n- The standard deviation controls the spread (width/height)\n:::\n\n# Z-scores & Standardization\n\n## Why standardize?\n\nDifferent normal distributions have different scales:\n\n- Heights measured in centimeters: $X \\sim N(170, 10)$\n- Test scores: $Y \\sim N(75, 8)$\n- Blood pressure: $Z \\sim N(120, 15)$\n\n**Problem:** How do we compare observations across different distributions?\n\n**Solution:** Convert to a common scale using **Z-scores**\n\n## What is a Z-score?\n\nThe **Z-score** tells you how many standard deviations an observation is from the mean:\n\nSuppose $X$ is an arbitrary random variable that follows a normal distribution with mean $\\mu$ and standard deviation $\\sigma$, i.e. $X \\sim N(\\mu, \\sigma)$\n\n$$Z = \\dfrac{X - \\mu}{\\sigma} \\Longleftrightarrow X = \\mu + Z\\sigma$$\n\n**Notation:**\n\n- $X$ = original observation\n- $\\mu$ = mean of the distribution\n- $\\sigma$ = standard deviation of the distribution\n- $Z$ = standardized score\n\n## Interpreting Z-scores\n\n| Z-score | Interpretation |\n|---------|----------------|\n| $Z = 0$ | Exactly at the mean |\n| $Z = 1$ | One SD above the mean |\n| $Z = -1$ | One SD below the mean |\n| $Z = 2.5$ | 2.5 SDs above the mean |\n| $Z = -1.8$ | 1.8 SDs below the mean |\n\n::: fragment\n**Rule of thumb:**\n\n- Z-scores between -2 and 2 are common\n- Z-scores beyond ±3 are rare (unusual observations)\n:::\n\n## Example: Height standardization\n\nSuppose adult male heights follow $N(\\mu = 175, \\sigma = 7)$ cm.\n\n**Question:** What is the Z-score for a man who is 189 cm tall?\n\n. . .\n\n$$Z = \\frac{X - \\mu}{\\sigma} = \\frac{189 - 175}{7} = \\frac{14}{7} = 2$$\n\n**Interpretation:** This man is 2 standard deviations above the mean height.\n\n## Standard Normal Distribution\n\nWhen we standardize a normal random variable, we get the **Standard Normal distribution**:\n\n$$Z \\sim N(\\mu = 0, \\sigma = 1)$$\n\n**Properties:**\n\n- Mean = 0\n- Standard deviation = 1\n- Denoted by $Z$\n- All normal distributions can be converted to this standard form\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_normal_poisson_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n\n\n# The Empirical Rule\n\n## The 68-95-99.7 Rule\n\nFor **any** normal distribution:\n\n- **68%** of observations fall within **1 SD** of the mean\n- **95%** of observations fall within **2 SDs** of the mean\n- **99.7%** of observations fall within **3 SDs** of the mean\n\n![Empirical rule (68–95–99.7).](/img_slides/empirical_rule_openintro.png){width=\"350px\"}\n\n<small>Source: *OpenIntro Biostatistics*.</small>\n\n## Using the Empirical Rule\n\n**Example:** IQ scores follow $N(\\mu = 100, \\sigma = 15)$\n\n**Questions:**\n\n1. What percentage of people have IQ between 85 and 115?\n\n. . .\n\n- 85 = 100 - 15 = μ - σ\n- 115 = 100 + 15 = μ + σ\n- **Answer: About 68%**. So about 2/3 of people fall within 15 points of 100.\n\n. . .\n\n2. What percentage have IQ between 70 and 130?\n\n. . .\n\n- 70 = 100 - 30 = μ - 2σ\n- 130 = 100 + 30 = μ + 2σ\n- **Answer: About 95%**\n\n## Why is the Empirical Rule useful?\n\n::: incremental\n1. **Quick estimates** without calculations\n2. **Identify unusual observations**\n    - Values beyond 2 SDs are uncommon (<5%)\n    - Values beyond 3 SDs are very rare (<0.3%)\n3. **Check data quality**\n    - If your data doesn't follow this pattern, it may not be normally distributed\n:::\n\n# Calculating Probabilities with R\n\n## Calculating probabilities from a Normal distribution\n\nThere are several ways to calculate probabilities from a normal distribution:\n\n1. **Calculus**  \n   *(not for us!)*\n\n2. **Normal probability tables**\n   - Included in the [textbook (Appendix B.1)](https://www.openintro.org/go/?id=stat_prob_tables_normal_t_chisq&referrer=/book/os/index.php)\n   - Helpful historically, but not required for this course\n\n3. **R commands (what we will use)**\n   - $P(Z \\leq q) =$ `pnorm(q, mean = 0, sd = 1)`\n   - $P(Z > q) =$ `pnorm(q, mean = 0, sd = 1, lower.tail = FALSE)`\n\n> In this course, we will calculate normal probabilities using **R**.\n\n## R functions for the Normal distribution\n\n::: {.callout-tip icon=\"false\"}\n## Four key functions\n\n| Function | Purpose | Example |\n|----------|---------|---------|\n| `dnorm()` | **Density** at a point | Height of curve at x |\n| `pnorm()` | **Cumulative probability** | P(X ≤ x) |\n| `qnorm()` | **Quantile/percentile** | What x gives P(X ≤ x) = p? |\n| `rnorm()` | **Random** samples | Generate random normal data |\n\n**Most common:** `pnorm()` and `qnorm()`\n:::\n\n## `pnorm()`: Cumulative probabilities\n\n\\\n\n`pnorm(q, mean, sd, lower.tail = TRUE)`\n\n**Cumulative probability** is the total area under the curve to the left of a value, i.e. the probability that a random variable is **less than or equal to a value**: $P(X \\le q)$.\n\n- `q` = the value you're interested in\n- `mean` $= \\mu$\n- `sd` $= \\sigma$\n- `lower.tail = TRUE` $\\longrightarrow P(X \\le q)$\n- `lower.tail = FALSE` $\\longrightarrow P(X \\gt q)$\n\n. . .\n\n\\\n\n**Example:** For standard normal $Z \\sim N(0, 1)$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# P(Z ≤ 1.96)\npnorm(1.96, mean = 0, sd = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9750021\n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\n**Interpretation:** About 97.5% of observations fall below Z = 1.96\n\n## Example: Standard normal probabilities\n\n\\\n\nLet $Z \\sim N(0, 1)$. Calculate:\n\n\\\n\n**1. $P(Z < 2.67)$**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(2.67)  # mean=0, sd=1 is the default\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9962074\n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\n\\\n\n**2. $P(Z > -0.37)$**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(-0.37, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6443088\n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\nOr \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pnorm(-0.37)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6443088\n```\n\n\n:::\n:::\n\n\n\n\n\n## Example: Standard normal probabilities (continued)\n\n\\\n\n**3. $P(-2.18 < Z < 2.46)$**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(2.46) - pnorm(-2.18)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9784244\n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\n\\\n\n**4. $P(Z = 1.53)$**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For continuous distributions, P(X = x) = 0\n0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n\n::: fragment\n**Remember:** For continuous random variables, the probability of any single exact value is 0!\n:::\n\n## Example: General normal distribution\n\n\\\n\nSuppose the distribution of diastolic blood pressure (DBP) in 35- to\n44-year old men is normally distributed with mean 80 mm Hg and variance\n144 mm Hg.\n\n\\\n\n$$X \\sim N(\\mu = 80, \\sigma = 12)$$ \n\n(Note: $variance = 144$, so $SD = \\sqrt{144} = 12$)\n\n\\\n\n**Question 1:** What proportion has mild hypertension (DBP between 90 and 99)?\n\n. . .\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# P(90 ≤ X ≤ 99)\npnorm(99, mean = 80, sd = 12) - pnorm(90, mean = 80, sd = 12)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1456556\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\nAbout 14.6% have mild hypertension.\n\n## Visualizing the probability\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_normal_poisson_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n\n## Same probability, different scale\n\n\\\n\nRecall the Z-score transformation:\n\n$$\nZ = \\frac{X - \\mu}{\\sigma}\n$$\n\nThis means any probability from a normal distribution can be computed in two equivalent ways:\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n**Original scale**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(99, mean = 80, sd = 12)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9433272\n```\n\n\n:::\n:::\n\n\n\n\n:::\n::: {.column width=\"50%\"}\n**Standardized scale**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm((99 - 80) / 12, mean = 0, sd = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9433272\n```\n\n\n:::\n:::\n\n\n\n\n:::\n::::\n\n\\ \n\n- These two probabilities are **identical**.\n- Standardization changes the **scale**, not the probability.\n\n\n\n# Finding Percentiles\n\n## `qnorm()`: Finding percentiles\n\\\n\n\n`qnorm(p, mean, sd, lower.tail = TRUE)`\n\n\\\n\nA **percentile** is the value below which a given percentage of observations fall;  \n`qnorm()` returns the **value on the x-axis** corresponding to a cumulative probability.\n\n- `p` = the probability/percentile (as a decimal)\n- `mean` = μ\n- `sd` = σ\n- Returns the **value** where $P(X ≤ value) = p$\n\n. . .\n\n\\\n\n**Example:** What Z-score has 97.5% of data below it?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(0.975, mean = 0, sd = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.959964\n```\n\n\n:::\n:::\n\n\n\n\nThis is the famous **1.96** critical value!\n\n## Example: Blood pressure percentiles\n\n\\\n\nDBP: $X \\sim N(80, 12)$\n\n\\\n\n**Question 2:** What is the 10th percentile?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(0.10, mean = 80, sd = 12)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 64.62138\n```\n\n\n:::\n:::\n\n\n\n\n10% of men have DBP below about 64.6 mm Hg.\n\n. . .\n\n\\\n\n**Question 3:** What is the 95th percentile?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(0.95, mean = 80, sd = 12)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 99.73824\n```\n\n\n:::\n:::\n\n\n\n\n95% of men have DBP below about 99.7 mm Hg.\n\n## Connecting percentiles to Z-scores\n\nAny percentile question can be solved with Z-scores:\n\n**What is the 10th percentile of N(80, 12)?**\n\n\\\n\n**Step 1:** Find Z-score for 10th percentile\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nz <- qnorm(0.10)\nz\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.281552\n```\n\n\n:::\n:::\n\n\n\n\n\\\n\n**Step 2:** Convert back to original scale\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- 80 + z * 12\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 64.62138\n```\n\n\n:::\n:::\n\n\n\n\n\\\n\nFormula: $X = \\mu + Z \\cdot \\sigma$\n\n# Normal Approximation to Binomial\n\n## When is a binomial approximately normal?\n\nRecall that a binomial random variable $X$ counts the total number of successes in $n$ independent trials, each with probability $p$ of a success.\n\nProbability function for $k = 0, 1, ..., n$ :\n    $$P(X = k) = {n\\choose k}p^k(1-p)^{n-k}$$\n\nAs *n* gets larger, the binomial distribution becomes more **symmetric** and can be approximated by a normal distribution.\n\n::: {.callout-note icon=\"false\"}\n**Rule of thumb:** Normal approximation works well when:\n\n$$np \\geq 10 \\quad \\text{and} \\quad n(1-p) \\geq 10$$\n\n- Ensures sample size ($n$) is moderately large and the $p$ is not too close to 0 or 1\n- Other resources use other criteria (like $npq>5$ or $np>5$)\n\n:::\n\n## Visual: Binomial approaching Normal\n\n**Binomial distributions for different $n$ (columns) and $p$ (rows)**\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_normal_poisson_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n\n\n\n## Normal approximation parameters\n\n\\\n\nIf $X \\sim \\text{Binomial}(n, p)$ and the conditions are met:\n\n$$X \\approx N(\\mu, \\sigma)$$\n\nwhere:\n\n$$\\mu = np \\quad \\text{and} \\quad \\sigma = \\sqrt{np(1-p)}$$\n\n::: fragment\n**These are the same formulas** for the mean and SD of a binomial distribution!\n:::\n\n## Continuity correction\n\nThe binomial is **discrete** (0, 1, 2, ...), but the Normal is **continuous**.  \n\\\n\nSo we \"nudge\" the cutoff by 0.5 when using the Normal approximation.\n\n- For **left-tail** probabilities ($<$ or $\\le$): **add 0.5**\n  - $P(X \\le k)$  becomes  $P(Y \\le k + 0.5)$\n  - $P(X < k)$    becomes  $P(Y \\le k - 0.5)$  (since $X < k$ means $X \\le k-1$)\n\n\n- For **right-tail** probabilities ($>$ or $\\ge$): **subtract 0.5**\n  - $P(X \\ge k)$  becomes  $P(Y \\ge k - 0.5)$\n  - $P(X > k)$    becomes  $P(Y \\ge k + 0.5)$  (since $X > k$ means $X \\ge k+1$)\n\n\\\nWhere $X$ is binomial, and $Y$ is the Normal approximation.\n\n## Example: COVID vaccination status\n\n\\\n\nAbout 25% of people that test positive for Covid-19 are vaccinated for\nit. Suppose 100 people have tested positive for Covid-19 (independently\nof each other). Let $X$ denote the number of people that are vaccinated\namong the 100 that tested positive. What is the probability that fewer\nthan 20 of the people that tested positive are vaccinated?\n\n\nLet $X$ = number vaccinated among the 100.\n\n\\\n\n**Question:** What is $P(X < 20)$?\n\n. . .\n\n**Check conditions:**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 100\np <- 0.25\n\nn * p  # Should be ≥ 10\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 25\n```\n\n\n:::\n\n```{.r .cell-code}\nn * (1 - p)  # Should be ≥ 10\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 75\n```\n\n\n:::\n:::\n\n\n\n\n✓ Conditions met!\n\n## Exact vs. Approximate probability (1/2)\n\n**Method 1: Exact (Binomial)**\n\n$$P(X < 20) = P(X \\leq 19) = P(X=0) + P(X=1) + \\cdots + P(X=19)$$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npbinom(19, size = 100, prob = 0.25)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09953041\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n. . .\n\n\\\n\n**Method 2: Normal Approximation**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- n * p\nsigma <- sqrt(n * p * (1 - p))\n\npnorm(19, mean = mu, sd = sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.08292833\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n. . .\n\n\\\n\nVery close! The normal approximation is 0.083 vs. exact 0.100.\n\n. . .\n\n## Exact vs. Approximate probability (2/2)\n\n\\\n\n***Method 3: Normal Approximation with continuity correction***\n\nBecause we want $P(X < 20) = P(X \\le 19)$, we use 19.5 in the normal approximation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- n * p\nsigma <- sqrt(n * p * (1 - p))\n\npnorm(19 + 0.5, mean = mu, sd = sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1020119\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\\\n\nWith continuity correction: 0.102\n\n## What’s really happening?\n\n- A binomial counts the number of successes\n- When n is large, this count behaves like a normal variable\n- The normal approximation lets us:\n  - Use Z-scores\n  - Use `pnorm()`\n  - Reuse everything we just learned\n\n## When to use each method?\n\n**Use Binomial (exact):**\n\n- When n is small or moderate\n- When you need exact probabilities\n- R handles this easily with `pbinom()`\n\n**Use Normal approximation:**\n\n- When n is very large (computing binomial probabilities becomes slow)\n- For theoretical understanding\n- Historically important (before computers!)\n\n\\\n\n**In practice:** With modern computers, we usually just use the exact binomial.\n\n## Checking whether Normal is reasonable (1/2)\n\nIn practice, we check normality visually:\n\n- Histogram or density plot (shape: roughly symmetric, unimodal)\n- Q-Q plot (points close to a straight line)\n- If using a normal model: check residuals, not raw outcomes\n\nWe usually avoid hypothesis tests (e.g., Shapiro-Wilk) because:\n\n- With large n, tiny deviations look \"significant\"\n- \tWith small n, tests have low power\n- Visuals + context are more informative\n\n\n[Quantile-quantile plots in ggplot2](https://ggplot2.tidyverse.org/reference/geom_qq.html)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example code\nggplot(data, \n       aes(sample = x)) +\n  stat_qq() +\n  stat_qq_line()\n```\n:::\n\n\n\n\n\n## Checking whether Normal is reasonable (2/2)\n\nA Q-Q plot compares your sample quantiles to theoretical Normal quantiles; straight-line agreement means the distributional shape matches Normal (especially in the tails).\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_normal_poisson_files/figure-revealjs/unnamed-chunk-35-1.png){width=960}\n:::\n:::\n\n\n\n\n\n# Poisson Distribution\n\n## Poisson distribution (counts)\n\nUse a Poisson model for the number of events in a fixed interval when:\n\n* Events occur independently\n* The event rate is roughly constant over the interval\n* We are counting events (0, 1, 2, …)\n\n\\\n\n**Notation:**\n\n$$X \\sim \\text{Pois}(\\lambda)$$\n\n**Interpretation:**\n\n* $\\lambda$ = expected number of events per interval (rate × time)\n* Mean = $\\lambda$\n* SD = $\\sqrt{\\lambda}$\n\n\\\n\n## Poisson distribution: when you see it, think Poisson\n\n::: {.callout-note icon=\"false\"}\nPoisson models counts of events in a fixed interval.\n\nAsk yourself:\n\n\"How many times does something happen in a given amount of time or space?\"\n:::\n\nCommon examples:\n\n- Number of ER arrivals per hour\n- Number of emails received per day\n- Number of mutations per gene\n- Number of accidents per mile of highway per year\n\n::: fragment\nKey clues:\n\n- Counting events: 0, 1, 2, 3, …\n- Fixed interval (time, distance, area)\n- Events are relatively rare and independent\n:::\n\n## R functions for Poisson\n\n::: {.callout-tip icon=\"false\"}\n## Four key functions\n\n| Function | Purpose | Example |\n|----------|---------|---------|\n| `dpois()` | **Probability** at a value | P(X = x) |\n| `ppois()` | **Cumulative probability** | P(X ≤ x) |\n| `qpois()` | **Quantile** | What x gives P(X ≤ x) = p? |\n| `rpois()` | **Random** samples | Simulate counts |\n\n**Most common:** `dpois()` and `ppois()`\n:::\n\n## Shape depends on λ\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_normal_poisson_files/figure-revealjs/unnamed-chunk-36-1.png){width=960}\n:::\n:::\n\n\n\n\nAs λ increases:\n\n- The distribution shifts to the right (larger expected counts)\n- The distribution becomes more symmetric\n- For large λ, it starts to look approximately Normal\n\n\n\n\n## Example: Typhoid deaths (λ scaling)\n\nSuppose there are on average 5 deaths per year.\n\n**1. Probability of exactly 3 deaths in 1 year:**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndpois(x = 3, lambda = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1403739\n```\n\n\n:::\n:::\n\n\n\n\n\\\n\n**2. Probability of exactly 2 deaths in 0.5 years:**\n\nRate scales with time, so $\\lambda_{0.5} = 5 \\times 0.5 = 2.5$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndpois(x = 2, lambda = 2.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2565156\n```\n\n\n:::\n:::\n\n\n\n\n\\\n\n**3. Probability of more than 12 deaths in 2 years:**\n\n$\\lambda_{2} = 5 \\times 2 = 10$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nppois(q = 12, lambda = 10, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2084435\n```\n\n\n:::\n\n```{.r .cell-code}\n# or: 1 - ppois(12, lambda = 10)\n```\n:::\n\n\n\n\n## Poisson approximation to Binomial (rare events)\n\nWhen $n$ is large and $p$ is small, a Binomial can be approximated by a Poisson:\n\nIf $X \\sim \\text{Binomial}(n, p)$ and $p$ is small, then\n$$X \\approx \\text{Pois}(\\lambda = np)$$\n\nThis is most useful when the **Normal approximation is not appropriate** (because $p$ is too close to 0).\n\n\\\n\n**Quick example:**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 1000\np <- 0.002\nlambda <- n * p\n\n# Approx P(X <= 3)\npbinom(3, size = n, prob = p)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8573042\n```\n\n\n:::\n\n```{.r .cell-code}\nppois(3, lambda = lambda)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8571235\n```\n\n\n:::\n:::\n\n\n\n\nVery close! (0.857 vs 0.857)\n\n<!-- # Poisson Distribution -->\n\n<!-- ## Introduction to the Poisson distribution -->\n\n<!-- -   The Poisson distribution is often used to model **count data (# of successes)**, especially for **rare events** -->\n\n<!--     -   It is a **discrete distribution!** -->\n\n\n\n<!-- -   It is used most often in settings where **events happen at a rate** $\\lambda$ per unit of population and per unit time -->\n\n<!-- :::: {.columns} -->\n<!-- ::: {.column width=\"50%\"} -->\n<!-- **Example:** Historical records of hospitalizations in New York City indicate that an average of 4.4 people are hospitalized each day for an acute myocardial infarction (AMI) -->\n\n<!-- - We can plot the distribution of hospitalizations on each day -->\n<!-- ::: -->\n\n<!-- ::: {.column width=\"50%\"} -->\n<!-- ```{r} -->\n<!-- #| echo: false -->\n<!-- #| fig-height: 4 -->\n\n<!-- lambda <- 4.4 -->\n<!-- x <- 0:15 -->\n\n<!-- df <- tibble(x = x, prob = dpois(x, lambda = lambda)) -->\n\n<!-- ggplot(df, aes(x = x, y = prob)) + -->\n<!--   geom_col(width = 0.7, fill = \"steelblue\", alpha = 0.7) + -->\n<!--   labs( -->\n<!--     x = \"Number of hospitalizations\", -->\n<!--     y = \"Probability\", -->\n<!--     title = paste0(\"Poisson(λ = \", lambda, \")\") -->\n<!--   ) -->\n<!-- ``` -->\n<!-- ::: -->\n<!-- :::: -->\n\n<!-- ## Poisson distribution -->\n\n<!-- -   Suppose events occur over time in such a way that -->\n\n<!--     1.  The probability an event occurs in an interval is proportional to the length of the interval. -->\n\n<!--     2.  Events occur independently at a rate $\\lambda$ per unit of time. -->\n\n<!-- -   Then the probability of exactly $x$ events in one unit of time is  -->\n<!-- $$P(X = k) = \\frac{e^{-\\lambda}\\lambda^{k}}{k!}, \\,\\, k = 0, 1, 2, \\ldots$$ -->\n\n<!-- -   For the Poisson distribution modeling the number of events in one unit of time: -->\n\n<!--     -   The mean is $\\lambda$. -->\n<!--     -   The standard deviation is $\\sqrt{\\lambda}$. -->\n\n<!-- -   Shorthand for a random variable, $X$, that has a Poisson distribution:  -->\n<!-- $$X \\sim \\text{Pois}(\\lambda)$$ -->\n\n<!-- ## Poisson distribution: R commands -->\n\n<!-- ::: {.callout-tip icon=\"false\"} -->\n<!-- ## Four key functions -->\n\n<!-- | Function | Purpose | Example | -->\n<!-- |----------|---------|---------| -->\n<!-- | `dpois()` | **Probability** at a value | P(X = x) | -->\n<!-- | `ppois()` | **Cumulative probability** | P(X ≤ x) | -->\n<!-- | `qpois()` | **Quantile** | What x gives P(X ≤ x) = p? | -->\n<!-- | `rpois()` | **Random** samples | Generate random Poisson data | -->\n\n<!-- **Most common:** `dpois()` and `ppois()` -->\n<!-- ::: -->\n\n<!-- ## Visualizing Poisson distributions -->\n<!-- ```{r} -->\n<!-- #| echo: false -->\n<!-- #| fig-height: 5 -->\n\n<!-- lambda <- 5 -->\n<!-- x <- 0:20 -->\n\n<!-- df <- tibble(x = x, prob = dpois(x, lambda = lambda)) -->\n\n<!-- ggplot(df, aes(x = x, y = prob)) + -->\n<!--   geom_col(width = 0.7, fill = \"steelblue\", alpha = 0.7) + -->\n<!--   geom_segment(aes(x = x, xend = x, y = 0, yend = prob),  -->\n<!--                linewidth = 0.8, alpha = 0.5) + -->\n<!--   labs( -->\n<!--     x = \"Number of successes (x)\", -->\n<!--     y = \"Probability, P(X = x)\", -->\n<!--     title = \"Poisson distribution with λ = 5\" -->\n<!--   ) + -->\n<!--   scale_x_continuous(breaks = seq(0, 20, 2)) -->\n<!-- ``` -->\n\n<!-- ::: notes -->\n<!-- - The distribution is discrete (bars, not a curve) -->\n<!-- - As λ increases, the distribution becomes more symmetric -->\n<!-- - When λ is large (≥20), it starts to look approximately normal -->\n<!-- ::: -->\n\n<!-- ## Example: Typhoid fever (1/4) -->\n\n<!-- **Scenario:** Suppose there are on average 5 deaths per year from typhoid fever. -->\n\n<!-- **Questions:** -->\n\n<!-- 1.  What is the probability of 3 deaths in a year? -->\n\n<!-- 2.  What is the probability of 2 deaths in 0.5 years? -->\n\n<!-- 3.  What is the probability of more than 12 deaths in 2 years? -->\n\n<!-- ## Example: Typhoid fever (2/4) -->\n\n<!-- **Question 1:** What is the probability of 3 deaths in a year? -->\n\n<!-- - $\\lambda = 5$ and we want $P(X = 3)$ -->\n\n<!-- $$P(X=3) = \\frac{e^{-5}5^{3}}{3!} = 0.1404$$ -->\n<!-- ```{r} -->\n<!-- dpois(x = 3, lambda = 5) -->\n<!-- ``` -->\n\n<!-- ## Example: Typhoid fever (3/4) -->\n\n<!-- **Question 2:** What is the probability of 2 deaths in 0.5 years? -->\n\n<!-- - $\\lambda = 5$ was the rate for **one year** -->\n<!-- - For **half a year**, we need to adjust λ: -->\n\n<!-- $$\\lambda = \\frac{5 \\text{ deaths}}{1 \\text{ year}} \\cdot \\frac{1 \\text{ year}}{2 \\text{ half-years}} = \\frac{2.5 \\text{ deaths}}{1 \\text{ half-year}}$$ -->\n\n<!-- $$P(X=2) = \\frac{e^{-2.5}2.5^{2}}{2!} = 0.2565$$ -->\n<!-- ```{r} -->\n<!-- dpois(x = 2, lambda = 2.5) -->\n<!-- ``` -->\n\n<!-- **Key insight:** λ scales with the time interval! -->\n\n<!-- ## Example: Typhoid fever (4/4) -->\n\n<!-- **Question 3:** What is the probability of more than 12 deaths in 2 years? -->\n\n<!-- - For **two years**, adjust λ: -->\n\n<!-- $$\\lambda = \\frac{5 \\text{ deaths}}{1 \\text{ year}} \\cdot 2 \\text{ years} = 10 \\text{ deaths per 2 years}$$ -->\n\n<!-- $$P(X>12) = 1 - P(X \\leq 12) = 1 - \\sum_{k=0}^{12}\\frac{e^{-10}10^{k}}{k!} = 0.2084$$ -->\n\n<!-- :::: {.columns} -->\n<!-- ::: {.column width=\"50%\"} -->\n<!-- ```{r} -->\n<!-- 1 - ppois(q = 12, lambda = 10) -->\n<!-- ``` -->\n<!-- ::: -->\n\n<!-- ::: {.column width=\"50%\"} -->\n<!-- ```{r} -->\n<!-- ppois(q = 12, lambda = 10,  -->\n<!--       lower.tail = FALSE) -->\n<!-- ``` -->\n<!-- ::: -->\n<!-- :::: -->\n\n<!-- ## Poisson approximation to Binomial -->\n\n<!-- The Poisson distribution can be used to **approximate** the binomial distribution when: -->\n\n<!-- - $n$ is **large** -->\n<!-- - $p$ is **small** -->\n\n<!-- This is useful when the Normal approximation doesn't work (because p is too extreme). -->\n\n<!-- **Approximation:** If $X \\sim \\text{Binomial}(n, p)$ with large n and small p: -->\n\n<!-- $$X \\approx \\text{Pois}(\\lambda = np)$$ -->\n\n<!-- ## Visual: Binomial → Poisson approximation -->\n<!-- ```{r} -->\n<!-- #| echo: false -->\n<!-- #| fig-height: 6 -->\n<!-- #| fig-width: 10 -->\n\n<!-- library(patchwork) -->\n\n<!-- # Small p, varying n -->\n<!-- n_vals <- c(60, 100, 200, 400) -->\n<!-- p <- 0.005 -->\n\n<!-- plots <- lapply(n_vals, function(n) { -->\n<!--   x <- 0:15 -->\n<!--   lambda <- n * p -->\n\n<!--   df <- tibble( -->\n<!--     x = x, -->\n<!--     binomial = dbinom(x, size = n, prob = p), -->\n<!--     poisson = dpois(x, lambda = lambda) -->\n<!--   ) -->\n\n<!--   ggplot(df, aes(x = x)) + -->\n<!--     geom_col(aes(y = binomial), fill = \"steelblue\", alpha = 0.5, width = 0.4, position = position_nudge(x = -0.2)) + -->\n<!--     geom_col(aes(y = poisson), fill = \"coral\", alpha = 0.5, width = 0.4, position = position_nudge(x = 0.2)) + -->\n<!--     labs(title = paste0(\"n = \", n, \", p = \", p), -->\n<!--          x = \"Number of successes\", -->\n<!--          y = \"Probability\") + -->\n<!--     theme(plot.title = element_text(hjust = 0.5, size = 14)) -->\n<!-- }) -->\n\n<!-- (plots[[1]] + plots[[2]]) / (plots[[3]] + plots[[4]]) -->\n<!-- ``` -->\n\n<!-- Blue = Binomial, Orange = Poisson(λ = np). They become nearly identical as n increases! -->\n\n<!-- ## When to use Poisson vs Normal approximation -->\n\n<!-- :::: {.columns} -->\n<!-- ::: {.column width=\"50%\"} -->\n<!-- **Use Poisson approximation when:** -->\n\n<!-- - $n$ is large -->\n<!-- - $p$ is small (close to 0) -->\n<!-- - Events are rare -->\n<!-- - $np < 10$ (Normal approximation fails) -->\n\n<!-- **Example:**  -->\n<!-- - n = 1000, p = 0.002 -->\n<!-- - λ = np = 2 -->\n<!-- ::: -->\n\n<!-- ::: {.column width=\"50%\"} -->\n<!-- **Use Normal approximation when:** -->\n\n<!-- - $n$ is large -->\n<!-- - $p$ is not too extreme -->\n<!-- - $np \\geq 10$ AND $n(1-p) \\geq 10$ -->\n\n<!-- **Example:** -->\n<!-- - n = 100, p = 0.25 -->\n<!-- - Both conditions met ✓ -->\n<!-- ::: -->\n<!-- :::: -->\n\n## Summary: Poisson distribution\n\n**Key characteristics:**\n\n- Models **count data** for rare events\n- Parameter: $\\lambda$ = rate × time\n- Mean = $\\lambda$, SD = $\\sqrt{\\lambda}$\n\n**R functions:**\n\n- `dpois(x, lambda)` → P(X = x)\n- `ppois(q, lambda)` → P(X ≤ q)\n\n**Important:** Remember to adjust λ when changing the time interval!\n\n**Approximation:** Can approximate Binomial when n is large and p is small.\n\n\n\n\n# Summary & Next Steps\n\n## What you need to know: Normal distribution\n\n**Conceptual understanding:**\n\n- Normal distributions are symmetric, bell-shaped, continuous\n- Fully characterized by mean (μ) and standard deviation (σ)\n- Z-scores standardize to N(0, 1): $Z = \\frac{X - \\mu}{\\sigma}$\n\n\\\n\n**Empirical Rule (68-95-99.7):**\n\n- 68% of data within 1 SD, 95% within 2 SDs, 99.7% within 3 SDs\n- Values beyond ±3 SDs are very rare\n\n\\\n\n**R skills:**\n\n- `pnorm(q, mean, sd)` → P(X ≤ q)\n- `qnorm(p, mean, sd)` → value at pth percentile\n- Normal approximation when $np \\geq 10$ AND $n(1-p) \\geq 10$\n\n## What you need to know: Poisson distribution\n\n**When to use Poisson:**\n\n- Counting rare events in a fixed interval\n- Events occur independently at rate λ\n\n\\\n\n**Key concepts:**\n\n- Parameter: λ = rate × time\n- Mean = λ, SD = $\\sqrt{\\lambda}$\n- Adjust λ when changing time intervals\n\n\\\n\n**R skills:**\n\n- `dpois(x, lambda)` → P(X = x)\n- `ppois(q, lambda)` → P(X ≤ q)\n- Poisson approximates Binomial when n is large and p is small\n\n## Key formulas (for reference)\n\nYou don't need to memorize these, but understand what they represent:\n\n\\\n\n**Z-score transformation:**\n$$Z = \\frac{X - \\mu}{\\sigma} \\quad \\text{or} \\quad X = \\mu + Z\\sigma$$\n\n\\\n\n**Normal approximation to Binomial:**\n\nWhen $np \\geq 10$ and $n(1-p) \\geq 10$:\n$$X \\sim \\text{Binomial}(n, p) \\approx N\\left(\\mu = np, \\sigma = \\sqrt{np(1-p)}\\right)$$\n\n\\\n\n**Poisson approximation to Binomial:**\n\nWhen $n$ is large and $p$ is small:\n$$X \\sim \\text{Binomial}(n, p) \\approx \\text{Pois}(\\lambda = np)$$\n\n",
    "supporting": [
      "10_normal_poisson_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}