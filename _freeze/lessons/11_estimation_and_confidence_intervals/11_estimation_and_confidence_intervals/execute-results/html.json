{
  "hash": "baa29abce2841a84fadfe6cbc861a547",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sampling Distributions and Confidence Intervals\"\nsubtitle: \"Textbook Sections 4.1–4.2\"\nauthor: \"Emile Latour, Nicky Wakim, Meike Niederhausen\"\ndate: \"2026-02-02\"\ndate-format: long\nformat:\n  revealjs:\n    theme: \"../../assets/css/reveal-bmsc620_v5.scss\"\n    slide-number: true\n    show-slide-number: all\n    width: 1955\n    height: 1100\n    footer: \"BMSC 620 | Sampling & Confidence Intervals\"\n    html-math-method: mathjax\n    chalkboard: true\nexecute:\n  echo: true\n  warning: false\n  message: false\n  freeze: auto\n---\n\n\n\n\n\n\n## \n\n![[Artwork by @allison_horst](https://allisonhorst.com/)](/img_slides/horst_samples.png){fig-align=\"center\"}\n\n# Learning Objectives \n\nBy the end of today's lecture, you will be able to:\n\n1. Distinguish between population parameters and sample statistics\n2. Explain the concept of sampling variability and the sampling distribution\n3. Apply the Central Limit Theorem to describe the distribution of sample means\n4. Calculate and interpret confidence intervals for a population mean\n5. Understand when to use the t-distribution vs. the normal distribution\n\n## Roadmap for Today\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n**Part 1: Sampling Fundamentals**\n\n- Population parameters vs. sample statistics\n- Point estimates\n- Sampling variability\n\n**Part 2: Sampling Distributions**\n\n- What is a sampling distribution?\n- Properties of the sampling distribution of means\n- Standard error\n\n**Part 3: Central Limit Theorem**\n\n- Statement of the CLT\n- When the CLT applies\n- Applications with R\n:::\n\n::: {.column width=\"50%\"}\n**Part 4: Introduction to Inference**\n\n- From point estimates to interval estimates\n- Confidence intervals: concept and interpretation\n\n**Part 5: Confidence Intervals in Practice**\n\n- CI when σ is known (z-based)\n- CI when σ is unknown (t-based)\n- The t-distribution\n\n**Part 6: Wrap-up**\n\n- Summary\n- Common misconceptions\n- Next steps\n:::\n::::\n\n# Sampling Fundamentals\n\n## Why do we sample?\n\n::: {.callout-note icon=\"false\"}\n## The fundamental challenge of statistics\n\nWe want to learn about a **population**, but we can only observe a **sample**.\n:::\n\n\\\n\n**Populations:**\n\n- Too large to measure everyone\n- Too expensive or time-consuming\n- Sometimes impossible (would you destroy every lightbulb to test lifespan?)\n\n\\\n\n**Samples:**\n\n- Smaller, manageable\n- If chosen properly, can tell us about the population\n- But there's uncertainty...\n\n## From Week 1: Population vs. sample\n\n:::: {.columns}\n::: {.column width=\"48%\"}\n::: {.callout-important icon=\"false\"}\n### (Target) Population\n\n* Group of interest being studied\n* Group from which the sample is selected\n  - studies often have _inclusion_ and/or _exclusion_ criteria\n* Almost always too expensive or logistically impossible to collect data for every case in a population\n:::\n:::\n\n::: {.column width=\"48%\"}\n::: {.callout-tip icon=\"false\"}\n### Sample\n\n* Group on which data are collected\n* A subset (of measurements) from the population\n:::\n:::\n:::\n\n\n- We use information from a sample to learn about the population from which it was drawn.\n- Goal is to get a __representative__ sample of the population: the characteristics of the sample are similar to the characteristics of the population\n\n## Population vs. Sample: Visual\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](11_estimation_and_confidence_intervals_files/figure-revealjs/unnamed-chunk-1-1.png){width=1152}\n:::\n:::\n\n\n\n\n## Population parameters vs. Sample statistics\n\nUnderstanding the notation is crucial for clear statistical thinking.\n\n\\\n\n:::: {.columns}\n::: {.column width=\"48%\"}\n::: {.callout-important icon=\"false\"}\n## Population Parameter\n\nFixed (but unknown) values describing the population\n\n**For the mean:**\n\n- Symbol: $\\mu$ (mu)\n- We want to know it but usually can't measure it\n\n**For standard deviation:**\n\n- Symbol: $\\sigma$ (sigma)\n- Also fixed and unknown\n\n**For proportion:**\n\n- Symbol: $p$ or $\\pi$ (pi)\n:::\n:::\n\n::: {.column width=\"48%\"}\n::: {.callout-tip icon=\"false\"}\n## Sample Statistic\n\nCalculated values from our sample data\n\n**For the mean:**\n\n- Symbol: $\\bar{x}$ (x-bar)\n- Our best guess at $\\mu$\n\n**For standard deviation:**\n\n- Symbol: $s$\n- Our estimate of $\\sigma$\n\n**For proportion:**\n\n- Symbol: $\\hat{p}$ (p-hat)\n:::\n:::\n:::\n\n## What is a point estimate?\n\nA **point estimate** is a single value calculated from sample data used to estimate a population parameter.\n\n\\\n\n**Examples:**\n\n- Sample mean ($\\bar{x}$) estimates population mean ($\\mu$)\n- Sample proportion ($\\hat{p}$) estimates population proportion ($p$)\n- Sample standard deviation ($s$) estimates population SD ($\\sigma$)\n\n\\\n\n::: {.callout-warning icon=\"false\"}\n## The problem with point estimates\n\nThey're just single numbers. They don't tell us:\n\n- How much uncertainty there is\n- How close we might be to the true value\n- Whether our sample was typical or unusual\n:::\n\n## Sampling variability: A demonstration in R (1/2)\n\nLet's see what happens when we take multiple samples from the same population.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a population\npopulation <- tibble(\n  height = rnorm(10000, mean = 65, sd = 3)\n)\n\n# Take 5 samples of size 50\nresults <- tibble(\n  sample_num = 1:5,\n  mean_height = NA_real_  # Initialize with missing values\n)\n\n# Calculate mean for each sample\nfor (i in 1:5) {\n  one_sample <- sample(population$height, size = 50)   # Take a random sample\n  results$mean_height[i] <- mean(one_sample)           # Calculate the mean\n}\n```\n:::\n\n\n\n\n## Sampling variability: A demonstration in R (2/2)\n\nThe results from taking \n\n- 5 random samples, \n- each size 50, \n- from our population of 10,000\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 2\n  sample_num mean_height\n       <int>       <dbl>\n1          1        64.8\n2          2        64.4\n3          3        64.8\n4          4        65.5\n5          5        64.7\n```\n\n\n:::\n:::\n\n\n\n\n\\\n\n**Notice:** Even from the same population, our sample means vary! This is **sampling variability** - it's not error, it's natural variation.\n\n## Visualizing sampling variability (1/3)\n\nWhat if we took many, many samples?\n\n- From the same population size 10,000 with $\\mu = 65$ and $\\sigma = 3$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Take 1000 samples, each of size 50\nmany_samples <- tibble(\n  sample_num = 1:1000,\n  mean_height = NA_real_  # Initialize with missing values\n)\n\n# Calculate mean for each sample\nfor (i in 1:1000) {\n  one_sample <- sample(population$height, size = 50)   # Take a random sample\n  many_samples$mean_height[i] <- mean(one_sample)      # Calculate the mean\n}\n```\n:::\n\n\n\n\n## Visualizing sampling variability (2/3)\n\nWhat if we took many, many samples?\n\n- From the same population size 10,000 with $\\mu = 65$ and $\\sigma = 3$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(many_samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1000    2\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(many_samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  sample_num mean_height\n       <int>       <dbl>\n1          1        64.3\n2          2        65.0\n3          3        65.1\n4          4        65.0\n5          5        63.9\n6          6        63.7\n```\n\n\n:::\n:::\n\n\n\n\n## Visualizing sampling variability (3/3)\n\nWhat if we took many, many samples?\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](11_estimation_and_confidence_intervals_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\n\n# The Sampling Distribution\n\n## What is a sampling distribution?\n\n::: {.callout-note icon=\"false\"}\n## Definition\n\nThe **sampling distribution** of a statistic is the distribution of that statistic's values across all possible samples of a given size from a population.\n:::\n\n\\\n\n**Think of it this way:**\n\n1. Imagine taking a sample of size $n$\n2. Calculate a statistic (like the mean)\n3. Write it down\n4. Repeat steps 1-3 for **all possible samples**\n5. The distribution of those statistics is the sampling distribution\n\n\\\n\n**Key insight:** The sampling distribution tells us how our estimates behave across different samples.\n\n## Three distributions to keep straight\n\n:::: {.columns}\n::: {.column width=\"32%\"}\n::: {.callout-note icon=\"false\"}\n## Population Distribution\n\n- Distribution of the variable in the population\n- Mean: $\\mu$, SD: $\\sigma$\n- **Fixed, but unknown**\n- We never observe this directly\n:::\n:::\n\n::: {.column width=\"32%\"}\n::: {.callout-tip icon=\"false\"}\n## Sample Distribution\n\n- Distribution of the variable in one sample\n- Mean: $\\bar{x}$, SD: $s$\n- **Random** (changes sample to sample)\n- What we actually observe\n:::\n:::\n\n::: {.column width=\"32%\"}\n::: {.callout-important icon=\"false\"}\n## Sampling Distribution\n\n- Distribution of a sample statistic across many samples\n- Mean: $\\mu_{\\bar{X}} = \\mu$, SD (SE): $\\frac{\\sigma}{\\sqrt{n}}$\n- **Theoretical** (describes variability of $\\bar{x}$)\n- Not the distribution of raw data!\n:::\n:::\n::::\n\n\n## Visual: Three distributions\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](11_estimation_and_confidence_intervals_files/figure-revealjs/unnamed-chunk-7-1.png){width=1248}\n:::\n:::\n\n\n\n\n## Why does the standard error exist?\n\nBefore we introduce the formula, let's understand the concept:\n\n\\\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n**The logic:**\n\n- Each random sample produces a slightly different estimate\n- Those estimates vary from sample to sample\n- That variability forms a sampling distribution\n::: \n\n::: {.column width=\"50%\"}\n**The standard error (SE) is:**\n\n- The standard deviation of the sampling distribution\n- A measure of how much a statistic varies across repeated samples\n:::\n::::\n\n\\\n\n::: {.callout-important icon=\"false\"}\n## Key distinction\n\n**Standard error** quantifies **sampling variability**, not data variability.\n\n- Standard deviation ($s$) → spread of data in one sample\n- Standard error ($SE$) → spread of statistics across many samples\n:::\n\n## Standard error: A special name\n\nThe standard deviation of a sampling distribution has a special name:\n\n::: {.callout-tip icon=\"false\"}\n## Standard Error (SE)\n\nThe **standard error** is the standard deviation of a sampling distribution.\n\nFor the sampling distribution of sample means:\n\n$$SE = \\frac{\\sigma}{\\sqrt{n}}$$\n\nwhere $\\sigma$ = population standard deviation and $n$ = sample size\n:::\n\n\n**What does SE tell us?**\n\nThe SE describes how far the sample mean ($\\bar{x}$) is expected to deviate from the true population mean ($\\mu$) across many different random samples of size $n$.\n\n\n**Key properties:**\n\n- Larger samples → smaller SE → more precise estimates\n- SE decreases as $\\sqrt{n}$ increases, not as $n$ (doubling sample size doesn't halve SE.)\n- In practice, we rarely know $\\sigma$, so we use: $SE = \\frac{s}{\\sqrt{n}}$\n\n## When to report SE vs. SD\n\nWhen presenting results, choose based on your goal:\n\n:::: {.columns}\n::: {.column width=\"48%\"}\n::: {.callout-note icon=\"false\"}\n## Report SD when...\n\n**Goal:** Describe the data\n\n**Use:** $\\bar{x} \\pm s$\n\n**Example:** \"Heights were 65.2 ± 3.1 inches\"\n\n**Interpretation:** Shows the spread of individual observations\n:::\n:::\n\n::: {.column width=\"48%\"}\n::: {.callout-tip icon=\"false\"}\n## Report SE when...\n\n**Goal:** Estimate population parameter\n\n**Use:** $\\bar{x} \\pm SE$\n\n**Example:** \"Mean height was 65.2 ± 0.44 inches\"\n\n**Interpretation:** Shows precision of the estimate\n:::\n:::\n::::\n\n\\\n\n::: {.callout-warning icon=\"false\"}\n## Common mistake\n\nDon't report SE to make your data look \"better\" (less variable). Use SD to describe variability in your sample, SE to quantify uncertainty about the population mean.\n:::\n\n\n\n## Why does sample size matter?\n\nLet's see the effect of sample size on the sampling distribution:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](11_estimation_and_confidence_intervals_files/figure-revealjs/unnamed-chunk-8-1.png){width=1152}\n:::\n:::\n\n\n\n\n# Central Limit Theorem\n\n## The Central Limit Theorem (CLT)\n\n::: {.callout-important icon=\"false\"}\n## Central Limit Theorem\n\nFor **sufficiently large** sample sizes, the sampling distribution of the sample mean is approximately normal, **regardless of the shape of the population distribution**.\n\nSpecifically, if we have a random sample of size $n$ from a population with mean $\\mu$ and standard deviation $\\sigma$:\n\n$$\\bar{X} \\sim N\\left(\\mu_{\\bar{X}} = \\mu, \\quad SE = \\frac{\\sigma}{\\sqrt{n}}\\right)$$\n:::\n\n\\\n\n**The key question:** What counts as \"sufficiently large\"?\n\n## When can we use the CLT?\n\nThe required sample size depends on the shape of the population distribution:\n\n:::: {.columns}\n::: {.column width=\"48%\"}\n**Population approximately normal**\n\n- CLT works for **any sample size**\n- Even $n = 5$ is fine\n- The sampling distribution is exactly normal\n\n**Population slightly skewed**\n\n- Usually $n \\geq 30$ is sufficient\n- This is the common \"rule of thumb\"\n:::\n\n::: {.column width=\"48%\"}\n**Population highly skewed**\n\n- May need $n \\geq 50$ or even larger\n- The \"30\" rule doesn't apply here!\n- More skewness → need larger $n$\n\n**Population with extreme outliers**\n\n- May need $n \\geq 100$ or more\n- Outliers slow down convergence to normality\n:::\n::::\n\n\\\n\n::: {.callout-tip icon=\"false\"}\n## In practice\n\nLook at your sample data:\n\n- Is it approximately symmetric with no extreme outliers? → $n \\geq 30$ likely okay\n- Is it very skewed or has outliers? → Consider larger $n$ or non-parametric methods\n:::\n\n## CLT in action: Starting with a skewed population\n\nLet's see what happens when we start with a **highly skewed** population:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](11_estimation_and_confidence_intervals_files/figure-revealjs/unnamed-chunk-9-1.png){width=1248}\n:::\n:::\n\n\n\n\n## Sampling distributions at different sample sizes\n\nNow watch what happens to the sampling distribution as we increase $n$:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](11_estimation_and_confidence_intervals_files/figure-revealjs/unnamed-chunk-10-1.png){width=1248}\n:::\n:::\n\n\n\n\n## Why the CLT is remarkable\n\n**The CLT works even if the population is:**\n\n- Slightly or moderately skewed\n- Uniform  \n- Bimodal\n- Many other non-normal shapes\n\n\\\n\n**The key insight:** Averages are less variable than individual observations, and with enough averaging (large enough $n$), the distribution of those averages becomes normal.\n\n\\\n\n::: {.callout-warning icon=\"false\"}\n## Don't blindly trust n ≥ 30\n\nThe \"$n \\geq 30$\" rule is a rough guideline, not a guarantee. \n\n- For symmetric distributions, 30 is usually plenty\n- For highly skewed distributions (like we just saw), you may need 50, 100, or more\n- Always look at your actual data before trusting the CLT\n:::\n\n## Why is this useful?\n\n\\\n\n- Routine studies involve data from a single sample, not repeated samples.\n- If $n$ is large, then regardless of the distribution of the original population, CLT provides a way of treating our single sample mean as one observation from a normal distribution. \n- The distribution of sample means derived from discrete distributions will also be normal provided $n$ is large.\n\n## Applying the CLT: Example\n\n::: {.callout-note icon=\"false\"}\n## Example: Heights\n\nSuppose the heights of adults in a population have mean $\\mu = 65$ inches and standard deviation $\\sigma = 3.5$ inches. We take a random sample of 50 adults.\n\n**What is the probability that the sample mean (yet to be determined) is greater than 66 inches?**\n:::\n\n\\\n\n**Step 1:** Check if we can use CLT\n\n- $n = 50 \\geq 30$ ✓\n- Heights are generally approximately normal (or at least not heavily skewed) ✓\n- We can assume the sampling distribution of $\\bar{X}$ is approximately normal\n\n**Step 2:** Find the distribution of $\\bar{X}$\n\n$$\\bar{X} \\sim N\\left(\\mu = 65, \\quad SE = \\frac{3.5}{\\sqrt{50}} = 0.495\\right)$$\n\n## Example continued: Using R\n\n**Step 3:** Calculate the probability using R\n\nWe want $P(\\bar{X} > 66)$\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](11_estimation_and_confidence_intervals_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\n\n\n## Example continued: Using R\n\n**Step 3:** Calculate the probability using R\n\nWe want $P(\\bar{X} > 66)$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define parameters\nn <- 50\nmu <- 65\nsigma <- 3.5\n\n# Calculate SE\nSE <- sigma / sqrt(n)\nSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4949747\n```\n\n\n:::\n:::\n\n\n\n\n\\\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate probability\npnorm(q = 66, mean = mu, sd = SE, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02167588\n```\n\n\n:::\n:::\n\n\n\n\n\n\\\n\n**Interpretation:** There is about a 2.2% chance of observing a sample mean greater than 66 inches if the true population mean is 65 inches.\n\n\n\n\n\n## What the CLT tells us in plain language\n\nThe Central Limit Theorem means:\n\n1. **Sample means tend toward normality** (for large enough $n$, even if the data aren't normal)\n2. **Sample means cluster around the population mean** ($\\mu$)\n3. **The spread depends on sample size** (larger $n$ → smaller spread)\n\n\\\n\n**Why this matters:**\n\n- We can use normal distribution tools even when our data aren't normal\n- We can quantify uncertainty about sample means\n- We can make probability statements (like we just did)\n- This is the foundation for confidence intervals and hypothesis tests\n\n\\\n\n::: {.callout-tip icon=\"false\"}\n## Looking ahead\n\nThe CLT is why we can construct confidence intervals and do hypothesis tests even when our data aren't perfectly normal - as long as our sample size is large enough!\n:::\n\n# Introduction to Inference\n\n## From estimation to inference\n\nSo far we've learned:\n\n- Population parameters vs. sample statistics\n- Sampling distributions\n- The Central Limit Theorem\n\n\\\n\n**Now we ask a bigger question:**\n\n::: {.callout-note icon=\"false\"}\n## The inference question\n\nGiven a sample statistic (like $\\bar{x} = 66.1$), what can we say about the population parameter ($\\mu$)?\n:::\n\n\\\n\n**Point estimates aren't enough** - they give us one number but no sense of uncertainty.\n\n**Solution:** Use interval estimates!\n\n## Point estimates vs. Interval estimates\n\n:::: {.columns}\n::: {.column width=\"48%\"}\n::: {.callout-warning icon=\"false\"}\n## Point Estimate\n\nA single value used to estimate a parameter\n\n**Example:** \"The mean height is 66.1 inches\"\n\n**Pros:**\n\n- Simple\n- Easy to communicate\n\n**Cons:**\n\n- No uncertainty quantified\n- Doesn't acknowledge sampling variability\n:::\n:::\n\n::: {.column width=\"48%\"}\n::: {.callout-tip icon=\"false\"}\n## Interval Estimate\n\nA range of plausible values for a parameter\n\n**Example:** \"The mean height is between 65.1 and 67.1 inches\"\n\n**Pros:**\n\n- Quantifies uncertainty\n- More honest about what we know\n\n**Cons:**\n\n- Less precise\n- Requires interpretation\n:::\n:::\n::::\n\n## What is a confidence interval?\n\n::: {.callout-important icon=\"false\"}\n## Confidence Interval\n\nA **confidence interval** is a range of values that is likely to contain the true population parameter with a specified level of confidence.\n\n**General form:**\n\n$$\\text{point estimate} \\pm \\text{margin of error}$$\n\nFor a mean:\n\n$$\\bar{x} \\pm \\text{(critical value)} \\times SE$$\n:::\n\n\\\n\n**The critical value depends on:**\n\n- The confidence level (commonly 95%)\n- The distribution we're using (normal or t)\n\n## Some new notation\n\nBefore we construct confidence intervals, we need to understand the notation for critical values:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](11_estimation_and_confidence_intervals_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n\n\n\\\n\n- $\\pm z_{1-\\alpha/2}$ is the value of $z$ such that $(1 - \\alpha) \\times 100\\%$ of the standard normal distribution is contained <br>between $- z_{1-\\alpha/2}$ and $+ z_{1-\\alpha/2}$.\n- Equivalently, $\\alpha \\times 100\\%$ is greater than $+ z_{1-\\alpha/2}$ and less than $- z_{1-\\alpha/2}$ combined. \n\n# Confidence Intervals: The Basics\n\n## Visualizing confidence intervals (1/2)\n\nLet's look at what confidence intervals represent:\n\n::::::: columns\n:::: {.column width=\"55%\"}\n\n\n<!-- Simulating Confidence Intervals: <http://www.rossmanchance.com/applets/ConfSim.html> -->\n\nThe figure shows CIs from 100 samples:\n\n-   100 samples: Calculate the mean and confidence interval of each sample\n-   The true value of $\\mu =65$ is the vertical black line\n-   The horizontal lines are 95% CIs from 100 samples\n    -   [**Blue**]{style=\"color:#0072B2\"}: the CI contains the true value of $\\mu$\n    -   [**Red**]{style=\"color:#C83532\"}: the CI *did not* contain the true value of $\\mu$\n\n\\n\n\n**What percent of CIs captured the true value of $\\mu$?**\n\n::::\n\n::: {.column width=\"10%\"}\n:::\n\n::: {.column width=\"25%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](11_estimation_and_confidence_intervals_files/figure-revealjs/unnamed-chunk-15-1.png){width=480}\n:::\n:::\n\n\n\n:::\n:::::::\n\n\n## Visualizing confidence intervals (2/2)\n\nLet's look at what confidence intervals represent:\n\n::::::: columns\n:::: {.column width=\"55%\"}\n\n\n\\\n\n#### Interpretation $(1 - \\alpha) \\times 100\\%$\n\n\\\n\nIf many samples are collected from a population, and a confidence interval is calculated for each one.\n\n\\\n\nWe expect that $(1 - \\alpha) \\times 100\\%$ of those intervals will contain the true population mean, $\\mu$.\n\n::::\n\n::: {.column width=\"10%\"}\n:::\n\n::: {.column width=\"25%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](11_estimation_and_confidence_intervals_files/figure-revealjs/unnamed-chunk-16-1.png){width=480}\n:::\n:::\n\n\n\n:::\n:::::::\n\n## How do we interpret confidence intervals? \n\n**Actual interpretation:**\n\n-   If we were to\n    -   **repeatedly take random samples** from a population and\n    -   calculate a 95% CI for each random sample,\n-   then we would **expect 95% of our CIs to contain the true population parameter** $\\mu$.\n\n<!-- \"Real life\": -->\n\n<!-- * We typically only take 1 random sample.   -->\n\n<!-- * How do we know if our CI is a lucky or unlucky one? -->\n\n\\\n\n**What we typically write as \"shorthand\":**\n\n-   In general form: We are 95% *confident* that (the 95% confidence interval) captures the value of the population parameter.\n\n\\\n\n**WRONG interpretation:**\n\n-   There is a 95% *chance* that (the 95% confidence interval) captures the value of the population parameter.\n    -   For one CI on its own, it either does or doesn't contain the population parameter with probability 0 or 1. We just don't know which!\n\n\n\n## Confidence interval when σ is known\n\nWhen we **know** the population standard deviation $\\sigma$:\n\n::: {.callout-tip icon=\"false\"}\n## CI for μ (with known σ)\n\n$$\\bar{x} \\pm z^* \\times \\frac{\\sigma}{\\sqrt{n}}$$\n\nwhere:\n\n- $\\bar{x}$ = sample mean\n- $z^*$ = critical value from standard normal distribution\n- $\\sigma$ = population standard deviation (known)\n- $n$ = sample size\n:::\n\n\\\n\n**For a 95% confidence interval:**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(0.975)  # 2.5% in each tail\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.959964\n```\n\n\n:::\n:::\n\n\n\n\nSo $z^* = 1.96$\n\n## What makes a confidence interval wide or narrow?\n\nBefore we calculate CIs, let's build intuition about what affects their width:\n\n\\\n\n:::: {.columns}\n::: {.column width=\"48%\"}\n**CI gets narrower when:**\n\n- Sample size increases  \n  ($\\uparrow n$ → $\\downarrow SE$)\n- Population variability is smaller  \n  ($\\downarrow \\sigma$ → $\\downarrow SE$)\n:::\n\n::: {.column width=\"48%\"}\n**CI gets wider when:**\n\n- Sample size is small  \n  ($\\downarrow n$ → $\\uparrow SE$)\n- Population variability is large  \n  ($\\uparrow \\sigma$ → $\\uparrow SE$)\n- Confidence level increases  \n  (99% vs 95% → larger critical value)\n:::\n::::\n\n\\\n\n::: {.callout-tip icon=\"false\"}\n## Nothing else affects CI width\n\nYou can only make a CI narrower by:\n\n1. Collecting more data\n2. Reducing measurement error  \n3. Accepting less confidence\n:::\n\n## Example: CI with known σ\n\n::: {.callout-note icon=\"false\"}\n## Example\n\n- A random sample of 50 adults has mean height $\\bar{x} = 66.1$ inches. \n- Assume the population standard deviation is known to be $\\sigma = 3$ inches.\n- Find a 95% confidence interval for the population mean height.\n:::\n\n::: {.columns}\n::: {.column width=\"48%\"}\n**Solution:**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxbar <- 66.1\nsigma <- 3\nn <- 50\nz_star <- qnorm(0.975)  # 1.96\n\n\n# Calculate SE\nSE <- sigma / sqrt(n)\nSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4242641\n```\n\n\n:::\n:::\n\n\n\n\n:::\n::: {.column width=\"48%\"}\n\n\\\n   \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate CI\nlower_ci <- xbar - z_star * SE\nupper_ci <- xbar + z_star * SE\n\nc(lower_ci, upper_ci)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 65.26846 66.93154\n```\n\n\n:::\n:::\n\n\n\n\n:::\n:::\n\n\\\n\n**We are 95% confident that the population mean height is between 65.27 and 66.93 inches.**\n\n## Interpreting confidence intervals: What they mean\n\n::: {.callout-important icon=\"false\"}\n## Correct interpretation\n\n\"We are 95% confident that the interval (65.27, 66.93) contains the true population mean height.\"\n\n**What this really means:**\n\nIf we were to take many samples and construct a 95% CI from each one, about 95% of those intervals would contain the true population mean $\\mu$.\n:::\n\n\\\n\n**Helpful analogy:**\n\nThink of each CI as a \"net\" trying to catch the true parameter. With 95% confidence, our net catches the parameter 95% of the time.\n\n## Interpreting confidence intervals: What they DON'T mean\n\n::: {.callout-warning icon=\"false\"}\n## Common misconceptions\n\n**WRONG:** \"There is a 95% probability that μ is in this interval.\"\n\n- The parameter μ is fixed (not random)\n- It either is or isn't in the interval\n- The randomness comes from the sampling process\n\n**WRONG:** \"95% of the data falls in this interval.\"\n\n- The CI is about the parameter, not the data\n- The data is in the sample, not in the CI\n\n**WRONG:** \"If we repeat the study, there's a 95% chance the new mean will be in this interval.\"\n\n- CIs are for parameters, not future statistics\n:::\n\n## What a 95% confidence interval actually means\n\nThis is the most important slide about interpretation:\n\n\\\n\n::: {.callout-important icon=\"false\"}\n## The key to understanding CIs\n\n**The method** used to create the interval has 95% long-run coverage.\n\nIf we repeated the study many times:\n\n- Each time, we'd get a different sample\n- Each sample would produce a different confidence interval\n- About 95% of those intervals would contain the true parameter $\\mu$\n:::\n\n\n::: {.callout-warning icon=\"false\"}\n## The critical insight\n\n**The parameter is fixed. The interval is random.**\n\nThe confidence is about the *procedure*, not about any single interval.\n\nYou cannot assess whether a specific CI is \"correct\" using just one dataset. The 95% guarantee comes from the long-run behavior of the method.\n:::\n\n## Different confidence levels\n\nWe can construct CIs at different confidence levels:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxbar <- 66.1\nsigma <- 3\nn <- 50\n\n# Calculate SE\nSE <- sigma / sqrt(n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 90% CI\nz_90 <- qnorm(0.95)  # 5% in each tail\nc(xbar - z_90 * SE, xbar + z_90 * SE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 65.40215 66.79785\n```\n\n\n:::\n:::\n\n\n\n\n\\\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 95% CI\nz_95 <- qnorm(0.975)  # 2.5% in each tail\nc(xbar - z_95 * SE, xbar + z_95 * SE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 65.26846 66.93154\n```\n\n\n:::\n:::\n\n\n\n\n\\\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 99% CI\nz_99 <- qnorm(0.995)  # 0.5% in each tail\nc(xbar - z_99 * SE, xbar + z_99 * SE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 65.00717 67.19283\n```\n\n\n:::\n:::\n\n\n\n\n\\\n\n**Trade-off:** Higher confidence → wider interval (less precision)\n\n**Notice:** \n- 90% CI: (65.41, 66.79) - narrowest, but least confident\n- 99% CI: (65.00, 67.20) - widest, but most confident\n\n## Different confidence levels (a different way)\n\nJust showing another way to do with with R\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxbar <- 66.1\nsigma <- 3\nn <- 50\nSE <- sigma / sqrt(n)\n\n# Instead of three separate calculations:\nconfidence_levels <- c(0.90, 0.95, 0.99)\nz_values <- qnorm(1 - (1 - confidence_levels)/2)\n\nresults <- tibble(\n  level = confidence_levels,\n  z_star = z_values,\n  lower = xbar - z_values * SE,\n  upper = xbar + z_values * SE\n)\n\nresults\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  level z_star lower upper\n  <dbl>  <dbl> <dbl> <dbl>\n1  0.9    1.64  65.4  66.8\n2  0.95   1.96  65.3  66.9\n3  0.99   2.58  65.0  67.2\n```\n\n\n:::\n:::\n\n\n\n\n\n## Confidence intervals are about procedures\n\nLet's emphasize the key conceptual point before moving on:\n\n\\\n\n:::: {.columns}\n::: {.column width=\"48%\"}\n**One dataset → one confidence interval**\n\n- You conduct one study\n- You get one sample\n- You calculate one interval\n- That interval either contains $\\mu$ or it doesn't\n\n**We just don't know which!**\n:::\n\n::: {.column width=\"48%\"}\n**The guarantee applies to the method, not a single interval**\n\n- The 95% comes from the procedure's long-run behavior\n- If everyone repeated your study, 95% of their CIs would contain $\\mu$\n- Your specific CI is one realization from that process\n:::\n::::\n\n\\\n\n::: {.callout-note icon=\"false\"}\n## Coverage is a long-run property\n\nYou **cannot** assess CI correctness using one dataset.\n\nConfidence comes from the repetition (in principle), not from the data alone.\n\nThis is why we say \"we are confident\" rather than \"there is a probability.\"\n:::\n\n\n# The t-Distribution\n\n## What if we don't know σ?\n\n\\\n\n**Reality check:** We almost never know the population standard deviation $\\sigma$.\n\n\\\n\n**Problem:** If we replace $\\sigma$ with $s$ in our CI formula:\n\n$$\\bar{x} \\pm z^* \\times \\frac{s}{\\sqrt{n}}$$\n\nThis **adds extra uncertainty** - we're now estimating both $\\mu$ and $\\sigma$!\n\n\\\n\n**Solution:** Use a different distribution that accounts for this extra uncertainty - the **t-distribution**.\n\n## The t-distribution\n\n\\\n\n::: columns\n::: {.column width=\"40%\"}\n\n::: {.callout-important icon=\"false\"}\n### Student's t-distribution\n\n- Is symmetric and bell-shaped (like the normal)\n- Has **heavier tails** than the normal distribution\n  - t-based intervals will be wider than Z based intervals\n- Depends on **degrees of freedom** (which for one sample: $df = n - 1$)\n- Approaches the normal distribution as $df$ increases\n:::\n:::\n\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](11_estimation_and_confidence_intervals_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n\n\n\n## Why degrees of freedom = n - 1?\n\n\\\n\n**Degrees of freedom** = number of independent pieces of information\n\n\\\n\nWhen calculating the sample standard deviation $s$:\n\n- We use $n$ observations\n- But we first calculate $\\bar{x}$ (which uses all $n$ values)\n- This \"uses up\" one degree of freedom\n- We're left with $n - 1$ independent pieces of information\n\n\\\n\n**Intuition:** If you know the mean and $n-1$ values, the $n$th value is determined.\n\n## Confidence interval with unknown σ\n\nWhen $\\sigma$ is **unknown** (which is almost always):\n\n::: {.callout-tip icon=\"false\"}\n## CI for μ (with unknown σ)\n\n$$\\bar{x} \\pm t^* \\times \\frac{s}{\\sqrt{n}}$$\n\nwhere:\n\n- $\\bar{x}$ = sample mean\n- $t^*$ = critical value from t-distribution with $df = n - 1$\n- $s$ = sample standard deviation\n- $n$ = sample size\n:::\n\n\\\n\n## `qt()`: Finding the critical value in R\n\nThe `qt()` function finds critical values from the t-distribution:\n```r\nqt(p, df, lower.tail = TRUE)\n```\n\n\\\n\n**Parameters:**\n\n- `p` = cumulative probability (e.g., 0.975 for 95% CI)\n- `df` = degrees of freedom ($n - 1$)\n- `lower.tail` = TRUE (default) gives left-tail probability\n\n\\\n\n**Returns:** The **t-value** where $P(T \\leq \\text{value}) = p$\n\n\\\n\n. . .\n\n\n**Example: 95% CI with n = 50**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For 95% CI, we want 2.5% in each tail, so p = 0.975\n# Degrees of freedom: df = n - 1 = 49\nqt(p = 0.975, df = 49)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.009575\n```\n\n\n:::\n:::\n\n\n\n\nCompare to $z^* = 1.96$ from the normal distribution - the t-value is slightly larger!\n\n## Example: CI with unknown σ\n\n::: {.callout-note icon=\"false\"}\n## Example\n\nA random sample of 50 adults has:\n\n- Mean height: $\\bar{x} = 66.1$ inches\n- Sample SD: $s = 3.5$ inches\n\nFind a 95% confidence interval for the population mean height.\n:::\n\n\\\n\n**Solution:**\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxbar <- 66.1\ns <- 3.5\nn <- 50\ndf <- n - 1\n\n# Critical value\nt_star <- qt(0.975, df = df)\nt_star\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.009575\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate SE (using s instead of σ)\nSE <- s / sqrt(n)\n\n# Calculate CI\nlower_ci <- xbar - t_star * SE\nupper_ci <- xbar + t_star * SE\n\nc(lower_ci, upper_ci)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 65.10531 67.09469\n```\n\n\n:::\n:::\n\n\n\n\n:::\n:::\n\n## Confidence interval (CI) for the mean $\\mu$ ($z$ vs. $t$)\n\n- In summary, we have two cases that lead to different ways to calculate the confidence interval\n\n:::: {.columns}\n::: {.column width=\"48%\"}\n::: {.callout-warning icon=\"false\"}\n### Case 1: We know the population standard deviation\n\n$$\\overline{x}\\ \\pm\\ z^*\\times \\text{SE}$$\n\n- with $\\text{SE} = \\frac{\\sigma}{\\sqrt{n}}$ and $\\sigma$ is the population standard deviation\n\n\\\n\n- For 95% CI, we use:\n  - $z^* =$ `qnorm(p = 0.975)` $=1.96$\n:::\n:::\n\n::: {.column width=\"48%\"}\n::: {.callout-tip icon=\"false\"}\n### Case 2: We **do not** know the population sd\n\n$$\\overline{x}\\ \\pm\\ t^*\\times \\text{SE}$$\n\n- with $\\text{SE} = \\frac{s}{\\sqrt{n}}$ and $s$ is the sample standard deviation\n\n\\\n\n- For 95% CI, we use:\n  - $t^* =$ `qt(p = 0.975, df = n-1)`\n:::\n:::\n::::\n\n\n## Comparing z-based vs. t-based CIs\n\nFor our example ($n = 50$):\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n::: {.callout-warning icon=\"false\"}\n### Case 1: We know the population standard deviation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# If we knew σ = 3.5 (z-based CI)\nz_star <- qnorm(0.975)\nSE <- (3.5 / sqrt(n))\n\nci_z <- xbar + c(-1, 1) * z_star * SE\nci_z\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 65.12987 67.07013\n```\n\n\n:::\n:::\n\n\n\n\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: {.callout-tip icon=\"false\"}\n### Case 2: We **do not** know the population sd\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using s = 3.5 (t-based CI)\nt_star <- qt(0.975, df = 49)\nSE <- (s / sqrt(n))\n\nci_t <- xbar + c(-1, 1) * t_star * SE\nci_t\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 65.10531 67.09469\n```\n\n\n:::\n:::\n\n\n\n:::\n:::\n::::\n\n\\\n\n**Notice:** The t-based CI is slightly wider (because $t^* > z^*$) - this reflects the extra uncertainty from estimating σ.\n\n\n## When to use t vs. z?\n\n::: {.callout-important icon=\"false\"}\n## Decision rule\n\n**Use t-distribution when:**\n\n- You don't know the population standard deviation $\\sigma$\n- You're using the sample standard deviation $s$\n- **(This is almost always in practice!)**\n\n**Use normal (z) distribution when:**\n\n- You know the population standard deviation $\\sigma$\n- **(This is rare in real applications)**\n:::\n\n\\\n\n**Rule of thumb we'll use in this class:**\n\nAlways use the t-distribution unless explicitly told you know $\\sigma$.\n\n# Summary and Key Takeaways\n\n## What you need to know: Sampling distributions\n\n**Key concepts:**\n\n1. **Sampling variability** is natural - different samples give different estimates\n2. The **sampling distribution** describes how statistics vary across samples\n3. **Standard error** (SE) measures the variability of sample means: $SE = \\frac{\\sigma}{\\sqrt{n}}$\n4. The **Central Limit Theorem** says that for $n \\geq 30$, sample means follow approximately normal (often for n >= 30, depending on skew/outliers)\n\n\\\n\n**In plain language:**\n\nIf we repeatedly sample from a population and calculate the mean each time, those means will form a normal distribution centered at the true population mean, with spread determined by the standard error.\n\n## What you need to know: Confidence intervals\n\n**Key concepts:**\n\n1. A **confidence interval** gives a range of plausible values for a parameter\n2. **95% confidence** means that 95% of such intervals would contain the true parameter\n3. Use the **t-distribution** when $\\sigma$ is unknown (almost always)\n4. General form: $\\bar{x} \\pm t^* \\times \\frac{s}{\\sqrt{n}}$\n\n\\\n\n**Critical R functions:**\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Finding critical values\nqt(0.975, df = n - 1)  # For 95% CI\n\n# Or for different confidence levels\nqt(0.95, df = n - 1)   # For 90% CI\nqt(0.995, df = n - 1)  # For 99% CI\n```\n:::\n\n\n\n\n## Common mistakes to avoid\n\n::: {.callout-warning icon=\"false\"}\n## Watch out for these!\n\n1. **Confusing the three distributions**\n   - Population distribution ≠ sample distribution ≠ sampling distribution\n\n2. **Misinterpreting confidence intervals**\n   - Not \"95% chance μ is in the interval\"\n   - Rather \"95% of such intervals contain μ\"\n\n3. **Using z when you should use t**\n   - If you calculated $s$ from your data, use t!\n\n4. **Forgetting the assumptions**\n   - CLT needs $n \\geq 30$ (or normal population)\n   - Or: smaller $n$ is okay if data is approximately symmetric\n:::\n\n## Key formulas for reference\n\nYou don't need to memorize these, but understand what they mean:\n\n\\\n\n**Standard Error:**\n$$SE = \\frac{\\sigma}{\\sqrt{n}} \\quad \\text{or} \\quad SE = \\frac{s}{\\sqrt{n}}$$\n\n\\\n\n**Confidence Interval (t-based):**\n$$\\bar{x} \\pm t^* \\times \\frac{s}{\\sqrt{n}}$$\n\nwhere $t^*$ comes from a t-distribution with $df = n - 1$\n\n\\\n\n**Confidence Interval (z-based, if σ known):**\n$$\\bar{x} \\pm z^* \\times \\frac{\\sigma}{\\sqrt{n}}$$\n\n## The inference pipeline\n\nLet's tie everything together:\n\n\\\n\n::: {.callout-important icon=\"false\"}\n## From population to inference\n\n**Population** → **Sample** → **Statistic** → **Sampling distribution** → **Confidence interval**\n:::\n\n\\\n\n**The process:**\n\n1. **Population** with unknown parameter $\\mu$\n2. Take a random **sample** of size $n$\n3. Calculate a **statistic** (e.g., $\\bar{x}$)\n4. Use the **sampling distribution** to understand variability\n5. Construct a **confidence interval** to quantify uncertainty\n\n\\\n\n**Key insights:** We never observe the population directly, so we use sampling distributions to quantify uncertainty and construct plausible ranges for parameters.\n\n## Looking ahead\n\n**Next time:**\n\n- More practice with confidence intervals\n- Introduction to hypothesis testing\n- The logic of statistical inference\n\n\\\n\n**For now:**\n\n- Practice calculating CIs with different confidence levels\n- Get comfortable with the t-distribution in R\n- Work on understanding (not just calculating) what CIs mean\n\n\\\n\n::: {.callout-tip icon=\"false\"}\n## Remember\n\nStatistical inference is about quantifying uncertainty. Confidence intervals give us a principled way to say \"we don't know exactly, but here's a plausible range.\"\n:::\n",
    "supporting": [
      "11_estimation_and_confidence_intervals_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}