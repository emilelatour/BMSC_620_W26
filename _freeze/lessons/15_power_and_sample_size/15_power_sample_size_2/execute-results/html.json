{
  "hash": "f7f0d2abcec6d6400a5a8a3d9c3e4e98",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Power and Sample Size\"\nsubtitle: \"Textbook Section 5.4\"\nauthor: \"Emile Latour, Nicky Wakim, Meike Niederhausen\"\ndate: \"2026-02-18\"\ndate-format: long\nformat:\n  revealjs:\n    theme: \"../../assets/css/reveal-bmsc620_v5.scss\"\n    slide-number: true\n    show-slide-number: all\n    width: 1955\n    height: 1100\n    footer: \"BMSC 620 | Power and Sample Size\"\n    html-math-method: mathjax\n    chalkboard: true\nexecute:\n  echo: true\n  warning: false\n  message: false\n  freeze: auto\n---\n\n\n\n\n\n# Learning Objectives\n\nBy the end of today's lecture, you will be able to:\n\n1.  Understand the four components in equilibrium in a hypothesis test\n2.  Define and interpret Type I and Type II errors\n3.  Define power and understand its role in study design\n4.  Calculate power and sample size using R for one-sample t-tests\n5.  Calculate power and sample size using R for two-sample t-tests\n\n## Roadmap for Today\n\n::::: columns\n::: {.column width=\"50%\"}\n**Part 1: The Four Components**\n\n-   What affects our ability to detect effects?\n-   Significance level ($\\alpha$)\n-   Sample size ($n$)\n-   Effect size\n-   Power ($1-\\beta$)\n\n**Part 2: Errors in Hypothesis Testing**\n\n-   Type I errors ($\\alpha$)\n-   Type II errors ($\\beta$)\n-   Power as \"correct detection\"\n-   Visualizing errors with distributions\n:::\n\n::: {.column width=\"50%\"}\n**Part 3: Calculating Power and Sample Size**\n\n-   Introduction to Cohen's *d*\n-   Using the `pwr` package in R\n-   One-sample t-test examples\n-   Two-sample t-test examples\n\n**Part 4: Study Design Applications**\n\n-   Planning studies with power in mind\n-   Interpreting existing study results\n-   Trade-offs and practical considerations\n:::\n:::::\n\n# Connecting to What We Know\n\n## Where we've been: Hypothesis testing\n\n**Over the past few lectures, we've learned to:**\n\n-   Use **confidence intervals** to estimate population parameters\n-   Conduct **hypothesis tests** to evaluate claims about parameters\n-   Work with three types of t-tests:\n    -   One-sample (compare to known value)\n    -   Paired (compare before/after)\n    -   Two independent samples (compare groups)\n\n\\\n\n**In each case, we:**\n\n1.  Collected data\n2.  Calculated a test statistic\n3.  Got a p-value\n4.  Made a conclusion (reject $H_0$ or fail to reject)\n\n## Quick reference: Everything we've covered {.smaller}\n\n\\\n\n|   | One-sample | Independent two-sample | Paired sample |\n|------------------|------------------|------------------|------------------|\n| Example | Body temp: Population mean is 98.6°F, is sample different? | Caffeine: taps/min between caffeine and non-caffeine group | Vegetarian diet: cholesterol before and after |\n| Sample statistic | $\\bar{x}$ | $\\bar{x}_1 - \\bar{x}_2$ | $\\bar{x}_d$ |\n| Population parameter | $\\mu$ | $\\mu_1 - \\mu_2$ | $\\mu_d$ or $\\delta$ |\n| Possible hypothesis tests | $H_0: \\mu = \\mu_0$ | $H_0: \\mu_1 = \\mu_2$ or $\\mu_1 - \\mu_2 = 0$ | $H_0: \\mu_d = 0$ or $\\delta = 0$ |\n| Standard error | $SE = \\frac{s}{\\sqrt{n}}$ | $SE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$ | $SE = \\frac{s_d}{\\sqrt{n}}$ |\n| Test statistic | $t = \\frac{\\bar{x} - \\mu_0}{SE}$ | $t = \\frac{(\\bar{x}_1 - \\bar{x}_2) - 0}{SE}$ | $t = \\frac{\\bar{x}_d - 0}{SE}$ |\n| Confidence intervals | $\\bar{x} \\pm t^* \\times SE$ | $(\\bar{x}_1 - \\bar{x}_2) \\pm t^* \\times SE$ | $\\bar{x}_d \\pm t^* \\times SE$ |\n\n\\\n\n*This summarizes all three test types - we'll use these concepts as we think about power*\n\n\n## The question we haven't answered yet\n\n\\\n\n**Scenario:** You're planning a new study\n\n\\\n\n::::: columns\n::: {.column width=\"48%\"}\n**Questions you need to answer:**\n\n-   How many participants do I need?\n-   Can I detect the effect I'm looking for?\n-   What if I can only recruit 20 people?\n-   Is my study worth running?\n:::\n\n::: {.column width=\"48%\"}\n**This is about study design:**\n\n-   *Before* collecting data\n-   *Before* spending time/money\n-   *Before* asking participants to volunteer\n-   Making sure your study can answer your question\n:::\n:::::\n\n\\\n\n**Today:** We learn how to design studies with adequate **power** to detect real effects\n\n## Motivating example: The caffeine study\n\n**Research Question:** Does caffeine increase finger tapping speed?\n\n\\\n\n**Study Design:**\n\n-   70 college students trained to tap fingers rapidly\n-   Randomly assigned to two groups:\n    -   **Control group:** Decaffeinated coffee (n=35)\n    -   **Caffeine group:** Coffee with 200mg caffeine (n=35)\n-   After 2 hours, measure taps per minute\n\n\\\n\n**Results:** The caffeine group had significantly higher taps/min (p < 0.001)\n\n## The questions we SHOULD have asked first\n\n**But what if we had asked BEFORE running the study:**\n\n-   \"We can only recruit 20 per group - is that enough?\"\n-   \"What difference can we actually detect with this sample size?\"\n-   \"How likely are we to find an effect if it's really there?\"\n\n\\\n\n**These questions are about POWER** - and that's what we'll learn today\n\n\\\n\n::: {.callout-note}\nPower analysis happens *before* data collection, not after!\n:::\n\n# Part 1: The Four Components\n\n## What affects our ability to detect an effect?\n\n**Scenario:** Imagine two populations that differ in their mean tapping speed\n\n\\\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15_power_sample_size_2_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n\n\\\n\n**Question:** When we take samples from these groups, will it be easy to tell them apart?\n\n**Answer:** It depends on several factors...\n\n## Scenario 1: Small standard deviation\n\n**Same difference (5 taps/min), but less variability (SD = 1 instead of 2)**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15_power_sample_size_2_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n\n\\\n\n**Observation:** Less overlap between distributions makes the difference easier to detect!\n\n## Scenario 2: Larger difference\n\n**Larger difference (10 taps/min instead of 5), same variability (SD = 2)**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15_power_sample_size_2_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n\n\\\n\n**Observation:** A larger true difference is easier to detect!\n\n## The four components in equilibrium\n\n**In any hypothesis test, four quantities are mathematically related:**\n\n\\\n\n::::::::: columns\n::::: {.column width=\"48%\"}\n::: {.callout-note icon=\"false\"}\n## 1. Significance level ($\\alpha$)\n\n-   Probability of Type I error\n-   Usually set at 0.05\n-   Set *before* collecting data\n-   Determines rejection region\n:::\n\n::: {.callout-note icon=\"false\"}\n## 2. Sample size ($n$)\n\n-   Number of observations\n-   Larger $n$ → smaller SE\n-   Often what we're trying to determine\n-   Constrained by time, cost, ethics\n:::\n:::::\n\n::::: {.column width=\"48%\"}\n::: {.callout-note icon=\"false\"}\n## 3. Effect size\n\n-   Difference we want to detect\n-   Relative to variability\n-   Often measured by Cohen's *d*\n-   Based on pilot data or literature\n:::\n\n::: {.callout-note icon=\"false\"}\n## 4. Power ($1-\\beta$)\n\n-   Probability of detecting a real effect\n-   Typically want 80% or 90%\n-   What we calculate given the others\n-   The probability of \"correctly rejecting\"\n:::\n:::::\n:::::::::\n\n\\\n\n**Key insight:** If you fix any three of these, the fourth is determined!\n\n# Part 2: Errors in Hypothesis Testing\n\n## Type I and Type II errors\n\n**Remember:** Hypothesis tests can make mistakes in two ways\n\n\\\n\n::::::: columns\n:::: {.column width=\"48%\"}\n::: {.callout-important icon=\"false\"}\n## Type I Error (False Positive)\n\n**Definition:** Reject $H_0$ when it's actually true\n\n**Notation:** Probability = $\\alpha$\n\n**Example:** Conclude caffeine increases tapping when it really doesn't\n\n**Control:** Set $\\alpha$ before study (usually 0.05)\n\n**Also called:** False positive, $\\alpha$ error\n:::\n::::\n\n:::: {.column width=\"48%\"}\n::: {.callout-warning icon=\"false\"}\n## Type II Error (False Negative)\n\n**Definition:** Fail to reject $H_0$ when it's false\n\n**Notation:** Probability = $\\beta$\n\n**Example:** Fail to detect caffeine effect when it really exists\n\n**Control:** Increase sample size, decrease variability\n\n**Also called:** False negative, $\\beta$ error\n:::\n::::\n:::::::\n\n\\\n\n**Trade-off:** Decreasing one type of error often increases the other!\n\n## Power: The probability of correctly detecting an effect\n\n::: {.callout-tip icon=\"false\"}\n## Power Definition\n\n**Power** = $1 - \\beta$ = Probability of correctly rejecting $H_0$ when it's false\n\n-   Also called \"sensitivity\" or \"true positive rate\"\n-   The probability of detecting an effect that actually exists\n-   Typically want power ≥ 0.80 (80%)\n-   Some fields require 0.90 (90%)\n:::\n\n\\\n\n**Why 80%?** This is a convention balancing:\n\n-   Reasonable chance of detecting real effects\n-   Practical constraints on sample size\n-   Cost and feasibility\n\n\\\n\n**Think of it this way:**\n\n-   High power = good \"detector\" - likely to find effect if it exists\n-   Low power = bad \"detector\" - might miss real effects\n\n## The complete picture\n\n| **Reality**              | $H_0$ True              | $H_0$ False             |\n|---------------------|-------------------------|---------------------------|\n| **Reject** $H_0$         | Type I Error ($\\alpha$) | **Power** ($1-\\beta$) ✓ |\n| **Fail to reject** $H_0$ | Correct ($1-\\alpha$) ✓  | Type II Error ($\\beta$) |\n\n\\\n\n**Correct decisions:**\n\n-   $1 - \\alpha$: Specificity (correctly fail to reject when $H_0$ true)\n-   $1 - \\beta$: Power (correctly reject when $H_0$ false)\n\n\\\n\n**Errors:**\n\n-   $\\alpha$: Type I error rate (false positive)\n-   $\\beta$: Type II error rate (false negative)\n\n\\\n\n**What we control in study design:** We typically fix $\\alpha$ at 0.05 and choose $n$ to achieve desired power\n\n## Visualizing these concepts\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15_power_sample_size_2_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n\n**Key observations:**\n\n-   Type I error ($\\alpha$): Area in tails under null distribution\n-   Type II error ($\\beta$): Area to left of critical value under alternative\n-   Power ($1-\\beta$): Area to right of critical value under alternative\n\n## What increases power?\n\n**Power increases when:**\n\n::::: columns\n::: {.column width=\"48%\"}\n**1. Larger sample size (**$n$)\n\n-   Reduces standard error\n-   Narrows sampling distributions\n-   Makes distributions more separated\n\n\\\n\n**2. Larger effect size**\n\n-   Greater true difference\n-   More separation between null and alternative\n-   Easier to distinguish groups\n:::\n\n::: {.column width=\"48%\"}\n**3. Less variability**\n\n-   Smaller population SD ($\\sigma$)\n-   Tighter distributions\n-   Less overlap\n\n\\\n\n**4. Higher significance level**\n\n-   Larger $\\alpha$ (but increases Type I error!)\n-   Usually not recommended\n-   Trade-off between errors\n:::\n:::::\n\n\\\n\n**Practical implications:**\n\n-   We usually have most control over sample size\n-   Can sometimes reduce variability through better measurement\n-   Effect size is determined by nature/treatment\n-   Significance level is conventionally fixed at 0.05\n\n# Part 3: Calculating Power and Sample Size\n\n## Why do power calculations?\n\n**Before collecting data (prospective):**\n\n-   Determine required sample size for adequate power\n-   Justify sample size in grant proposals/protocols\n-   Avoid underpowered studies that waste resources\n-   Ensure ethical use of participants\n\n\\\n\n**After collecting data (post-hoc):**\n\n-   Understand power of completed study\n-   Interpret non-significant results\n-   Plan follow-up studies\n\n\\\n\n::: callout-warning\n## Important Note\n\nPost-hoc power calculations for non-significant results can be misleading! Better to report confidence intervals showing uncertainty.\n:::\n\n## Cohen's d: Standardized effect size\n\n**Problem:** Effect sizes are in original units (e.g., taps/min, mmHg, °F)\n\n-   Hard to compare across studies\n-   Can't have general guidelines\n\n\\\n\n**Solution:** Cohen's *d* standardizes the effect size\n\n\\\n\n::: {.callout-note icon=\"false\"}\n## Cohen's d Formulas\n\n**One-sample test (or paired):** $$d = \\frac{\\bar{x} - \\mu_0}{s} \\quad \\text{or} \\quad d = \\frac{\\bar{x}_d - \\delta_0}{s_d}$$\n\n**Two-sample test:** $$d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{\\text{pooled}}}$$\n\nwhere $s_{\\text{pooled}} = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}$\n:::\n\n## Interpreting Cohen's d\n\n**Cohen's guidelines for effect size:**\n\n| Effect Size | $d$ value | Interpretation                               |\n|-------------|-----------|----------------------------------------------|\n| Small       | 0.2       | Difficult to detect, subtle effect           |\n| Medium      | 0.5       | Moderate effect, visible to careful observer |\n| Large       | 0.8       | Large effect, obvious to casual observer     |\n\n\\\n\n**Example interpretations:**\n\n-   *d* = 0.2: Treatment shifts mean by 0.2 standard deviations\n-   *d* = 0.5: Treatment shifts mean by half a standard deviation\n-   *d* = 0.8: Treatment shifts mean by 0.8 standard deviations\n\n\\\n\n**Important:** These are just guidelines! What's \"small\" or \"large\" depends on context\n\n-   In medicine: Small effects can be clinically important\n-   In psychology: Large effects might indicate measurement problems\n\n## The pwr package in R\n\n**We'll use the `pwr` package for power calculations**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pwr)\n```\n:::\n\n\n\n\\\n\n**Key function:** `pwr.t.test()`\n\n-   Works for one-sample, two-sample, and paired t-tests\n-   Specify all parameters except one\n-   Returns the missing parameter\n\n\\\n\n**Function structure:**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr.t.test(n = NULL,           # Sample size per group\n           d = NULL,           # Cohen's d effect size  \n           sig.level = 0.05,   # Significance level (α)\n           power = NULL,       # Power (1-β)\n           type = \"two.sample\", # or \"one.sample\", \"paired\"\n           alternative = \"two.sided\") # or \"less\", \"greater\"\n```\n:::\n\n\n\n**Leave out the parameter you want to calculate!**\n\n## Example 1: One-sample test - Finding sample size\n\n**Scenario:** Body temperature study\n\n-   We believe true mean is 98.25°F (vs. claimed 98.6°F)\n-   Pilot data suggests SD ≈ 0.73°F\n-   Want 80% power with $\\alpha = 0.05$\n-   **Question:** How many people do we need?\n\n\\\n\n**Step 1:** Calculate Cohen's d\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Effect size\nmu0 <- 98.6      # Null value\nmu_true <- 98.25 # What we believe is true\ns <- 0.73        # Standard deviation from pilot data\n\nd <- (mu_true - mu0) / s\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.4794521\n```\n\n\n:::\n:::\n\n\n\n**Interpretation:** The true mean differs from null by 0.48 standard deviations\n\n## Example 1: One-sample test - Finding sample size (cont.)\n\n**Step 2:** Use `pwr.t.test()` to find required sample size\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult <- pwr.t.test(\n  d = (98.6 - 98.25) / 0.73,  # Cohen's d\n  sig.level = 0.05,            # α = 0.05\n  power = 0.80,                # Want 80% power\n  type = \"one.sample\",         # One-sample test\n  alternative = \"two.sided\"    # Two-sided test\n)\n\nresult\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     One-sample t test power calculation \n\n              n = 36.11196\n              d = 0.4794521\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n```\n\n\n:::\n:::\n\n\n\n\\\n\n**Conclusion:** We need **37 participants** to have 80% power to detect this difference at $\\alpha = 0.05$\n\n## Visualizing the power curve\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(result)\n```\n\n::: {.cell-output-display}\n![](15_power_sample_size_2_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n**The curve shows:** As sample size increases, power increases (holding other factors constant)\n\n## Example 2: One-sample test - Calculating power\n\n**Scenario:** Same body temperature study, but we already collected data\n\n-   n = 130 participants\n-   Effect size: d = 0.479\n-   $\\alpha = 0.05$\n-   **Question:** What power did we have?\n\n\\\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult_power <- pwr.t.test(\n  n = 130,                     # Sample size we collected\n  d = (98.6 - 98.25) / 0.73,  # Cohen's d\n  sig.level = 0.05,            # α = 0.05\n  type = \"one.sample\",\n  alternative = \"two.sided\"\n)\n\nresult_power\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     One-sample t test power calculation \n\n              n = 130\n              d = 0.4794521\n      sig.level = 0.05\n          power = 0.9997354\n    alternative = two.sided\n```\n\n\n:::\n:::\n\n\n\n\\\n\n**Conclusion:** With n=130, we had **100%** power! (Very high - almost certain to detect the effect if it exists)\n\n## Example 3: Two-sample test - Finding sample size\n\n**Scenario:** Caffeine tapping study\n\n-   Want to detect 2 taps/min difference between groups\n-   Expect SD ≈ 2.6 taps/min in each group\n-   Want 80% power with $\\alpha = 0.05$\n-   **Question:** How many participants per group?\n\n\\\n\n**Step 1:** Calculate Cohen's d\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiff <- 2      # Difference we want to detect\nsd_pooled <- 2.6  # Expected SD in each group\n\nd <- diff / sd_pooled\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7692308\n```\n\n\n:::\n:::\n\n\n\n\\\n\n**Step 2:** Calculate required sample size\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult_caff <- pwr.t.test(\n  d = d,\n  sig.level = 0.05,\n  power = 0.80,\n  type = \"two.sample\",        # Two independent groups\n  alternative = \"two.sided\"\n)\n\nresult_caff\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 27.52331\n              d = 0.7692308\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n\n**Conclusion:** Need **28 participants per group** (total n = 56)\n\n## Example 4: Two-sample test - Calculating power\n\n**Scenario:** We ran the caffeine study with 35 per group\n\n-   Effect size: d = 0.77\n-   $\\alpha = 0.05$\n-   **Question:** What power did we have?\n\n\\\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult_caff_power <- pwr.t.test(\n  n = 35,\n  d = 2 / 2.6,\n  sig.level = 0.05,\n  type = \"two.sample\",\n  alternative = \"two.sided\"\n)\n\nresult_caff_power\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 35\n              d = 0.7692308\n      sig.level = 0.05\n          power = 0.8872805\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](15_power_sample_size_2_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n**Conclusion:** With n=35 per group, we had **88.7%** power (excellent!)\n\n## One-sided vs two-sided tests and power\n\n**Two-sided test:** $H_A: \\mu \\neq \\mu_0$ (most common)\n\n**One-sided test:** $H_A: \\mu > \\mu_0$ or $H_A: \\mu < \\mu_0$\n\n\\\n\n**Power comparison:**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Two-sided test\ntwo_sided <- pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.80, \n                        type = \"two.sample\", alternative = \"two.sided\")\n\n# One-sided test  \none_sided <- pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.80,\n                        type = \"two.sample\", alternative = \"greater\")\n\ncat(\"Two-sided requires n =\", ceiling(two_sided$n), \"per group\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTwo-sided requires n = 64 per group\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"One-sided requires n =\", ceiling(one_sided$n), \"per group\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOne-sided requires n = 51 per group\n```\n\n\n:::\n:::\n\n\n\n\\\n\n**Key point:** One-sided tests require smaller sample sizes BUT:\n\n-   Only justified when direction is known *a priori*\n-   Can't detect effects in \"wrong\" direction\n-   Generally not recommended unless strong scientific justification\n\n# Part 4: Study Design Applications\n\n## The power-sample size trade-off\n\n::::: columns\n::: {.column width=\"48%\"}\n**Increasing sample size:**\n\n✓ Increases power\n\n✓ More likely to detect real effects\n\n✓ More precise estimates\n\n\\\n\n✗ More expensive\n\n✗ Takes longer\n\n✗ May not be feasible\n:::\n\n::: {.column width=\"48%\"}\n**Practical constraints:**\n\n-   Budget limitations\n-   Time constraints\n-   Available participants\n-   Ethical considerations\n-   Feasibility\n\n\\\n\n**The balance:** Choose smallest $n$ that gives adequate power (usually 80-90%)\n:::\n:::::\n\n## Common mistakes in power analysis\n\n::: callout-warning\n## Mistake 1: Post-hoc power for non-significant results\n\n**Don't do this:** \"Our result was non-significant (p=0.12). Let me calculate power...\"\n\n**Why it's wrong:** Post-hoc power for non-significant results is always low - tells you nothing!\n\n**Do instead:** Report confidence interval showing uncertainty\n:::\n\n\\\n\n::: callout-warning\n## Mistake 2: Using observed effect size for power\n\n**Don't do this:** Calculate power using the effect size you observed\n\n**Why it's wrong:** If result was non-significant, observed effect is likely underestimate\n\n**Do instead:** Use effect size from pilot data, literature, or smallest clinically meaningful effect\n:::\n\n## Common mistakes continued\n\n::: callout-warning\n## Mistake 3: Ignoring practical significance\n\n**Don't do this:** Design study to detect any statistically significant difference\n\n**Why it's wrong:** Tiny, clinically meaningless effects can be significant with large n\n\n**Do instead:** Base power on *clinically/scientifically meaningful* effect size\n:::\n\n\\\n\n::: callout-warning\n## Mistake 4: Not considering variability\n\n**Don't do this:** Assume optimistic (low) SD when planning sample size\n\n**Why it's wrong:** Real data often more variable → study underpowered\n\n**Do instead:** Use conservative (higher) SD estimate, or add 10-20% to planned n as buffer\n:::\n\n## Real-world example: A cautionary tale\n\n**Study:** New drug to lower blood pressure\n\n-   Powered to detect 5 mmHg reduction\n-   Assumed SD = 10 mmHg (from literature)\n-   Calculated need for n = 64 per group\n-   Only recruited n = 50 per group (budget constraints)\n-   Actual SD in study was 12 mmHg (more variability than expected)\n\n\\\n\n**Result:**\n\n-   Observed reduction: 4 mmHg (close to target!)\n-   P-value = 0.09 (not significant at 0.05 level)\n-   Actual power was only 58% (not the planned 80%)\n\n\\\n\n**Lessons:**\n\n1.  Small deviations from plan can substantially reduce power\n2.  Under-recruitment is very common - build in buffer\n3.  Conservative SD estimates are wise\n\n## When is a study \"underpowered\"?\n\n**Conventionally:**\n\n-   Power \\< 50%: Severely underpowered\n-   Power 50-70%: Underpowered\n-   Power 70-80%: Marginally adequate\n-   Power 80-90%: Good\n-   Power \\> 90%: Excellent (sometimes wasteful)\n\n\\\n\n**Practical considerations:**\n\n-   **80% power** is standard for many studies\n-   **90% power** for important decisions (e.g., FDA approval)\n-   **Lower power acceptable** for:\n    -   Pilot/feasibility studies\n    -   Exploratory research\n    -   Studies where negative result is informative\n\n\\\n\n::: callout-tip\nAlways report your power analysis in papers/grants! Shows thoughtful study design.\n:::\n\n## Using power analysis in different scenarios\n\n::::: columns\n::: {.column width=\"48%\"}\n**Planning a new study:**\n\n1.  Review literature for expected effect size\n2.  Use conservative estimates\n3.  Calculate sample size for 80% power\n4.  Add 10-20% buffer for dropout\n5.  Check if feasible\n\n\\\n\n**Evaluating a completed study:**\n\n1.  Calculate achieved power\n2.  If significant: power is high\n3.  If non-significant: report CI, not power\n4.  Use for planning follow-up\n:::\n\n::: {.column width=\"48%\"}\n**Reviewing others' work:**\n\n1.  Check if power analysis reported\n2.  Evaluate if assumptions reasonable\n3.  Consider if study adequately powered\n4.  Be skeptical of underpowered studies\n\n\\\n\n**Grant writing:**\n\n1.  Justify sample size with power analysis\n2.  Show you calculated it prospectively\n3.  Document all assumptions\n4.  Include sensitivity analyses\n:::\n:::::\n\n# Wrap-up and Key Takeaways\n\n## Summary: The big picture\n\n**Four quantities in equilibrium:**\n\n1.  **Significance level** ($\\alpha$) - usually fixed at 0.05\n2.  **Effect size** - determined by nature/treatment\n3.  **Sample size** ($n$) - what we typically calculate\n4.  **Power** ($1-\\beta$) - probability of detecting real effect\n\n\\\n\n**Key concepts:**\n\n-   Power = Probability of correctly detecting an effect when it exists\n-   Type I error ($\\alpha$) = False positive\n-   Type II error ($\\beta$) = False negative\\\n-   Cohen's *d* = Standardized effect size\n-   Typical target: 80-90% power\n\n## Key R functions\n\n**Main function:** `pwr.t.test()`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate sample size (leave n = NULL)\npwr.t.test(d = 0.5, sig.level = 0.05, power = 0.80,\n           type = \"two.sample\", alternative = \"two.sided\")\n\n# Calculate power (leave power = NULL)  \npwr.t.test(n = 50, d = 0.5, sig.level = 0.05,\n           type = \"two.sample\", alternative = \"two.sided\")\n\n# Calculate detectable effect size (leave d = NULL)\npwr.t.test(n = 50, sig.level = 0.05, power = 0.80,\n           type = \"two.sample\", alternative = \"two.sided\")\n```\n:::\n\n\n\n\\\n\n**Types:** `\"one.sample\"`, `\"two.sample\"`, `\"paired\"`\n\n**Alternatives:** `\"two.sided\"` (most common), `\"less\"`, `\"greater\"`\n\n## Best practices for power analysis\n\n1.  **Plan prospectively** - before collecting data\n2.  **Use realistic effect sizes** - from literature or pilot data\\\n3.  **Be conservative** - overestimate SD, add buffer to sample size\n4.  **Consider practical significance** - not just statistical\n5.  **Report your analysis** - document all assumptions\n6.  **Don't do post-hoc power** for non-significant results\n7.  **Use confidence intervals** to show uncertainty\n8.  **Consider feasibility** - balance power with resources\n\n## Additional resources\n\n**R documentation:**\n\n-   `?pwr.t.test` - help for power calculations\n-   PASS documentation: <https://www.ncss.com/software/pass/pass-documentation/#Means>\n\n\\\n\n**Interactive tools:**\n\n-   <https://rpsychologist.com/d3/NHST/> - Visualize power\n-   <https://www.statmethods.net/stats/power.html> - Power analysis examples\n\n\\\n\n**Textbook:**\n\n-   Section 5.4: Power and Sample Size for means\n\n\\\n\n**Further reading:**\n\n-   Cohen, J. (1988). *Statistical Power Analysis for the Behavioral Sciences*\n-   Lenth, R. V. (2001). \"Some Practical Guidelines for Effective Sample Size Determination\"\n\n## Looking ahead\n\n**Next time:**\n\n-   Comparing two proportions\n-   Chi-square tests\n-   Categorical data analysis\n\n\\\n\n**How today connects:**\n\n-   Power applies to ALL hypothesis tests (not just t-tests!)\n-   Same principles for proportions, chi-square, regression, etc.\n-   Understanding power helps you critically evaluate research\n-   Essential skill for your own research career\n\n\\\n\n**Practice:**\n\n-   Homework will have power calculation problems\n-   Try different scenarios in R\n-   Calculate power for studies you read about\n\n## Questions?\n\n**Key questions to ask yourself:**\n\n-   Can you explain the four components of hypothesis testing?\n-   What's the difference between Type I and Type II errors?\n-   Why do we care about power?\n-   How do you calculate sample size for a t-test?\n-   When is a study underpowered?\n\n\\\n\n**Office hours:** Come discuss your research study designs!\n\n**Next class:** We'll start talking about categorical data and proportions\n",
    "supporting": [
      "15_power_sample_size_2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}