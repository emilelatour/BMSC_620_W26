---
title: "The Normal Distribution"
subtitle: "BSTA 511/611 - Distributions of Random Variables"
author: "Your Name"
date: "Wednesday, Week 4"
format: 
  revealjs:
    theme: simple
    chalkboard: true
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: Normal Distribution Lecture
    html-math-method: mathjax
    highlight-style: arrow
execute:
  echo: true
  freeze: auto
---

```{r}
#| label: "setup" 
#| include: false
#| message: false
#| warning: false

library(tidyverse)
library(knitr)

# set ggplot theme for slides 
theme_set(theme_bw(base_size = 22))
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

# Learning Objectives {background-color="#3070BF"}

By the end of today's lecture, you will be able to:

1. Describe the characteristics and importance of the Normal distribution
2. Calculate and interpret Z-scores to standardize observations
3. Use the Empirical Rule to estimate probabilities
4. Calculate probabilities using the Normal distribution (with R and tables)
5. Find percentiles and critical values
6. Apply the Normal approximation to the Binomial distribution

## Roadmap for Today

:::: {.columns}
::: {.column width="50%"}
**Part 1: Understanding the Normal Distribution**

- What makes it special?
- Parameters and notation
- Standard Normal distribution

**Part 2: Z-scores & Standardization**

- Converting to standard units
- Interpretation and comparison

**Part 3: The Empirical Rule**

- The 68-95-99.7 rule
- Quick probability estimates
:::

::: {.column width="50%"}
**Part 4: Calculating Probabilities**

- Using R functions
- Types of probability questions
- Real-world applications

**Part 5: Finding Percentiles**

- Inverse problems
- Clinical thresholds

**Part 6: Normal Approximation**

- When and why to use it
- Connecting to Binomial
:::
::::

# Part 1: The Normal Distribution {background-color="#3070BF"}

Why is this distribution so important?

## Why Study the Normal Distribution?

The Normal distribution is **the most important** probability distribution in statistics because:

::: {.incremental}
- **Many natural phenomena** follow it (heights, weights, blood pressure, IQ scores, measurement errors)
- **Central Limit Theorem**: Sample means tend toward normality (more on this later in the course!)
- Foundation for many **statistical tests and confidence intervals**
- Easy to work with mathematically
- Well-studied with extensive tables and software support
:::

. . .

::: {.callout-important}
## Key Point
Even when data aren't perfectly normal, the Normal distribution often provides a good approximation for making inferences about populations.
:::

## Characteristics of the Normal Distribution

A random variable $X$ follows a **Normal distribution** if it has:

:::: {.columns}
::: {.column width="45%"}
**Shape:**

- Symmetric bell curve
- Unimodal (single peak)
- Tails extend to infinity (theoretically)

**Parameters:**

- **Mean** $\mu$: center of distribution
- **Standard deviation** $\sigma$: spread/variability
:::

::: {.column width="55%"}
```{r}
#| echo: false
#| fig-height: 5
#| fig-width: 7

x <- seq(-4, 4, length.out = 1000)
y <- dnorm(x)

ggplot(data.frame(x = x, y = y), 
       aes(x, y)) +
  geom_line(size = 1.5, 
            color = "#3070BF") +
  geom_vline(xintercept = 0, 
             linetype = "dashed", 
             color = "red", 
             linewidth = 1) +
  annotate("text", x = 0, y = 0.42, label = "Mean (μ)", color = "red", size = 6) +
  annotate("segment", x = 0, xend = 1, y = 0.25, yend = 0.25, 
           arrow = arrow(length = unit(0.3, "cm"), ends = "both"), 
           color = "darkgreen", size = 1) +
  annotate("text", x = 0.5, y = 0.27, label = "σ", color = "darkgreen", size = 7) +
  labs(x = "Value", y = "Density", 
       title = "The Normal Distribution") +
  theme_minimal(base_size = 18)
```
:::
::::

## Normal Distribution: Notation

::: {.callout-note icon=false}
## Notation
If a random variable $X$ follows a Normal distribution with mean $\mu$ and standard deviation $\sigma$, we write:

$$X \sim \text{Normal}(\mu, \sigma)$$

or equivalently:

$$X \sim N(\mu, \sigma)$$
:::

. . .

**Examples:**

- Heights of adult men: $X \sim N(70 \text{ inches}, 3.3 \text{ inches})$
- SAT scores: $X \sim N(1500, 300)$
- Diastolic blood pressure in 35-44 year old men: $X \sim N(80 \text{ mm Hg}, 12 \text{ mm Hg})$

## Effect of Parameters: Mean $\mu$

Changing the **mean** $\mu$ **shifts** the distribution left or right:

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 12

x <- seq(-10, 16, length.out = 1000)

data <- tibble(
  x = rep(x, 3),
  mean = rep(c(-2, 3, 8), each = length(x)),
  y = dnorm(x, mean = mean, sd = 2)
)

ggplot(data, aes(x, y, color = factor(mean))) +
  geom_line(size = 1.5) +
  scale_color_manual(values = c("#E69F00", "#56B4E9", "#009E73"),
                     name = "Mean (μ)",
                     labels = c("μ = -2", "μ = 3", "μ = 8")) +
  labs(x = "Value", y = "Density",
       title = "Normal Distributions with Different Means (all have σ = 2)") +
  theme_minimal(base_size = 20) +
  theme(legend.position = "top")
```

## Effect of Parameters: Standard Deviation $\sigma$

Changing the **standard deviation** $\sigma$ **stretches or compresses** the distribution:

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 12

x <- seq(-15, 15, length.out = 1000)

data <- tibble(
  x = rep(x, 3),
  sd = rep(c(1, 2.5, 5), each = length(x)),
  y = dnorm(x, mean = 0, sd = sd)
)

ggplot(data, aes(x, y, color = factor(sd))) +
  geom_line(size = 1.5) +
  scale_color_manual(values = c("#D55E00", "#CC79A7", "#0072B2"),
                     name = "Std Dev (σ)",
                     labels = c("σ = 1", "σ = 2.5", "σ = 5")) +
  labs(x = "Value", y = "Density",
       title = "Normal Distributions with Different Standard Deviations (all have μ = 0)") +
  theme_minimal(base_size = 20) +
  theme(legend.position = "top")
```

::: {.callout-tip}
Larger $\sigma$ → More spread out (flatter, wider curve)

Smaller $\sigma$ → Less spread out (taller, narrower curve)
:::

## The Standard Normal Distribution

::: {.callout-important icon=false}
## Definition: Standard Normal Distribution
The **Standard Normal distribution** is a special case with:

- Mean $\mu = 0$
- Standard deviation $\sigma = 1$

We denote this as: $Z \sim N(0, 1)$
:::

. . .

**Why is this special?**

::: {.incremental}
- Simplifies calculations and comparisons
- Has universal tables of probabilities
- Any Normal distribution can be converted to standard Normal
- We use $Z$ (not $X$) to denote standard normal variables
:::

# Part 2: Z-scores & Standardization {background-color="#3070BF"}

Converting to a common scale

## What is a Z-score?

::: {.callout-note icon=false}
## Definition: Z-score
The **Z-score** (also called **standard score**) tells you how many standard deviations an observation is from the mean.

$$Z = \frac{X - \mu}{\sigma}$$

where:

- $X$ = observation value
- $\mu$ = population mean
- $\sigma$ = population standard deviation
:::

. . .

**Interpretation:**

- $Z = 0$: observation equals the mean
- $Z > 0$: observation is **above** the mean
- $Z < 0$: observation is **below** the mean
- $|Z| = 2$: observation is 2 standard deviations from the mean

## Visual Understanding of Z-scores

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 13

# Create the data
x <- seq(-4, 4, length.out = 1000)
y <- dnorm(x)

# Create plot
ggplot(data.frame(x = x, y = y), aes(x, y)) +
  geom_line(size = 1.5, color = "#3070BF") +
  geom_vline(xintercept = c(-2, -1, 0, 1, 2), linetype = "dashed", alpha = 0.5) +
  geom_point(data = data.frame(x = c(-2, -1, 0, 1, 2), y = dnorm(c(-2, -1, 0, 1, 2))),
             aes(x, y), size = 5, color = "red") +
  annotate("text", x = c(-2, -1, 0, 1, 2), y = -0.02, 
           label = c("Z = -2", "Z = -1", "Z = 0", "Z = 1", "Z = 2"),
           size = 6, color = "red") +
  labs(x = "Z-score (standard deviations from mean)", y = "Density",
       title = "Standard Normal Distribution with Key Z-scores") +
  theme_minimal(base_size = 20) +
  ylim(-0.05, 0.45)
```

## Example: Comparing Scores Across Different Tests

::: {.callout-tip icon=false}
## Example: SAT vs ACT
The SAT and ACT are two standardized tests for college admissions:

- **SAT scores:** $\sim N(1500, 300)$
- **ACT scores:** $\sim N(21, 5)$

Two students take different tests:

- Student A scores **1800** on the SAT
- Student B scores **24** on the ACT

**Question:** Who performed better relative to other test-takers?
:::

. . .

We **can't compare directly** (different scales!) — we need Z-scores!

## Example: SAT vs ACT (Solution)

**Student A (SAT):**

$$Z_A = \frac{X - \mu}{\sigma} = \frac{1800 - 1500}{300} = \frac{300}{300} = 1.0$$

. . .

**Student B (ACT):**

$$Z_B = \frac{X - \mu}{\sigma} = \frac{24 - 21}{5} = \frac{3}{5} = 0.6$$

. . .

::: {.callout-important}
## Conclusion
Student A performed better! 

- Student A scored 1.0 standard deviations above average
- Student B scored 0.6 standard deviations above average
- Student A's score is more extreme (further from the mean)
:::

## Visual Comparison: SAT vs ACT

```{r}
#| echo: false
#| fig-height: 7
#| fig-width: 13

library(patchwork)

# SAT plot
x_sat <- seq(600, 2400, length.out = 1000)
y_sat <- dnorm(x_sat, mean = 1500, sd = 300)

p1 <- ggplot(data.frame(x = x_sat, y = y_sat), aes(x, y)) +
  geom_line(size = 1.5, color = "#3070BF") +
  geom_vline(xintercept = 1800, color = "red", size = 1.5, linetype = "dashed") +
  geom_vline(xintercept = 1500, color = "darkgreen", size = 1, linetype = "dotted") +
  annotate("text", x = 1800, y = 0.0012, label = "Student A\n1800", 
           color = "red", size = 6, fontface = "bold") +
  annotate("text", x = 1500, y = 0.0014, label = "Mean\n1500", 
           color = "darkgreen", size = 5) +
  labs(title = "SAT Scores: N(1500, 300)", x = "SAT Score", y = "Density") +
  theme_minimal(base_size = 18)

# ACT plot
x_act <- seq(6, 36, length.out = 1000)
y_act <- dnorm(x_act, mean = 21, sd = 5)

p2 <- ggplot(data.frame(x = x_act, y = y_act), aes(x, y)) +
  geom_line(size = 1.5, color = "#E69F00") +
  geom_vline(xintercept = 24, color = "red", size = 1.5, linetype = "dashed") +
  geom_vline(xintercept = 21, color = "darkgreen", size = 1, linetype = "dotted") +
  annotate("text", x = 24, y = 0.07, label = "Student B\n24", 
           color = "red", size = 6, fontface = "bold") +
  annotate("text", x = 21, y = 0.082, label = "Mean\n21", 
           color = "darkgreen", size = 5) +
  labs(title = "ACT Scores: N(21, 5)", x = "ACT Score", y = "Density") +
  theme_minimal(base_size = 18)

p1 / p2
```

## Converting from Z-score Back to X

We can also go **backwards** — from Z-score to original value:

::: {.callout-note icon=false}
## Formula: Z to X
$$X = \mu + Z\sigma$$
:::

. . .

::: {.callout-tip icon=false}
## Example
**Question:** What ACT score is equivalent to an SAT score of 1800?

We found $Z_A = 1.0$ for SAT = 1800.

For ACT with $\mu = 21$ and $\sigma = 5$:

$$X = 21 + (1.0)(5) = 26$$

An ACT score of **26** is equivalent to an SAT score of **1800**.
:::

## Your Turn! (Think-Pair-Share)

::: {.callout-warning icon=false}
## Practice Problem
Systolic blood pressure (SBP) for adults in the U.S. aged 18-39 follows approximately $N(115, 17.5)$ mm Hg.

A 30-year-old has SBP of **140 mm Hg**.

**Questions:**

1. Calculate the Z-score for this person
2. How unusual is this blood pressure?
3. What would you tell this person?

*Take 2 minutes to work with a neighbor, then we'll discuss.*
:::

. . .

**Solution:**

$$Z = \frac{140 - 115}{17.5} = \frac{25}{17.5} \approx 1.43$$

This person's SBP is about **1.4 standard deviations above average** — moderately elevated!

# Part 3: The Empirical Rule {background-color="#3070BF"}

A quick way to estimate probabilities

## The Empirical Rule (68-95-99.7 Rule)

For **any** Normal distribution:

::: {.incremental}
- Approximately **68%** of observations fall within **1 SD** of the mean
- Approximately **95%** of observations fall within **2 SD** of the mean  
- Approximately **99.7%** of observations fall within **3 SD** of the mean
:::

. . .

```{r}
#| echo: false
#| fig-height: 5
#| fig-width: 13

x <- seq(-4, 4, length.out = 1000)
y <- dnorm(x)

# Create base plot
p <- ggplot(data.frame(x = x, y = y), aes(x, y)) +
  geom_line(size = 1.5, color = "#3070BF")

# Add shaded regions
p <- p +
  geom_area(data = data.frame(x = x[x >= -3 & x <= 3], 
                               y = y[x >= -3 & x <= 3]),
            aes(x, y), fill = "yellow", alpha = 0.3) +
  geom_area(data = data.frame(x = x[x >= -2 & x <= 2], 
                               y = y[x >= -2 & x <= 2]),
            aes(x, y), fill = "orange", alpha = 0.3) +
  geom_area(data = data.frame(x = x[x >= -1 & x <= 1], 
                               y = y[x >= -1 & x <= 1]),
            aes(x, y), fill = "red", alpha = 0.3)

# Add vertical lines and labels
p <- p +
  geom_vline(xintercept = c(-3, -2, -1, 0, 1, 2, 3), linetype = "dashed", alpha = 0.5) +
  annotate("text", x = 0, y = 0.15, label = "68%", size = 7, fontface = "bold") +
  annotate("text", x = 0, y = 0.08, label = "95%", size = 7, fontface = "bold") +
  annotate("text", x = 0, y = 0.03, label = "99.7%", size = 7, fontface = "bold") +
  labs(x = "Standard deviations from mean", y = "Density",
       title = "The Empirical Rule: 68-95-99.7") +
  scale_x_continuous(breaks = -3:3,
                     labels = c("μ-3σ", "μ-2σ", "μ-σ", "μ", "μ+σ", "μ+2σ", "μ+3σ")) +
  theme_minimal(base_size = 20)

p
```

## Using the Empirical Rule: Example

::: {.callout-tip icon=false}
## Example: Blood Pressure
Diastolic blood pressure in 35-44 year old men: $N(80, 12)$ mm Hg

**Question:** What percentage of men have DBP between 68 and 92 mm Hg?
:::

. . .

**Solution:**

::: {.incremental}
- First, calculate how many SDs from the mean:
  - Lower bound: $68 = 80 - 12 = \mu - 1\sigma$
  - Upper bound: $92 = 80 + 12 = \mu + 1\sigma$
- This is the interval $[\mu - \sigma, \mu + \sigma]$
- By the Empirical Rule: approximately **68%** of men have DBP in this range
:::

## The Empirical Rule: Key Implications

::: {.callout-important}
## What the Empirical Rule tells us:

**Common values** ($|Z| < 2$):

- About 95% of data falls within 2 SD of mean
- Values in this range are "typical" or "expected"

**Unusual values** ($|Z| > 2$):

- Only about 5% of data is more than 2 SD from mean
- These might warrant attention or investigation

**Very unusual values** ($|Z| > 3$):

- Only about 0.3% of data is more than 3 SD from mean
- These are rare and potentially concerning
:::

## Quick Practice

::: {.callout-warning icon=false}
## Poll Everywhere Question
Male heights: $N(70 \text{ inches}, 3.3 \text{ inches})$

Using **only** the Empirical Rule, approximately what percentage of adult males are between 63.4 and 76.6 inches tall?

A. 68%  
B. 95%  
C. 99.7%  
D. Cannot determine from Empirical Rule
:::

. . .

**Answer:** B. 95%

- $63.4 = 70 - 2(3.3) = \mu - 2\sigma$
- $76.6 = 70 + 2(3.3) = \mu + 2\sigma$

# Part 4: Calculating Exact Probabilities {background-color="#3070BF"}

Going beyond approximations

## Why Do We Need Exact Calculations?

The Empirical Rule is great for **quick estimates**, but:

::: {.incremental}
- It only works for 1, 2, or 3 standard deviations
- What about $P(X < 85)$? Or $P(72 < X < 88)$?
- What if we need exact probabilities (not approximations)?
:::

. . .

::: {.callout-important}
## Solution: Use Technology!
We calculate exact probabilities using:

1. **R functions** (recommended for this class!)
2. Normal probability tables (in textbook Appendix B.1)
3. Online calculators
:::

## Normal Distribution Functions in R

::: {.callout-note icon=false}
## R Functions for Normal Distribution

| Function | Purpose | Example |
|----------|---------|---------|
| `pnorm()` | **P**robability: Find $P(X \leq x)$ | `pnorm(1.5, mean=0, sd=1)` |
| `qnorm()` | **Q**uantile: Find value for given probability | `qnorm(0.95, mean=0, sd=1)` |
| `dnorm()` | **D**ensity: Height of curve at $x$ | `dnorm(0, mean=0, sd=1)` |
| `rnorm()` | **R**andom: Generate random values | `rnorm(100, mean=0, sd=1)` |

**We'll focus on `pnorm()` and `qnorm()` today**
:::

## Using `pnorm()`: The Basics

`pnorm(q, mean, sd, lower.tail)`

::: {.incremental}
- `q`: the value (quantile) you're asking about
- `mean`: $\mu$ of the distribution
- `sd`: $\sigma$ of the distribution  
- `lower.tail`: 
  - `TRUE` (default) → gives $P(X \leq q)$ (left tail)
  - `FALSE` → gives $P(X > q)$ (right tail)
:::

. . .

**Examples:**

```{r}
#| echo: true
# For standard normal Z ~ N(0,1)
pnorm(q = 1.5, mean = 0, sd = 1)  # P(Z ≤ 1.5)
```

```{r}
#| echo: true
pnorm(q = 1.5, mean = 0, sd = 1, lower.tail = FALSE)  # P(Z > 1.5)
```

## Example: Calculate $P(Z < 2.67)$

::: {.callout-tip icon=false}
## Problem
Let $Z \sim N(0, 1)$. Find $P(Z < 2.67)$.
:::

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 10

x <- seq(-4, 4, length.out = 1000)
y <- dnorm(x)

ggplot(data.frame(x = x, y = y), aes(x, y)) +
  geom_line(size = 1.5, color = "#3070BF") +
  geom_area(data = data.frame(x = x[x <= 2.67], y = y[x <= 2.67]),
            aes(x, y), fill = "lightblue", alpha = 0.7) +
  geom_vline(xintercept = 2.67, color = "red", size = 1.5, linetype = "dashed") +
  annotate("text", x = 2.67, y = 0.02, label = "z = 2.67", color = "red", size = 6) +
  annotate("text", x = 0, y = 0.2, label = "Shaded area = P(Z < 2.67)", size = 7) +
  labs(x = "Z", y = "Density") +
  theme_minimal(base_size = 20)
```

```{r}
pnorm(q = 2.67, mean = 0, sd = 1, lower.tail = TRUE)
```

**Answer:** $P(Z < 2.67) = 0.9962$

## Example: Calculate $P(Z > -0.37)$

::: {.callout-tip icon=false}
## Problem
Let $Z \sim N(0, 1)$. Find $P(Z > -0.37)$.
:::

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 10

x <- seq(-4, 4, length.out = 1000)
y <- dnorm(x)

ggplot(data.frame(x = x, y = y), aes(x, y)) +
  geom_line(size = 1.5, color = "#3070BF") +
  geom_area(data = data.frame(x = x[x >= -0.37], y = y[x >= -0.37]),
            aes(x, y), fill = "lightcoral", alpha = 0.7) +
  geom_vline(xintercept = -0.37, color = "red", size = 1.5, linetype = "dashed") +
  annotate("text", x = -0.37, y = 0.02, label = "z = -0.37", color = "red", size = 6) +
  annotate("text", x = 1, y = 0.25, label = "Shaded area = P(Z > -0.37)", size = 7) +
  labs(x = "Z", y = "Density") +
  theme_minimal(base_size = 20)
```

```{r}
pnorm(q = -0.37, mean = 0, sd = 1, lower.tail = FALSE)
```

**Answer:** $P(Z > -0.37) = 0.6443$

## Example: Calculate $P(-2.18 < Z < 2.46)$

::: {.callout-tip icon=false}
## Problem
Let $Z \sim N(0, 1)$. Find $P(-2.18 < Z < 2.46)$.
:::

```{r}
#| echo: false
#| fig-height: 4
#| fig-width: 10

x <- seq(-4, 4, length.out = 1000)
y <- dnorm(x)

ggplot(data.frame(x = x, y = y), aes(x, y)) +
  geom_line(size = 1.5, color = "#3070BF") +
  geom_area(data = data.frame(x = x[x >= -2.18 & x <= 2.46], 
                               y = y[x >= -2.18 & x <= 2.46]),
            aes(x, y), fill = "lightgreen", alpha = 0.7) +
  geom_vline(xintercept = c(-2.18, 2.46), color = "red", size = 1.5, linetype = "dashed") +
  annotate("text", x = -2.18, y = 0.02, label = "z = -2.18", color = "red", size = 5) +
  annotate("text", x = 2.46, y = 0.02, label = "z = 2.46", color = "red", size = 5) +
  annotate("text", x = 0, y = 0.25, label = "Shaded area =\nP(-2.18 < Z < 2.46)", size = 7) +
  labs(x = "Z", y = "Density") +
  theme_minimal(base_size = 20)
```

**Strategy:** Find $P(Z < 2.46)$ and subtract $P(Z < -2.18)$

```{r}
pnorm(2.46) - pnorm(-2.18)
```

**Answer:** $P(-2.18 < Z < 2.46) = 0.9784$

## Important Note: Continuous Distributions

::: {.callout-warning}
## For Continuous Random Variables
For any continuous random variable (including Normal):

$$P(X = \text{exact value}) = 0$$

Therefore:

- $P(X < a) = P(X \leq a)$
- $P(X > a) = P(X \geq a)$
- $P(a < X < b) = P(a \leq X \leq b)$

**This is different from discrete distributions!**
:::

## Example: Calculate $P(Z = 1.53)$

::: {.callout-tip icon=false}
## Problem
Let $Z \sim N(0, 1)$. Find $P(Z = 1.53)$.
:::

. . .

**Answer:** $P(Z = 1.53) = 0$

The probability of getting *exactly* 1.53 (with infinite decimal precision) is zero!

. . .

::: {.callout-note}
This is why we always ask about **intervals** with continuous distributions:

- $P(Z < 1.53)$ ✓
- $P(Z > 1.53)$ ✓  
- $P(1.5 < Z < 1.6)$ ✓
- $P(Z = 1.53)$ = 0
:::

# Real-World Application {background-color="#3070BF"}

Diastolic Blood Pressure Example

## Example: Diastolic Blood Pressure (Setup)

::: {.callout-tip icon=false}
## Clinical Scenario
Diastolic blood pressure (DBP) in 35-44 year old men is normally distributed:

$$\text{DBP} \sim N(\mu = 80 \text{ mm Hg}, \sigma = 12 \text{ mm Hg})$$

**Note:** The problem gives variance = 144, so $\sigma = \sqrt{144} = 12$

We'll answer three questions:

1. What proportion has mild hypertension (DBP between 90-99)?
2. What is the 10th percentile?
3. What is the 95th percentile?
:::

## Question 1: Mild Hypertension

::: {.callout-tip icon=false}
## Question
Mild hypertension is DBP between 90 and 99 mm Hg. What proportion of this population has mild hypertension?
:::

```{r}
#| echo: false
#| fig-height: 4.5
#| fig-width: 11

x <- seq(40, 120, length.out = 1000)
y <- dnorm(x, mean = 80, sd = 12)

ggplot(data.frame(x = x, y = y), aes(x, y)) +
  geom_line(size = 1.5, color = "#3070BF") +
  geom_area(data = data.frame(x = x[x >= 90 & x <= 99], 
                               y = y[x >= 90 & x <= 99]),
            aes(x, y), fill = "orange", alpha = 0.7) +
  geom_vline(xintercept = c(90, 99), color = "red", size = 1.5, linetype = "dashed") +
  annotate("text", x = 94.5, y = 0.015, label = "Mild\nHypertension", size = 7, fontface = "bold") +
  labs(x = "Diastolic Blood Pressure (mm Hg)", y = "Density",
       title = "DBP ~ N(80, 12)") +
  theme_minimal(base_size = 20)
```

## Question 1: Solution

Find $P(90 < \text{DBP} < 99)$:

```{r}
# P(DBP ≤ 99) - P(DBP ≤ 90)
pnorm(q = 99, mean = 80, sd = 12) - 
  pnorm(q = 90, mean = 80, sd = 12)
```

. . .

::: {.callout-important}
## Answer
Approximately **18.2%** of 35-44 year old men have mild hypertension (DBP between 90-99 mm Hg).
:::

# Part 5: Finding Percentiles {background-color="#3070BF"}

Working backwards from probability to value

## Percentiles: The Inverse Problem

So far we've been finding: **Given a value, what's the probability?**

$$X = 90 \rightarrow P(X < 90) = ?$$

. . .

Now we ask: **Given a probability, what's the value?**

$$P(X < ?) = 0.10 \rightarrow X = ?$$

. . .

::: {.callout-note icon=false}
## Definition: Percentile
The $p$-th **percentile** is the value below which $p$% of observations fall.

- 10th percentile: 10% of observations are below this value
- 95th percentile: 95% of observations are below this value
- 50th percentile: the median (half below, half above)
:::

## Using `qnorm()` to Find Percentiles

`qnorm(p, mean, sd, lower.tail)`

::: {.incremental}
- `p`: the probability (percentile as a proportion)
- `mean`: $\mu$ of the distribution
- `sd`: $\sigma$ of the distribution
- `lower.tail`: `TRUE` gives left-tail percentile (default)
:::

. . .

**Example:**

```{r}
# What is the 95th percentile of Z ~ N(0,1)?
qnorm(p = 0.95, mean = 0, sd = 1)
```

The 95th percentile of the standard normal is **1.645**

(95% of observations fall below $Z = 1.645$)

## Question 2: 10th Percentile of DBP

::: {.callout-tip icon=false}
## Question
What is the 10th percentile of DBP distribution?

DBP $\sim N(80, 12)$
:::

```{r}
#| echo: false
#| fig-height: 4.5
#| fig-width: 11

x <- seq(40, 120, length.out = 1000)
y <- dnorm(x, mean = 80, sd = 12)

q10 <- qnorm(0.10, mean = 80, sd = 12)

ggplot(data.frame(x = x, y = y), aes(x, y)) +
  geom_line(size = 1.5, color = "#3070BF") +
  geom_area(data = data.frame(x = x[x <= q10], 
                               y = y[x <= q10]),
            aes(x, y), fill = "lightblue", alpha = 0.7) +
  geom_vline(xintercept = q10, color = "red", size = 1.5, linetype = "dashed") +
  annotate("text", x = q10, y = 0.025, label = "10th\npercentile", size = 7, color = "red", fontface = "bold") +
  annotate("text", x = 60, y = 0.015, label = "10%", size = 8, fontface = "bold") +
  labs(x = "Diastolic Blood Pressure (mm Hg)", y = "Density") +
  theme_minimal(base_size = 20)
```

## Question 2: Solution

```{r}
qnorm(p = 0.10, mean = 80, sd = 12)
```

. . .

::: {.callout-important}
## Answer
The 10th percentile is **64.6 mm Hg**.

**Interpretation:** 10% of men ages 35-44 have DBP below 64.6 mm Hg, and 90% have DBP above this value.
:::

## Question 3: 95th Percentile of DBP

::: {.callout-tip icon=false}
## Question
What is the 95th percentile of DBP distribution?
:::

```{r}
#| echo: false
#| fig-height: 4.5
#| fig-width: 11

q95 <- qnorm(0.95, mean = 80, sd = 12)

ggplot(data.frame(x = x, y = y), aes(x, y)) +
  geom_line(size = 1.5, color = "#3070BF") +
  geom_area(data = data.frame(x = x[x <= q95], 
                               y = y[x <= q95]),
            aes(x, y), fill = "lightcoral", alpha = 0.7) +
  geom_vline(xintercept = q95, color = "red", size = 1.5, linetype = "dashed") +
  annotate("text", x = q95, y = 0.025, label = "95th\npercentile", size = 7, color = "red", fontface = "bold") +
  annotate("text", x = 90, y = 0.015, label = "95%", size = 8, fontface = "bold") +
  labs(x = "Diastolic Blood Pressure (mm Hg)", y = "Density") +
  theme_minimal(base_size = 20)
```

```{r}
qnorm(p = 0.95, mean = 80, sd = 12)
```

. . .

::: {.callout-important}
## Answer
The 95th percentile is **99.7 mm Hg**.

**Interpretation:** 95% of men have DBP below 99.7, only 5% have higher values.
:::

## Clinical Thresholds and Percentiles

::: {.callout-note}
## Why Percentiles Matter in Medicine

Percentiles help define clinical cut-offs:

- **Blood pressure:** Hypertension defined by percentiles
- **Growth charts:** Children's height/weight tracked by percentiles
- **Lab values:** "Normal ranges" often 2.5th to 97.5th percentile
- **Risk assessment:** High cholesterol, low bone density, etc.

Percentiles put individual values in context of the population!
:::

# Part 6: Normal Approximation to Binomial {background-color="#3070BF"}

When the binomial gets difficult to compute

## Recall: The Binomial Distribution

For a binomial random variable $X \sim \text{Binomial}(n, p)$:

$$P(X = k) = \binom{n}{k} p^k (1-p)^{n-k} = \frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}$$

. . .

**Problems:**

::: {.incremental}
- Computing $P(X \leq 50)$ when $n = 100$? Need to sum 51 terms!
- Factorials become huge for large $n$
- Even computers can struggle with very large $n$
:::

. . .

::: {.callout-important}
## Solution
When $n$ is large, the binomial distribution looks approximately normal!

We can use Normal distribution as an approximation.
:::

## Visual: Binomial → Normal as $n$ Increases

```{r}
#| echo: false
#| fig-height: 6.5
#| fig-width: 13

library(patchwork)

make_binom_plot <- function(n, p, title) {
  x <- 0:n
  probs <- dbinom(x, size = n, prob = p)
  mu <- n * p
  sigma <- sqrt(n * p * (1-p))
  
  x_norm <- seq(0, n, length.out = 1000)
  y_norm <- dnorm(x_norm, mean = mu, sd = sigma)
  
  ggplot() +
    geom_col(aes(x = x, y = probs), fill = "#3070BF", alpha = 0.7, width = 0.8) +
    geom_line(aes(x = x_norm, y = y_norm), color = "red", size = 1.5) +
    labs(title = title, x = "Number of successes", y = "Probability") +
    theme_minimal(base_size = 16) +
    theme(plot.title = element_text(hjust = 0.5, size = 18))
}

p1 <- make_binom_plot(10, 0.25, "n = 10, p = 0.25")
p2 <- make_binom_plot(30, 0.25, "n = 30, p = 0.25")
p3 <- make_binom_plot(100, 0.25, "n = 100, p = 0.25")
p4 <- make_binom_plot(300, 0.25, "n = 300, p = 0.25")

(p1 | p2) / (p3 | p4)
```

Blue = Binomial distribution; Red = Normal approximation

## When Can We Use Normal Approximation?

::: {.callout-important icon=false}
## Rule: Normal Approximation to Binomial
If $X \sim \text{Binomial}(n, p)$ and **both** conditions are met:

1. $np \geq 10$
2. $n(1-p) \geq 10$

Then we can approximate: $$X \sim \text{Normal}(\mu = np, \sigma = \sqrt{np(1-p)})$$
:::

. . .

**Why these conditions?**

- Ensures $n$ is large enough
- Ensures $p$ is not too close to 0 or 1 (so distribution is somewhat symmetric)

## Example: COVID-19 Vaccination Status (Setup)

::: {.callout-tip icon=false}
## Problem
About 25% of people who test positive for COVID-19 are vaccinated. 

Suppose 100 people test positive (independently). Let $X$ = number vaccinated among the 100.

**Question:** What is the probability that fewer than 20 are vaccinated?

We'll solve this **two ways:**

1. Exact (Binomial)
2. Approximate (Normal)
:::

## Check Conditions for Normal Approximation

$n = 100$, $p = 0.25$

**Condition 1:** $np = 100 \times 0.25 = 25 \geq 10$ ✓

**Condition 2:** $n(1-p) = 100 \times 0.75 = 75 \geq 10$ ✓

. . .

::: {.callout-note}
## Both conditions met!
We can use Normal approximation.

**Parameters:**

- Mean: $\mu = np = 25$
- SD: $\sigma = \sqrt{np(1-p)} = \sqrt{100 \times 0.25 \times 0.75} = \sqrt{18.75} = 4.33$
:::

## Solution 1: Exact Binomial Probability

Want $P(X < 20) = P(X \leq 19)$

$$P(X \leq 19) = \sum_{k=0}^{19} \binom{100}{k} (0.25)^k (0.75)^{100-k}$$

. . .

```{r}
# Exact binomial calculation
pbinom(q = 19, size = 100, prob = 0.25)
```

::: {.callout-important}
## Exact Answer
$P(X < 20) = 0.1066$ (10.66%)
:::

## Solution 2: Normal Approximation

Approximate: $X \sim N(\mu = 25, \sigma = 4.33)$

Want $P(X < 20)$

. . .

**Without continuity correction:**

```{r}
pnorm(q = 20, mean = 25, sd = sqrt(100*0.25*0.75))
```

. . .

**With continuity correction** (use 19.5 instead of 20):

```{r}
pnorm(q = 19.5, mean = 25, sd = sqrt(100*0.25*0.75))
```

. . .

::: {.callout-important}
## Comparison
- Exact (Binomial): 0.1066
- Normal approximation: 0.1220 (without correction) or 0.1003 (with correction)
- Continuity correction gives better approximation!
:::

## What is Continuity Correction?

::: {.callout-note icon=false}
## The Issue
- **Binomial** is discrete: $X$ can only be 0, 1, 2, 3, ...
- **Normal** is continuous: $X$ can be any real number

When we approximate discrete with continuous, we lose some accuracy.
:::

. . .

::: {.callout-tip icon=false}
## The Fix: Continuity Correction
Adjust the cutoff by ±0.5:

- For $P(X \leq k)$: use $P(X \leq k + 0.5)$ with Normal
- For $P(X < k)$: use $P(X \leq k - 0.5)$ with Normal  
- For $P(X \geq k)$: use $P(X \geq k - 0.5)$ with Normal
- For $P(X > k)$: use $P(X \geq k + 0.5)$ with Normal
:::

## Visualizing Continuity Correction

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 13

# Binomial bars
x_binom <- 10:40
y_binom <- dbinom(x_binom, size = 100, prob = 0.25)

# Normal curve
x_norm <- seq(10, 40, length.out = 1000)
y_norm <- dnorm(x_norm, mean = 25, sd = sqrt(18.75))

# Create plot
ggplot() +
  # Binomial bars
  geom_col(aes(x = x_binom, y = y_binom), 
           fill = "#3070BF", alpha = 0.5, width = 1) +
  # Normal curve
  geom_line(aes(x = x_norm, y = y_norm), color = "red", size = 1.5) +
  # Highlight region for P(X < 20)
  geom_col(aes(x = x_binom[x_binom < 20], y = y_binom[x_binom < 20]),
           fill = "orange", alpha = 0.8, width = 1) +
  # Vertical lines
  geom_vline(xintercept = 19.5, color = "darkgreen", size = 2, linetype = "dashed") +
  geom_vline(xintercept = 20, color = "purple", size = 2, linetype = "dotted") +
  annotate("text", x = 19.5, y = 0.08, label = "19.5\n(with correction)", 
           color = "darkgreen", size = 6, fontface = "bold") +
  annotate("text", x = 20, y = 0.05, label = "20\n(without correction)", 
           color = "purple", size = 6, fontface = "bold") +
  labs(title = "P(X < 20): Binomial (bars) vs Normal (red curve)",
       subtitle = "Continuity correction uses 19.5 instead of 20",
       x = "Number of successes", y = "Probability/Density") +
  theme_minimal(base_size = 20)
```

## When to Use Each Method

::: {.callout-note}
## Choosing Between Binomial and Normal

**Use Exact Binomial when:**

- $n$ is small (easy to compute)
- Need maximum accuracy
- Conditions for Normal approximation not met

**Use Normal Approximation when:**

- $n$ is large (binomial computation difficult)
- $np \geq 10$ AND $n(1-p) \geq 10$
- Need a quick estimate
- With modern software, binomial is often feasible anyway!
:::

. . .

::: {.callout-warning}
In this class, we'll primarily use **exact** calculations with R, but you should understand when approximation is appropriate!
:::

# Summary & Wrap-Up {background-color="#3070BF"}

Key takeaways from today

## What We Learned Today

::: {.callout-important icon=false}
## Key Concepts

1. **Normal Distribution Basics**
   - Bell-shaped, symmetric, characterized by $\mu$ and $\sigma$
   - Most important distribution in statistics

2. **Z-scores**
   - Standardize observations: $Z = \frac{X - \mu}{\sigma}$
   - Allow comparison across different scales

3. **Empirical Rule**
   - 68% within 1 SD, 95% within 2 SD, 99.7% within 3 SD
   - Quick probability estimates
:::

## What We Learned Today (cont.)

::: {.callout-important icon=false}
## Key Concepts (continued)

4. **Calculating Probabilities**
   - `pnorm()`: Find probability given value
   - Always specify `mean`, `sd`, and `lower.tail`

5. **Finding Percentiles**
   - `qnorm()`: Find value given probability
   - Critical for clinical thresholds

6. **Normal Approximation to Binomial**
   - Works when $np \geq 10$ and $n(1-p) \geq 10$
   - Use continuity correction for better accuracy
:::

## R Functions Summary

| Task | Function | Example |
|------|----------|---------|
| Find $P(X \leq x)$ | `pnorm(x, mean, sd)` | `pnorm(85, 80, 12)` |
| Find $P(X > x)$ | `pnorm(x, mean, sd, lower.tail=F)` | `pnorm(85, 80, 12, lower.tail=F)` |
| Find $x$ for given probability | `qnorm(p, mean, sd)` | `qnorm(0.95, 80, 12)` |
| Binomial probability | `pbinom(k, size, prob)` | `pbinom(19, 100, 0.25)` |

## Practice Recommendations

::: {.callout-tip}
## To Master This Material

1. **Practice Z-score calculations** by hand first
2. **Draw pictures** for every probability problem
3. **Try both table and R** to build understanding
4. **Work through textbook examples** 3.17-3.32
5. **Complete homework problems** - they reinforce key concepts
6. **Use online visualizations** to build intuition
:::

## Looking Ahead

::: {.callout-note icon=false}
## Next Steps

**Next class:** Sampling distributions (Chapter 4)

- How sample means behave
- Central Limit Theorem (uses Normal distribution!)
- Standard errors

**Why today matters:**

- Normal distribution is the **foundation** for inference
- Z-scores appear throughout statistics
- Probability calculations are essential for hypothesis testing and confidence intervals
:::

## Questions?

::: {.callout-note icon=false}
## Office Hours & Resources

- **Office hours:** [Your times]
- **Textbook:** Section 3.3 (pages 152-167)
- **Practice problems:** See Sakai/Canvas
- **R help:** RStudio has great documentation for `?pnorm` and `?qnorm`

Don't hesitate to reach out if you need help!
:::

# Optional: Poisson Distribution {background-color="#CC5500"}

*Brief introduction - details in optional recording*

## What is the Poisson Distribution?

::: {.callout-note icon=false}
## Quick Overview
The **Poisson distribution** models **counts of rare events**:

- Number of earthquakes per year
- Number of hospital admissions per day
- Number of typos per page
- Number of accidents per week

**Key feature:** Events occur randomly at a constant **rate** $\lambda$
:::

. . .

$$P(X = k) = \frac{e^{-\lambda}\lambda^k}{k!}, \quad k = 0, 1, 2, \ldots$$

- Mean = $\lambda$
- Standard deviation = $\sqrt{\lambda}$

## When to Recognize Poisson

::: {.callout-important}
## You might need Poisson when:

- Counting **rare events**
- Events occur at a **rate** (per time, per area, per volume)
- Events are **independent**
- Examples:
  - Disease incidence rates
  - Customer arrivals
  - Radioactive decay
  - Manufacturing defects
:::

. . .

**For more details:** Watch the optional Poisson lecture recording on Canvas!

## End of Lecture {background-color="#3070BF"}

**Great work today!**

See you next time for Sampling Distributions.

Don't forget:

- Review Z-scores and probability calculations
- Complete homework problems
- Practice with R functions
