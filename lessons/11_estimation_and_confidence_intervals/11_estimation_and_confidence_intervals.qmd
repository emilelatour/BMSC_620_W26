---
title: "Sampling Distributions and Confidence Intervals"
subtitle: "Textbook Sections 4.1–4.2"
author: "Emile Latour, Nicky Wakim, Meike Niederhausen"
date: "`r library(here); source(here('class_dates.R')); w5d1`"
date-format: long
format:
  revealjs:
    theme: "../../assets/css/reveal-bmsc620_v5.scss"
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: "BMSC 620 | Sampling & Confidence Intervals"
    html-math-method: mathjax
    chalkboard: true
execute:
  echo: true
  warning: false
  message: false
  freeze: auto
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(glue)
library(knitr)
library(patchwork)
library(moderndive)
library(oibiostat)
library(see)

library(lamisc)
library(laviz)        

# Set theme for plots
theme_set(laviz::theme_minimal_white(grid = "none", 
                                     axis = "xy", 
                                     base_size = 16))

set.seed(456)
```

## 

![[Artwork by @allison_horst](https://allisonhorst.com/)](/img_slides/horst_samples.png){fig-align="center"}

# Learning Objectives 

By the end of today's lecture, you will be able to:

1. Distinguish between population parameters and sample statistics
2. Explain the concept of sampling variability and the sampling distribution
3. Apply the Central Limit Theorem to describe the distribution of sample means
4. Calculate and interpret confidence intervals for a population mean
5. Understand when to use the t-distribution vs. the normal distribution

## Roadmap for Today

:::: {.columns}
::: {.column width="50%"}
**Part 1: Sampling Fundamentals**

- Population parameters vs. sample statistics
- Point estimates
- Sampling variability

**Part 2: Sampling Distributions**

- What is a sampling distribution?
- Properties of the sampling distribution of means
- Standard error

**Part 3: Central Limit Theorem**

- Statement of the CLT
- When the CLT applies
- Applications with R
:::

::: {.column width="50%"}
**Part 4: Introduction to Inference**

- From point estimates to interval estimates
- Confidence intervals: concept and interpretation

**Part 5: Confidence Intervals in Practice**

- CI when σ is known (z-based)
- CI when σ is unknown (t-based)
- The t-distribution

**Part 6: Wrap-up**

- Summary
- Common misconceptions
- Next steps
:::
::::

# Sampling Fundamentals

## Why do we sample?

::: {.callout-note icon="false"}
## The fundamental challenge of statistics

We want to learn about a **population**, but we can only observe a **sample**.
:::

\

**Populations:**

- Too large to measure everyone
- Too expensive or time-consuming
- Sometimes impossible (would you destroy every lightbulb to test lifespan?)

\

**Samples:**

- Smaller, manageable
- If chosen properly, can tell us about the population
- But there's uncertainty...

## From Week 1: Population vs. sample

:::: {.columns}
::: {.column width="48%"}
::: {.callout-important icon="false"}
### (Target) Population

* Group of interest being studied
* Group from which the sample is selected
  - studies often have _inclusion_ and/or _exclusion_ criteria
* Almost always too expensive or logistically impossible to collect data for every case in a population
:::
:::

::: {.column width="48%"}
::: {.callout-tip icon="false"}
### Sample

* Group on which data are collected
* A subset (of measurements) from the population
:::
:::
:::


- We use information from a sample to learn about the population from which it was drawn.
- Goal is to get a __representative__ sample of the population: the characteristics of the sample are similar to the characteristics of the population

## Population vs. Sample: Visual

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 12

# Create population
pop_size <- 10000
pop_data <- tibble(
  id = 1:pop_size,
  value = rnorm(pop_size, mean = 65, sd = 3),
  type = "Population"
)

# Take a sample
sample_size <- 50
sample_ids <- sample(pop_data$id, sample_size)
sample_data <- pop_data %>%
  mutate(type = if_else(id %in% sample_ids, "Sample", "Not sampled"))

# Plot
p1 <- ggplot(pop_data, aes(x = value)) +
  geom_histogram(fill = "#999999", color = "black", bins = 40) +
  geom_vline(xintercept = mean(pop_data$value), 
             color = "red", linewidth = 1.5, linetype = "dashed") +
  labs(
    title = "Population Distribution",
    subtitle = glue("μ = {round(mean(pop_data$value), 2)}, σ = {round(sd(pop_data$value), 2)}"),
    x = "Height (inches)",
    y = "Count", 
    caption = "Ex: represents all 10,000 adults in a particular city"
  ) + 
  theme(plot.caption = element_text(hjust = 0.0))

p2 <- ggplot(sample_data %>% filter(type == "Sample"), aes(x = value)) +
  geom_histogram(fill = "#0072B2", color = "black", bins = 15) +
  geom_vline(xintercept = mean(sample_data$value[sample_data$type == "Sample"]), 
             color = "blue", linewidth = 1.5, linetype = "dashed") +
  labs(
    title = "Sample Distribution",
    # subtitle = glue("x̄ = {round(mean(sample_data$value[sample_data$type == 'Sample']), 2)}, s = {round(sd(sample_data$value[sample_data$type == 'Sample']), 2)}"),
    subtitle = bquote(bar(x) == .(round(mean(sample_data$value[sample_data$type == 'Sample']), 2)) ~ ", " ~ s == .(round(sd(sample_data$value[sample_data$type == 'Sample']), 2))),
    x = "Height (inches)",
    y = "Count", 
    caption = "Ex: represents 50 randomly selected people\nfrom the population of 10,000."
  )+ 
  theme(plot.caption = element_text(hjust = 0.0))

p1 + p2
```

## Population parameters vs. Sample statistics

Understanding the notation is crucial for clear statistical thinking.

\

:::: {.columns}
::: {.column width="48%"}
::: {.callout-important icon="false"}
## Population Parameter

Fixed (but unknown) values describing the population

**For the mean:**

- Symbol: $\mu$ (mu)
- We want to know it but usually can't measure it

**For standard deviation:**

- Symbol: $\sigma$ (sigma)
- Also fixed and unknown

**For proportion:**

- Symbol: $p$ or $\pi$ (pi)
:::
:::

::: {.column width="48%"}
::: {.callout-tip icon="false"}
## Sample Statistic

Calculated values from our sample data

**For the mean:**

- Symbol: $\bar{x}$ (x-bar)
- Our best guess at $\mu$

**For standard deviation:**

- Symbol: $s$
- Our estimate of $\sigma$

**For proportion:**

- Symbol: $\hat{p}$ (p-hat)
:::
:::
:::

## What is a point estimate?

A **point estimate** is a single value calculated from sample data used to estimate a population parameter.

\

**Examples:**

- Sample mean ($\bar{x}$) estimates population mean ($\mu$)
- Sample proportion ($\hat{p}$) estimates population proportion ($p$)
- Sample standard deviation ($s$) estimates population SD ($\sigma$)

\

::: {.callout-warning icon="false"}
## The problem with point estimates

They're just single numbers. They don't tell us:

- How much uncertainty there is
- How close we might be to the true value
- Whether our sample was typical or unusual
:::

## Sampling variability: A demonstration in R (1/2)

Let's see what happens when we take multiple samples from the same population.

```{r}
# Create a population
population <- tibble(
  height = rnorm(10000, mean = 65, sd = 3)
)

# Take 5 samples of size 50
results <- tibble(
  sample_num = 1:5,
  mean_height = NA_real_  # Initialize with missing values
)

# Calculate mean for each sample
for (i in 1:5) {
  one_sample <- sample(population$height, size = 50)   # Take a random sample
  results$mean_height[i] <- mean(one_sample)           # Calculate the mean
}

```

## Sampling variability: A demonstration in R (2/2)

The results from taking 

- 5 random samples, 
- each size 50, 
- from our population of 10,000

```{r}
results
```

\

**Notice:** Even from the same population, our sample means vary! This is **sampling variability** - it's not error, it's natural variation.

## Visualizing sampling variability (1/3)

What if we took many, many samples?

- From the same population size 10,000 with $\mu = 65$ and $\sigma = 3$

```{r}
# Take 1000 samples, each of size 50
many_samples <- tibble(
  sample_num = 1:1000,
  mean_height = NA_real_  # Initialize with missing values
)

# Calculate mean for each sample
for (i in 1:1000) {
  one_sample <- sample(population$height, size = 50)   # Take a random sample
  many_samples$mean_height[i] <- mean(one_sample)      # Calculate the mean
}

```

## Visualizing sampling variability (2/3)

What if we took many, many samples?

- From the same population size 10,000 with $\mu = 65$ and $\sigma = 3$

```{r}
dim(many_samples)
head(many_samples)

```

## Visualizing sampling variability (3/3)

What if we took many, many samples?

```{r}
#| fig-height: 5
#| fig-width: 10
#| echo: false

ggplot(many_samples, aes(x = mean_height)) +
  geom_histogram(bins = 40, fill = "#0072B2", color = "black", alpha = 0.7) +
  geom_vline(xintercept = 65, color = "red", linewidth = 1.5, linetype = "dashed") + 
  labs(
    title = "Distribution of 1000 sample means (n = 50)",
    subtitle = "Red line shows true population mean (μ = 65)",
    x = expression(Sample~mean~(bar(x))),
    y = "Count"
  )
```

# The Sampling Distribution

## What is a sampling distribution?

::: {.callout-note icon="false"}
## Definition

The **sampling distribution** of a statistic is the distribution of that statistic's values across all possible samples of a given size from a population.
:::

\

**Think of it this way:**

1. Imagine taking a sample of size $n$
2. Calculate a statistic (like the mean)
3. Write it down
4. Repeat steps 1-3 for **all possible samples**
5. The distribution of those statistics is the sampling distribution

\

**Key insight:** The sampling distribution tells us how our estimates behave across different samples.

## Three distributions to keep straight

:::: {.columns}
::: {.column width="32%"}
::: {.callout-note icon="false"}
## Population Distribution

- Distribution of the variable in the population
- Mean: $\mu$, SD: $\sigma$
- **Fixed, but unknown**
- We never observe this directly
:::
:::

::: {.column width="32%"}
::: {.callout-tip icon="false"}
## Sample Distribution

- Distribution of the variable in one sample
- Mean: $\bar{x}$, SD: $s$
- **Random** (changes sample to sample)
- What we actually observe
:::
:::

::: {.column width="32%"}
::: {.callout-important icon="false"}
## Sampling Distribution

- Distribution of a sample statistic across many samples
- Mean: $\mu_{\bar{X}} = \mu$, SD (SE): $\frac{\sigma}{\sqrt{n}}$
- **Theoretical** (describes variability of $\bar{x}$)
- Not the distribution of raw data!
:::
:::
::::


## Visual: Three distributions

```{r}
#| echo: false
#| fig-height: 5
#| fig-width: 13

set.seed(101)

# Population
pop <- tibble(value = rnorm(50000, 65, 3))

# One sample
one_sample <- sample(pop$value, 50)

# Many sample means
many_means <- replicate(5000, mean(sample(pop$value, 50)))

p1 <- ggplot(pop, aes(x = value)) +
  geom_histogram(bins = 50, fill = "#999999", color = "black") +
  labs(title = "Population Distribution",
       subtitle = "All individuals (N = 50,000)",
       x = "Height", y = "Count")

p2 <- ggplot(tibble(value = one_sample), aes(x = value)) +
  geom_histogram(bins = 15, fill = "#0072B2", color = "black") +
  labs(title = "Sample Distribution",
       subtitle = "One sample (n = 50)",
       x = "Height", y = "Count")

p3 <- ggplot(tibble(mean_value = many_means), aes(x = mean_value)) +
  geom_histogram(bins = 40, fill = "#E69F00", color = "black") +
  labs(title = "Sampling Distribution",
       subtitle = "Distribution of sample means\n(5000 samples of n = 50)",
       x = expression(Sample~mean~(bar(x))),
       y = "Count")

p1 + p2 + p3
```

## Why does the standard error exist?

Before we introduce the formula, let's understand the concept:

\

:::: {.columns}
::: {.column width="50%"}
**The logic:**

- Each random sample produces a slightly different estimate
- Those estimates vary from sample to sample
- That variability forms a sampling distribution
::: 

::: {.column width="50%"}
**The standard error (SE) is:**

- The standard deviation of the sampling distribution
- A measure of how much a statistic varies across repeated samples
:::
::::

\

::: {.callout-important icon="false"}
## Key distinction

**Standard error** quantifies **sampling variability**, not data variability.

- Standard deviation ($s$) → spread of data in one sample
- Standard error ($SE$) → spread of statistics across many samples
:::

## Standard error: A special name

The standard deviation of a sampling distribution has a special name:

::: {.callout-tip icon="false"}
## Standard Error (SE)

The **standard error** is the standard deviation of a sampling distribution.

For the sampling distribution of sample means:

$$SE = \frac{\sigma}{\sqrt{n}}$$

where $\sigma$ = population standard deviation and $n$ = sample size
:::


**What does SE tell us?**

The SE describes how far the sample mean ($\bar{x}$) is expected to deviate from the true population mean ($\mu$) across many different random samples of size $n$.


**Key properties:**

- Larger samples → smaller SE → more precise estimates
- SE decreases as $\sqrt{n}$ increases, not as $n$ (doubling sample size doesn't halve SE.)
- In practice, we rarely know $\sigma$, so we use: $SE = \frac{s}{\sqrt{n}}$

## When to report SE vs. SD

When presenting results, choose based on your goal:

:::: {.columns}
::: {.column width="48%"}
::: {.callout-note icon="false"}
## Report SD when...

**Goal:** Describe the data

**Use:** $\bar{x} \pm s$

**Example:** "Heights were 65.2 ± 3.1 inches"

**Interpretation:** Shows the spread of individual observations
:::
:::

::: {.column width="48%"}
::: {.callout-tip icon="false"}
## Report SE when...

**Goal:** Estimate population parameter

**Use:** $\bar{x} \pm SE$

**Example:** "Mean height was 65.2 ± 0.44 inches"

**Interpretation:** Shows precision of the estimate
:::
:::
::::

\

::: {.callout-warning icon="false"}
## Common mistake

Don't report SE to make your data look "better" (less variable). Use SD to describe variability in your sample, SE to quantify uncertainty about the population mean.
:::



## Why does sample size matter?

Let's see the effect of sample size on the sampling distribution:

```{r}
#| fig-height: 6
#| fig-width: 12
#| echo: false

pop <- tibble(value = rnorm(10000, 65, 3))

# Different sample sizes
n_vals <- c(10, 30, 100)

results_list <- map(n_vals, function(n) {
  tibble(
    sample_mean = replicate(2000, mean(sample(pop$value, n))),
    n = glue("n = {n}")
  )
})

results <- bind_rows(results_list) %>% 
  mutate(n = factor(n, 
                    levels = c("n = 10", 
                               "n = 30", 
                               "n = 100")))

ggplot(results, aes(x = sample_mean)) +
  geom_histogram(bins = 40, fill = "#0072B2", color = "black", alpha = 0.7) +
  geom_vline(xintercept = 65, color = "red", linewidth = 1, linetype = "dashed") +
  facet_wrap(~ n, ncol = 3) +
  labs(
    title = "Sampling distributions for different sample sizes",
    subtitle = "Notice how the spread decreases as n increases",
    x = expression(Sample~mean~(bar(x))),
    y = "Count"
  )
```

# Central Limit Theorem

## The Central Limit Theorem (CLT)

::: {.callout-important icon="false"}
## Central Limit Theorem

For **sufficiently large** sample sizes, the sampling distribution of the sample mean is approximately normal, **regardless of the shape of the population distribution**.

Specifically, if we have a random sample of size $n$ from a population with mean $\mu$ and standard deviation $\sigma$:

$$\bar{X} \sim N\left(\mu_{\bar{X}} = \mu, \quad SE = \frac{\sigma}{\sqrt{n}}\right)$$
:::

\

**The key question:** What counts as "sufficiently large"?

## When can we use the CLT?

The required sample size depends on the shape of the population distribution:

:::: {.columns}
::: {.column width="48%"}
**Population approximately normal**

- CLT works for **any sample size**
- Even $n = 5$ is fine
- The sampling distribution is exactly normal

**Population slightly skewed**

- Usually $n \geq 30$ is sufficient
- This is the common "rule of thumb"
:::

::: {.column width="48%"}
**Population highly skewed**

- May need $n \geq 50$ or even larger
- The "30" rule doesn't apply here!
- More skewness → need larger $n$

**Population with extreme outliers**

- May need $n \geq 100$ or more
- Outliers slow down convergence to normality
:::
::::

\

::: {.callout-tip icon="false"}
## In practice

Look at your sample data:

- Is it approximately symmetric with no extreme outliers? → $n \geq 30$ likely okay
- Is it very skewed or has outliers? → Consider larger $n$ or non-parametric methods
:::

## CLT in action: Starting with a skewed population

Let's see what happens when we start with a **highly skewed** population:
```{r}
#| fig-height: 6
#| fig-width: 13
#| echo: false

# Create a very skewed population (exponential)
skewed_pop <- tibble(value = rexp(10000, rate = 0.2))

p1 <- ggplot(skewed_pop, aes(x = value)) +
  geom_histogram(bins = 50, fill = "#999999", color = "black") +
  labs(title = "Population Distribution (Highly Right-Skewed)",
       subtitle = "This is definitely not normal!",
       x = "Value", y = "Count") +
  coord_cartesian(xlim = c(0, 40))

p1
```

## Sampling distributions at different sample sizes

Now watch what happens to the sampling distribution as we increase $n$:
```{r}
#| fig-height: 7
#| fig-width: 13
#| echo: false

# Sample means for different n
n_vals <- c(5, 15, 30, 100)

sampling_dists <- map(n_vals, function(n) {
  tibble(
    sample_mean = replicate(3000, mean(sample(skewed_pop$value, n))),
    n = glue("n = {n}")
  )
}) %>% bind_rows() %>% 
  mutate(n = factor(n, levels = c("n = 5", "n = 15", "n = 30", "n = 100")))

p2 <- ggplot(sampling_dists, aes(x = sample_mean)) +
  geom_histogram(bins = 40, fill = "#E69F00", color = "black") +
  facet_wrap(~ n, ncol = 2, scales = "free_y") +
  labs(title = "Sampling Distributions of the Mean at Different Sample Sizes",
       subtitle = "Notice: Even with n = 30, there's still some right skew. By n = 100, it's quite normal.",
       x = expression(Sample~mean~(bar(x))),
       y = "Count")

p2
```

## Why the CLT is remarkable

**The CLT works even if the population is:**

- Slightly or moderately skewed
- Uniform  
- Bimodal
- Many other non-normal shapes

\

**The key insight:** Averages are less variable than individual observations, and with enough averaging (large enough $n$), the distribution of those averages becomes normal.

\

::: {.callout-warning icon="false"}
## Don't blindly trust n ≥ 30

The "$n \geq 30$" rule is a rough guideline, not a guarantee. 

- For symmetric distributions, 30 is usually plenty
- For highly skewed distributions (like we just saw), you may need 50, 100, or more
- Always look at your actual data before trusting the CLT
:::

## Why is this useful?

\

- Routine studies involve data from a single sample, not repeated samples.
- If $n$ is large, then regardless of the distribution of the original population, CLT provides a way of treating our single sample mean as one observation from a normal distribution. 
- The distribution of sample means derived from discrete distributions will also be normal provided $n$ is large.

## Applying the CLT: Example

::: {.callout-note icon="false"}
## Example: Heights

Suppose the heights of adults in a population have mean $\mu = 65$ inches and standard deviation $\sigma = 3.5$ inches. We take a random sample of 50 adults.

**What is the probability that the sample mean (yet to be determined) is greater than 66 inches?**
:::

\

**Step 1:** Check if we can use CLT

- $n = 50 \geq 30$ ✓
- Heights are generally approximately normal (or at least not heavily skewed) ✓
- We can assume the sampling distribution of $\bar{X}$ is approximately normal

**Step 2:** Find the distribution of $\bar{X}$

$$\bar{X} \sim N\left(\mu = 65, \quad SE = \frac{3.5}{\sqrt{50}} = 0.495\right)$$

## Example continued: Using R

**Step 3:** Calculate the probability using R

We want $P(\bar{X} > 66)$

```{r}
#| echo: false
#| fig-height: 5

# Define parameters
n <- 50
mu <- 65
sigma <- 3.5

# Calculate SE
SE <- sigma / sqrt(n)
# SE

sig <- SE
a <- 66
# b <- 99

x <- seq(mu - 4 * sig, mu + 4 * sig, length.out = 1000)
y <- dnorm(x, mean = mu, sd = sig)

df <- tibble(x = x, y = y)

ggplot(df, aes(x = x, y = y)) +
  geom_line(linewidth = 1) +
  # geom_area(data = df %>% filter(x >= a & x <= b),
  #           fill = "coral", alpha = 0.6) +
  geom_area(data = df %>% filter(x >= a),
            fill = "#E69F00", alpha = 0.6) +
  geom_segment(aes(x = a, 
                   xend = a, 
                   y = 0, 
                   yend = dnorm(a, mu, sig)), 
               linetype = "dashed") + 
  # geom_segment(aes(x = b, 
  #                  xend = b, 
  #                  y = 0, 
  #                  yend = dnorm(b, mu, sigma)), 
  #              linetype = "dashed") + 
  annotate("text", x = a + 0.3, y = 0.20,
           # label = glue::glue("P(90 ≤ X ≤ 99) = {norm_diff}"),
           label = expression(P(bar(X) > 66)),
           size = 6) +
  scale_x_continuous(breaks=c(mu-2*sig,mu-1*sig,mu, mu+1*sig, mu+2*sig), 
                     labels = \(x) lamisc::fmt_num(x = x, accuracy = 1.0)) + 
  scale_y_continuous(labels = NULL, 
                     breaks = NULL, 
                     expand = c(0, 0)) +
  labs(x = "Mean height from samples (inches)",
       y = NULL,
       title = NULL) + 
  laviz::theme_minimal_white(grid = "none", 
                                     axis = "xy", 
                             ticks = TRUE, 
                                     base_size = 16)
```


## Example continued: Using R

**Step 3:** Calculate the probability using R

We want $P(\bar{X} > 66)$


```{r}
# Define parameters
n <- 50
mu <- 65
sigma <- 3.5

# Calculate SE
SE <- sigma / sqrt(n)
SE
```

\

```{r}
# Calculate probability
pnorm(q = 66, mean = mu, sd = SE, lower.tail = FALSE)
```


\

**Interpretation:** There is about a 2.2% chance of observing a sample mean greater than 66 inches if the true population mean is 65 inches.





## What the CLT tells us in plain language

The Central Limit Theorem means:

1. **Sample means tend toward normality** (for large enough $n$, even if the data aren't normal)
2. **Sample means cluster around the population mean** ($\mu$)
3. **The spread depends on sample size** (larger $n$ → smaller spread)

\

**Why this matters:**

- We can use normal distribution tools even when our data aren't normal
- We can quantify uncertainty about sample means
- We can make probability statements (like we just did)
- This is the foundation for confidence intervals and hypothesis tests

\

::: {.callout-tip icon="false"}
## Looking ahead

The CLT is why we can construct confidence intervals and do hypothesis tests even when our data aren't perfectly normal - as long as our sample size is large enough!
:::

# Introduction to Inference

## From estimation to inference

So far we've learned:

- Population parameters vs. sample statistics
- Sampling distributions
- The Central Limit Theorem

\

**Now we ask a bigger question:**

::: {.callout-note icon="false"}
## The inference question

Given a sample statistic (like $\bar{x} = 66.1$), what can we say about the population parameter ($\mu$)?
:::

\

**Point estimates aren't enough** - they give us one number but no sense of uncertainty.

**Solution:** Use interval estimates!

## Point estimates vs. Interval estimates

:::: {.columns}
::: {.column width="48%"}
::: {.callout-warning icon="false"}
## Point Estimate

A single value used to estimate a parameter

**Example:** "The mean height is 66.1 inches"

**Pros:**

- Simple
- Easy to communicate

**Cons:**

- No uncertainty quantified
- Doesn't acknowledge sampling variability
:::
:::

::: {.column width="48%"}
::: {.callout-tip icon="false"}
## Interval Estimate

A range of plausible values for a parameter

**Example:** "The mean height is between 65.1 and 67.1 inches"

**Pros:**

- Quantifies uncertainty
- More honest about what we know

**Cons:**

- Less precise
- Requires interpretation
:::
:::
::::

## What is a confidence interval?

::: {.callout-important icon="false"}
## Confidence Interval

A **confidence interval** is a range of values that is likely to contain the true population parameter with a specified level of confidence.

**General form:**

$$\text{point estimate} \pm \text{margin of error}$$

For a mean:

$$\bar{x} \pm \text{(critical value)} \times SE$$
:::

\

**The critical value depends on:**

- The confidence level (commonly 95%)
- The distribution we're using (normal or t)

## Some new notation

Before we construct confidence intervals, we need to understand the notation for critical values:

```{r}
#| echo: false

mu <- 0
sigma <- 1

df <- tibble::tibble(
  x = seq(mu - 4*sigma, mu + 4*sigma, length.out = 1200),
  y = dnorm(x, mean = mu, sd = sigma)
)

# a <- qnorm(1 - 0.175 / 2, lower.tail = T)
# b <- qnorm(1 - 0.175 / 2, lower.tail = F)

alpha <- 0.175  # Fudged to make the plot readable
a <- qnorm(1 - alpha/2)   # positive
b <- qnorm(alpha/2)       # negative

ya <- dnorm(a, mean = mu, sd = sigma)
yb <- dnorm(b, mean = mu, sd = sigma)

ggplot() +
  # Draw the full distribution curve
  geom_line(data = df,
            aes(x = x, y = y), 
            linewidth = 0.8) +
  
  geom_area(data = df %>% filter(x <= b),
            aes(x = x, y = y), 
            fill = "gray80", 
            alpha = 0.8) +
  geom_segment(aes(x = b, xend = b, 
                   y = 0, yend = yb), 
               linewidth = 0.8) +
  
  geom_area(data = df %>% filter(x >= a),
            aes(x = x, y = y), 
            fill = "gray80", 
            alpha = 0.8) +
  geom_segment(aes(x = a, xend = a, 
                   y = 0, yend = ya), 
               linewidth = 0.8) + 
  
  annotate("text", 
           x = 0, 
           y = 0.15, 
           label = expression((1 - alpha)),
         size = 7) + 
  
  # Left tail annotation (α/2)
  annotate("text", 
           x = -3,  # adjust position as needed
           y = 0.1, 
           label = expression(alpha/2),
           size = 5) + 
  geom_segment(aes(x = -2.8, xend = -1.9, 
                   y = 0.1, yend = 0.05), 
               linewidth = 0.5) + 
  # Right tail annotation (α/2)
  annotate("text", 
           x = 3,  # adjust position as needed
           y = 0.1, 
           label = expression(alpha/2),
           size = 5) + 
  geom_segment(aes(x = 2.8, xend = 1.9, 
                   y = 0.1, yend = 0.05), 
               linewidth = 0.5) +
  
  # X-axis labels
  scale_x_continuous(
    breaks = c(-4, -2, b, 0, a, 2, 4),
    labels = expression(-4, -2, -z[1-alpha/2], 0, z[1-alpha/2], 2, 4)
  ) + 
  
  scale_y_continuous(expand = c(0, 0), limits = c(0, max(df$y) * 1.1)) +
  
  # Labels and theme
  labs(x = "Z", y = "Density")
```


\

- $\pm z_{1-\alpha/2}$ is the value of $z$ such that $(1 - \alpha) \times 100\%$ of the standard normal distribution is contained <br>between $- z_{1-\alpha/2}$ and $+ z_{1-\alpha/2}$.
- Equivalently, $\alpha \times 100\%$ is greater than $+ z_{1-\alpha/2}$ and less than $- z_{1-\alpha/2}$ combined. 

# Confidence Intervals: The Basics

## Visualizing confidence intervals (1/2)

Let's look at what confidence intervals represent:

::::::: columns
:::: {.column width="55%"}


<!-- Simulating Confidence Intervals: <http://www.rossmanchance.com/applets/ConfSim.html> -->

The figure shows CIs from 100 samples:

-   100 samples: Calculate the mean and confidence interval of each sample
-   The true value of $\mu =65$ is the vertical black line
-   The horizontal lines are 95% CIs from 100 samples
    -   [**Blue**]{style="color:#0072B2"}: the CI contains the true value of $\mu$
    -   [**Red**]{style="color:#C83532"}: the CI *did not* contain the true value of $\mu$

\n

**What percent of CIs captured the true value of $\mu$?**

::::

::: {.column width="10%"}
:::

::: {.column width="25%"}
```{r}
#| echo: false
#| fig-height: 11
#| fig-width: 5

# True population
pop <- tibble(value = rnorm(10000, mean = 65, sd = 3))
true_mu <- 65

# Take 100 samples and calculate CIs
n <- 50
z_star <- 1.96
sigma <- 3

ci_data <- tibble(
  sample_num = 1:100
) %>%
  mutate(
    sample_mean = map_dbl(sample_num, ~ mean(sample(pop$value, n))),
    se = sigma / sqrt(n),
    lower = sample_mean - z_star * se,
    upper = sample_mean + z_star * se,
    contains_mu = lower <= true_mu & upper >= true_mu
  )

# ci_plot <- ggplot(ci_data, aes(y = sample_num, color = contains_mu)) +
#   geom_point(aes(x = sample_mean), size = 3) +
#   geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2, linewidth = 1) +
#   geom_vline(xintercept = true_mu, linewidth = 1.5, linetype = "dashed", color = "black") +
#   scale_color_manual(values = c("TRUE" = "#0072B2", 
#                                 "FALSE" = "red"),
#                      # labels = c("Contains μ", "Misses μ")) +
#                      labels = rev(c("Contains μ", "Misses μ"))) +
#                      
#   labs(
#     title = NULL,
#     subtitle = NULL,
#     x = expression(mu*' = 65' ),
#     y = NULL,
#     color = ""
#   ) + 
#   laviz::theme_minimal_white(grid = "none", 
#                                      axis = "none", 
#                                      base_size = 24) + 
#   theme(legend.position = "top", 
#         axis.text.x = element_blank(), 
#         axis.text.y = element_blank())


ci_plot <- ci_data %>% 
  mutate(contains_mu = factor(contains_mu, 
                              levels = c(TRUE, FALSE), 
                              labels = c("Contains μ", "Misses μ"))) %>% 
ggplot(aes(y = sample_num, color = contains_mu)) +
  geom_point(aes(x = sample_mean), size = 3) +
  geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2, linewidth = 1) +
  geom_vline(xintercept = true_mu, linewidth = 1.5, linetype = "dashed", color = "black") +
  scale_color_manual(values = c("Contains μ" = "#0072B2", 
                                "Misses μ" = "red")) +
                     
  labs(
    title = NULL,
    subtitle = NULL,
    x = expression(mu*' = 65' ),
    y = NULL,
    color = ""
  ) + 
  laviz::theme_minimal_white(grid = "none", 
                                     axis = "none", 
                                     base_size = 24) + 
  theme(legend.position = "top", 
        axis.text.x = element_blank(), 
        axis.text.y = element_blank())

ci_plot
```
:::
:::::::


## Visualizing confidence intervals (2/2)

Let's look at what confidence intervals represent:

::::::: columns
:::: {.column width="55%"}


\

#### Interpretation $(1 - \alpha) \times 100\%$

\

If many samples are collected from a population, and a confidence interval is calculated for each one.

\

We expect that $(1 - \alpha) \times 100\%$ of those intervals will contain the true population mean, $\mu$.

::::

::: {.column width="10%"}
:::

::: {.column width="25%"}
```{r}
#| echo: false
#| fig-height: 11
#| fig-width: 5


ci_plot
```
:::
:::::::

## How do we interpret confidence intervals? 

**Actual interpretation:**

-   If we were to
    -   **repeatedly take random samples** from a population and
    -   calculate a 95% CI for each random sample,
-   then we would **expect 95% of our CIs to contain the true population parameter** $\mu$.

<!-- "Real life": -->

<!-- * We typically only take 1 random sample.   -->

<!-- * How do we know if our CI is a lucky or unlucky one? -->

\

**What we typically write as "shorthand":**

-   In general form: We are 95% *confident* that (the 95% confidence interval) captures the value of the population parameter.

\

**WRONG interpretation:**

-   There is a 95% *chance* that (the 95% confidence interval) captures the value of the population parameter.
    -   For one CI on its own, it either does or doesn't contain the population parameter with probability 0 or 1. We just don't know which!



## Confidence interval when σ is known

When we **know** the population standard deviation $\sigma$:

::: {.callout-tip icon="false"}
## CI for μ (with known σ)

$$\bar{x} \pm z^* \times \frac{\sigma}{\sqrt{n}}$$

where:

- $\bar{x}$ = sample mean
- $z^*$ = critical value from standard normal distribution
- $\sigma$ = population standard deviation (known)
- $n$ = sample size
:::

\

**For a 95% confidence interval:**

```{r}
qnorm(0.975)  # 2.5% in each tail
```

So $z^* = 1.96$

## What makes a confidence interval wide or narrow?

Before we calculate CIs, let's build intuition about what affects their width:

\

:::: {.columns}
::: {.column width="48%"}
**CI gets narrower when:**

- Sample size increases  
  ($\uparrow n$ → $\downarrow SE$)
- Population variability is smaller  
  ($\downarrow \sigma$ → $\downarrow SE$)
:::

::: {.column width="48%"}
**CI gets wider when:**

- Sample size is small  
  ($\downarrow n$ → $\uparrow SE$)
- Population variability is large  
  ($\uparrow \sigma$ → $\uparrow SE$)
- Confidence level increases  
  (99% vs 95% → larger critical value)
:::
::::

\

::: {.callout-tip icon="false"}
## Nothing else affects CI width

You can only make a CI narrower by:

1. Collecting more data
2. Reducing measurement error  
3. Accepting less confidence
:::

## Example: CI with known σ

::: {.callout-note icon="false"}
## Example

- A random sample of 50 adults has mean height $\bar{x} = 66.1$ inches. 
- Assume the population standard deviation is known to be $\sigma = 3$ inches.
- Find a 95% confidence interval for the population mean height.
:::

::: {.columns}
::: {.column width="48%"}
**Solution:**

```{r}
xbar <- 66.1
sigma <- 3
n <- 50
z_star <- qnorm(0.975)  # 1.96


# Calculate SE
SE <- sigma / sqrt(n)
SE
```

:::
::: {.column width="48%"}

\
   
```{r}
# Calculate CI
lower_ci <- xbar - z_star * SE
upper_ci <- xbar + z_star * SE

c(lower_ci, upper_ci)
```

:::
:::

\

**We are 95% confident that the population mean height is between `r lamisc::fmt_num(lower_ci, 0.01)` and `r lamisc::fmt_num(upper_ci, 0.01)` inches.**

## Interpreting confidence intervals: What they mean

::: {.callout-important icon="false"}
## Correct interpretation

"We are 95% confident that the interval (`r lamisc::fmt_num(lower_ci, 0.01)`, `r lamisc::fmt_num(upper_ci, 0.01)`) contains the true population mean height."

**What this really means:**

If we were to take many samples and construct a 95% CI from each one, about 95% of those intervals would contain the true population mean $\mu$.
:::

\

**Helpful analogy:**

Think of each CI as a "net" trying to catch the true parameter. With 95% confidence, our net catches the parameter 95% of the time.

## Interpreting confidence intervals: What they DON'T mean

::: {.callout-warning icon="false"}
## Common misconceptions

**WRONG:** "There is a 95% probability that μ is in this interval."

- The parameter μ is fixed (not random)
- It either is or isn't in the interval
- The randomness comes from the sampling process

**WRONG:** "95% of the data falls in this interval."

- The CI is about the parameter, not the data
- The data is in the sample, not in the CI

**WRONG:** "If we repeat the study, there's a 95% chance the new mean will be in this interval."

- CIs are for parameters, not future statistics
:::

## What a 95% confidence interval actually means

This is the most important slide about interpretation:

\

::: {.callout-important icon="false"}
## The key to understanding CIs

**The method** used to create the interval has 95% long-run coverage.

If we repeated the study many times:

- Each time, we'd get a different sample
- Each sample would produce a different confidence interval
- About 95% of those intervals would contain the true parameter $\mu$
:::


::: {.callout-warning icon="false"}
## The critical insight

**The parameter is fixed. The interval is random.**

The confidence is about the *procedure*, not about any single interval.

You cannot assess whether a specific CI is "correct" using just one dataset. The 95% guarantee comes from the long-run behavior of the method.
:::

## Different confidence levels

We can construct CIs at different confidence levels:

```{r}
xbar <- 66.1
sigma <- 3
n <- 50

# Calculate SE
SE <- sigma / sqrt(n)
```


```{r}
# 90% CI
z_90 <- qnorm(0.95)  # 5% in each tail
c(xbar - z_90 * SE, xbar + z_90 * SE)
```

\

```{r}
# 95% CI
z_95 <- qnorm(0.975)  # 2.5% in each tail
c(xbar - z_95 * SE, xbar + z_95 * SE)
```

\

```{r}
# 99% CI
z_99 <- qnorm(0.995)  # 0.5% in each tail
c(xbar - z_99 * SE, xbar + z_99 * SE)
```

\

**Trade-off:** Higher confidence → wider interval (less precision)

**Notice:** 
- 90% CI: (65.41, 66.79) - narrowest, but least confident
- 99% CI: (65.00, 67.20) - widest, but most confident

## Different confidence levels (a different way)

Just showing another way to do with with R

```{r}
xbar <- 66.1
sigma <- 3
n <- 50
SE <- sigma / sqrt(n)

# Instead of three separate calculations:
confidence_levels <- c(0.90, 0.95, 0.99)
z_values <- qnorm(1 - (1 - confidence_levels)/2)

results <- tibble(
  level = confidence_levels,
  z_star = z_values,
  lower = xbar - z_values * SE,
  upper = xbar + z_values * SE
)

results
```


## Confidence intervals are about procedures

Let's emphasize the key conceptual point before moving on:

\

:::: {.columns}
::: {.column width="48%"}
**One dataset → one confidence interval**

- You conduct one study
- You get one sample
- You calculate one interval
- That interval either contains $\mu$ or it doesn't

**We just don't know which!**
:::

::: {.column width="48%"}
**The guarantee applies to the method, not a single interval**

- The 95% comes from the procedure's long-run behavior
- If everyone repeated your study, 95% of their CIs would contain $\mu$
- Your specific CI is one realization from that process
:::
::::

\

::: {.callout-note icon="false"}
## Coverage is a long-run property

You **cannot** assess CI correctness using one dataset.

Confidence comes from the repetition (in principle), not from the data alone.

This is why we say "we are confident" rather than "there is a probability."
:::


# The t-Distribution

## What if we don't know σ?

\

**Reality check:** We almost never know the population standard deviation $\sigma$.

\

**Problem:** If we replace $\sigma$ with $s$ in our CI formula:

$$\bar{x} \pm z^* \times \frac{s}{\sqrt{n}}$$

This **adds extra uncertainty** - we're now estimating both $\mu$ and $\sigma$!

\

**Solution:** Use a different distribution that accounts for this extra uncertainty - the **t-distribution**.

## The t-distribution

\

::: columns
::: {.column width="40%"}

::: {.callout-important icon="false"}
### Student's t-distribution

- Is symmetric and bell-shaped (like the normal)
- Has **heavier tails** than the normal distribution
  - t-based intervals will be wider than Z based intervals
- Depends on **degrees of freedom** (which for one sample: $df = n - 1$)
- Approaches the normal distribution as $df$ increases
:::
:::

::: {.column width="60%"}
```{r}
#| echo: false


x <- seq(-4, 4, length.out = 200)

df_plot <- tibble(
  x = x,
  `Standard Normal` = dnorm(x),
  `t (df = 5)` = dt(x, df = 5),
  `t (df = 3)` = dt(x, df = 3)
) %>%
  pivot_longer(-x, names_to = "Distribution", values_to = "density")

ggplot(df_plot, aes(x = x, y = density, color = Distribution)) +
  geom_line(linewidth = 1.2) +
  labs(
    title = "Comparing t-distributions to the standard normal",
    subtitle = "Notice how t-distributions have heavier tails, especially with small df",
    x = "Value",
    y = "Density"
  ) +
  scale_y_continuous(expand = c(0.01, 0)) + 
  theme(legend.position = c(0.8, 0.8)) + 
  see::scale_color_okabeito()
```
:::
:::




## Why degrees of freedom = n - 1?

\

**Degrees of freedom** = number of independent pieces of information

\

When calculating the sample standard deviation $s$:

- We use $n$ observations
- But we first calculate $\bar{x}$ (which uses all $n$ values)
- This "uses up" one degree of freedom
- We're left with $n - 1$ independent pieces of information

\

**Intuition:** If you know the mean and $n-1$ values, the $n$th value is determined.

## Confidence interval with unknown σ

When $\sigma$ is **unknown** (which is almost always):

::: {.callout-tip icon="false"}
## CI for μ (with unknown σ)

$$\bar{x} \pm t^* \times \frac{s}{\sqrt{n}}$$

where:

- $\bar{x}$ = sample mean
- $t^*$ = critical value from t-distribution with $df = n - 1$
- $s$ = sample standard deviation
- $n$ = sample size
:::

\

## `qt()`: Finding the critical value in R

The `qt()` function finds critical values from the t-distribution:
```r
qt(p, df, lower.tail = TRUE)
```

\

**Parameters:**

- `p` = cumulative probability (e.g., 0.975 for 95% CI)
- `df` = degrees of freedom ($n - 1$)
- `lower.tail` = TRUE (default) gives left-tail probability

\

**Returns:** The **t-value** where $P(T \leq \text{value}) = p$

\

. . .


**Example: 95% CI with n = 50**
```{r}
# For 95% CI, we want 2.5% in each tail, so p = 0.975
# Degrees of freedom: df = n - 1 = 49
qt(p = 0.975, df = 49)
```

Compare to $z^* = 1.96$ from the normal distribution - the t-value is slightly larger!

## Example: CI with unknown σ

::: {.callout-note icon="false"}
## Example

A random sample of 50 adults has:

- Mean height: $\bar{x} = 66.1$ inches
- Sample SD: $s = 3.5$ inches

Find a 95% confidence interval for the population mean height.
:::

\

**Solution:**

::: columns
::: {.column width="50%"}
```{r}
xbar <- 66.1
s <- 3.5
n <- 50
df <- n - 1

# Critical value
t_star <- qt(0.975, df = df)
t_star
```
:::

::: {.column width="50%"}
```{r}
# Calculate SE (using s instead of σ)
SE <- s / sqrt(n)

# Calculate CI
lower_ci <- xbar - t_star * SE
upper_ci <- xbar + t_star * SE

c(lower_ci, upper_ci)
```

:::
:::

## Confidence interval (CI) for the mean $\mu$ ($z$ vs. $t$)

- In summary, we have two cases that lead to different ways to calculate the confidence interval

:::: {.columns}
::: {.column width="48%"}
::: {.callout-warning icon="false"}
### Case 1: We know the population standard deviation

$$\overline{x}\ \pm\ z^*\times \text{SE}$$

- with $\text{SE} = \frac{\sigma}{\sqrt{n}}$ and $\sigma$ is the population standard deviation

\

- For 95% CI, we use:
  - $z^* =$ `qnorm(p = 0.975)` $=1.96$
:::
:::

::: {.column width="48%"}
::: {.callout-tip icon="false"}
### Case 2: We **do not** know the population sd

$$\overline{x}\ \pm\ t^*\times \text{SE}$$

- with $\text{SE} = \frac{s}{\sqrt{n}}$ and $s$ is the sample standard deviation

\

- For 95% CI, we use:
  - $t^* =$ `qt(p = 0.975, df = n-1)`
:::
:::
::::


## Comparing z-based vs. t-based CIs

For our example ($n = 50$):

:::: {.columns}
::: {.column width="50%"}
::: {.callout-warning icon="false"}
### Case 1: We know the population standard deviation

```{r}
# If we knew σ = 3.5 (z-based CI)
z_star <- qnorm(0.975)
SE <- (3.5 / sqrt(n))

ci_z <- xbar + c(-1, 1) * z_star * SE
ci_z
```

:::
:::

::: {.column width="50%"}
::: {.callout-tip icon="false"}
### Case 2: We **do not** know the population sd

```{r}
# Using s = 3.5 (t-based CI)
t_star <- qt(0.975, df = 49)
SE <- (s / sqrt(n))

ci_t <- xbar + c(-1, 1) * t_star * SE
ci_t
```
:::
:::
::::

\

**Notice:** The t-based CI is slightly wider (because $t^* > z^*$) - this reflects the extra uncertainty from estimating σ.


## When to use t vs. z?

::: {.callout-important icon="false"}
## Decision rule

**Use t-distribution when:**

- You don't know the population standard deviation $\sigma$
- You're using the sample standard deviation $s$
- **(This is almost always in practice!)**

**Use normal (z) distribution when:**

- You know the population standard deviation $\sigma$
- **(This is rare in real applications)**
:::

\

**Rule of thumb we'll use in this class:**

Always use the t-distribution unless explicitly told you know $\sigma$.

# Summary and Key Takeaways

## What you need to know: Sampling distributions

**Key concepts:**

1. **Sampling variability** is natural - different samples give different estimates
2. The **sampling distribution** describes how statistics vary across samples
3. **Standard error** (SE) measures the variability of sample means: $SE = \frac{\sigma}{\sqrt{n}}$
4. The **Central Limit Theorem** says that for $n \geq 30$, sample means follow approximately normal (often for $n \geq 30$, depending on skew/outliers)

\

**In plain language:**

If we repeatedly sample from a population and calculate the mean each time, those means will form a normal distribution centered at the true population mean, with spread determined by the standard error.

## What you need to know: Confidence intervals

**Key concepts:**

1. A **confidence interval** gives a range of plausible values for a parameter
2. **95% confidence** means that 95% of such intervals would contain the true parameter
3. Use the **t-distribution** when $\sigma$ is unknown (almost always)
4. General form: $\bar{x} \pm t^* \times \frac{s}{\sqrt{n}}$

\

**Critical R functions:**

```{r}
#| eval: false

# Finding critical values
qt(0.975, df = n - 1)  # For 95% CI

# Or for different confidence levels
qt(0.95, df = n - 1)   # For 90% CI
qt(0.995, df = n - 1)  # For 99% CI
```

## Common mistakes to avoid

::: {.callout-warning icon="false"}
## Watch out for these!

1. **Confusing the three distributions**
   - Population distribution ≠ sample distribution ≠ sampling distribution

2. **Misinterpreting confidence intervals**
   - Not "95% chance μ is in the interval"
   - Rather "95% of such intervals contain μ"

3. **Using z when you should use t**
   - If you calculated $s$ from your data, use t!

4. **Forgetting the assumptions**
   - CLT needs $n \geq 30$ (or normal population)
   - Or: smaller $n$ is okay if data is approximately symmetric
:::

## Key formulas for reference

You don't need to memorize these, but understand what they mean:

\

**Standard Error:**
$$SE = \frac{\sigma}{\sqrt{n}} \quad \text{or} \quad SE = \frac{s}{\sqrt{n}}$$

\

**Confidence Interval (t-based):**
$$\bar{x} \pm t^* \times \frac{s}{\sqrt{n}}$$

where $t^*$ comes from a t-distribution with $df = n - 1$

\

**Confidence Interval (z-based, if σ known):**
$$\bar{x} \pm z^* \times \frac{\sigma}{\sqrt{n}}$$

## The inference pipeline

Let's tie everything together:

\

::: {.callout-important icon="false"}
## From population to inference

**Population** → **Sample** → **Statistic** → **Sampling distribution** → **Confidence interval**
:::

\

**The process:**

1. **Population** with unknown parameter $\mu$
2. Take a random **sample** of size $n$
3. Calculate a **statistic** (e.g., $\bar{x}$)
4. Use the **sampling distribution** to understand variability
5. Construct a **confidence interval** to quantify uncertainty

\

**Key insights:** We never observe the population directly, so we use sampling distributions to quantify uncertainty and construct plausible ranges for parameters.

## Looking ahead

**Next time:**

- More practice with confidence intervals
- Introduction to hypothesis testing
- The logic of statistical inference

\

**For now:**

- Practice calculating CIs with different confidence levels
- Get comfortable with the t-distribution in R
- Work on understanding (not just calculating) what CIs mean

\

::: {.callout-tip icon="false"}
## Remember

Statistical inference is about quantifying uncertainty. Confidence intervals give us a principled way to say "we don't know exactly, but here's a plausible range."
:::



<!-- NOTES for next year: -->

<!-- Looking through your slides, I can identify several areas where there's redundancy that likely made the lecture feel repetitive. Here's my advice organized by topic: -->

<!-- ## Major Redundancies to Address -->

<!-- ### 1. **The "Why is this useful?" slide (after CLT)** -->
<!-- **Problem:** This slide just restates what you already said about the CLT. The three bullets are: -->
<!-- - "We only have one sample" (already obvious from the entire sampling discussion) -->
<!-- - "CLT lets us use normal distribution" (literally what you just explained for 3-4 slides) -->
<!-- - "Works for discrete distributions too" (minor technical point that interrupts flow) -->

<!-- **Suggestion:** Delete this slide entirely. The "Looking ahead" callout box on the previous slide already bridges to inference better. Or, if you want to keep something, replace it with a concrete example showing HOW you'd use CLT in practice. -->

<!-- ### 2. **CI Interpretation is covered in 5 separate slides** -->
<!-- You have: -->
<!-- - "How do we interpret confidence intervals?"  -->
<!-- - "Interpreting confidence intervals: What they mean" -->
<!-- - "Interpreting confidence intervals: What they DON'T mean" -->
<!-- - "What a 95% confidence interval actually means" -->
<!-- - "Confidence intervals are about procedures" -->

<!-- **Problem:** While CI interpretation is important and students struggle with it, five slides pounds the same point. The "correct/wrong" format becomes repetitive. -->

<!-- **Suggestion for next year:** -->
<!-- - Combine "What they mean" and "What a 95% CI actually means" into ONE slide -->
<!-- - Keep "What they DON'T mean" as a separate slide (the misconceptions are valuable) -->
<!-- - Delete "Confidence intervals are about procedures" - it repeats the same "long-run coverage" concept that's already in the other interpretation slides -->
<!-- - Consider making the "Visualizing confidence intervals" activity more interactive to build intuition INSTEAD of so much text -->

<!-- ### 3. **"Why does sample size matter?" slide** -->
<!-- **Problem:** You show sample size effects visually, but: -->
<!-- - The SE formula already tells them this (larger n → smaller SE) -->
<!-- - The CLT slides with different n values already showed this -->
<!-- - It's somewhat redundant with the "What makes a CI wide or narrow?" slide -->

<!-- **Suggestion:** Either delete this OR move it earlier to RIGHT AFTER you introduce SE (before CLT), as a visual proof that the formula makes sense. -->

<!-- ### 4. **Three Distributions Explained Multiple Times** -->
<!-- You explain population vs. sample vs. sampling distribution in: -->
<!-- - "Three distributions to keep straight" (the good one with three columns) -->
<!-- - "Visual: Three distributions" (the plot version) -->
<!-- - Then references throughout -->

<!-- **Suggestion:** Keep both of these BUT add a comment note to yourself: "Students will still confuse these - plan to revisit with examples later, but don't re-explain the definitions multiple times during lecture." -->

<!-- ### 5. **t vs. z decision rule repeated three times** -->
<!-- - "Confidence interval for the mean μ (z vs. t)" slide -->
<!-- - "When to use t vs. z?" slide   -->
<!-- - "Common mistakes to avoid" mentions it again -->

<!-- **Suggestion:** The comparison slide is good. The "When to use t vs. z?" slide is also good. Delete the third mention in common mistakes OR just make it a bullet that says "Don't use z when you should use t" without re-explaining the rule. -->

<!-- ## Other Streamlining Suggestions -->

<!-- ### 6. **Standard Error coverage** -->
<!-- You have: -->
<!-- - "Why does the standard error exist?"  -->
<!-- - "Standard error: A special name" -->
<!-- - "When to report SE vs. SD" -->

<!-- **Suggestion:** The three slides are fine but feel a bit drawn out. Consider: -->
<!-- - Combine "Why does SE exist?" and "Standard error: A special name" into one slide -->
<!-- - Keep "When to report SE vs. SD" separate as it's practical advice -->

<!-- ### 7. **The summary section** -->
<!-- Your "What you need to know" slides largely repeat what was already covered. -->

<!-- **Suggestion:** Make these more action-oriented: -->
<!-- - Instead of "Key concepts: 1. Sampling variability is natural..." (which you already taught) -->
<!-- - Try "For homework/exams you need to: 1. Calculate SE using s/√n, 2. Find t* using qt(), 3. Construct CI using x̄ ± t* × SE" -->
<!-- - This feels more helpful as a reference than restating concepts -->

<!-- ## Structural Suggestions -->

<!-- **Consider this flow for next year:** -->
<!-- 1. Sampling fundamentals (keep as is) -->
<!-- 2. Introduce sampling distribution and SE together (combine some slides) -->
<!-- 3. Show why sample size matters (visual) RIGHT AFTER introducing SE -->
<!-- 4. CLT (keep as is, delete "why is this useful") -->
<!-- 5. Jump straight to "From estimation to inference"  -->
<!-- 6. One CI interpretation slide (the good one), one misconceptions slide -->
<!-- 7. Mechanics of CI calculation (keep as is) -->
<!-- 8. t-distribution (keep as is) -->
<!-- 9. Streamlined summary that's more practical/action-oriented -->

<!-- ## What to Keep -->

<!-- Don't change these - they worked well: -->
<!-- - The Allison Horst artwork opener -->
<!-- - The visual demonstrations (population vs. sample, sampling variability) -->
<!-- - The CLT demonstration with skewed data -->
<!-- - The 100 CIs visual -->
<!-- - The t-distribution comparison plot -->
<!-- - All the R code examples -->

<!-- ## Comments to Add in Slides -->

<!-- Here are specific comments you could add for next year: -->

<!-- ``` -->
<!-- <!-- NEXT YEAR: Delete "Why is this useful?" slide - redundant with CLT explanation --> -->

<!-- <!-- NEXT YEAR: Combine CI interpretation slides - currently have 5 slides saying the same thing --> -->

<!-- <!-- NEXT YEAR: Move "Why sample size matters" visual to right after SE introduction, or delete --> -->

<!-- <!-- NEXT YEAR: "When to use t vs z" - already covered in comparison slide, maybe delete this one --> -->

<!-- <!-- NEXT YEAR: Streamline summary - make it more "here's what you DO" rather than restating concepts --> -->
<!-- ``` -->

<!-- The lecture has great content, but it seems like you're being very thorough at the expense of pace and punch. Students probably felt like "okay, I get it" around the third time hearing about CI interpretation. Trust that they got it the first time, and move on! -->