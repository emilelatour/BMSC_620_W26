---
title: "Response to Muddy Points"
subtitle: "Lecture: Proportions and Inference (Week 8, Day 1)"
author: "Emile"
date: "2026-02-23"
date-format: long
format:
  html:
    toc: true
    toc-depth: 3
    toc-location: right
    html-math-method: mathjax
    embed-resources: true
execute:
  echo: true
  warning: false
  message: false
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(knitr)

library(lamisc)
library(laviz)

theme_set(laviz::theme_minimal_white(grid = "none", axis = FALSE, base_size = 14))
```

# Overview

Thank you everyone for the thoughtful survey responses! Seeing ~93% of you rate the pace as "about right" is really encouraging, and the feedback on the aspirin example and the R workflow discussion was great to hear.

::: {.callout-note}
You do not need to read every section below. Each heading addresses a specific muddy point raised in class. Feel free to focus on the ones most relevant to you.
:::

---

# Interpreting the odds ratio: "0.55 times the odds"

> *"The odds of a heart attack among those taking aspirin are 0.55 times the odds among those taking a placebo. I'm just having trouble wrapping my head around this wording --- in this case, could we consider the placebo odds to be 1? Since the formula is (1 - 0.55) to calculate the reduction of risk?"*

This is a really common point of confusion, and it is worth unpacking carefully.

**What does an OR of 0.55 mean?**

An odds ratio of 0.55 means the odds of a heart attack in the aspirin group are 55% *of* the odds in the placebo group. You can think of the placebo group as the reference --- its odds ratio is, by definition, 1.0. The aspirin group's OR of 0.55 is measured relative to that reference.

So yes, in a sense you are right: the placebo group plays the role of "1" in the comparison. But it is important to be precise here --- we are not saying the placebo odds *equal* 1 (that would require specific numerical values). We are saying the placebo group is the **reference category**, and an OR of 0.55 means the aspirin group's odds are 55% as large.

**What about (1 − 0.55) = 0.45?**

This quantity --- 1 minus the OR --- is sometimes described informally as the "percent reduction in odds." An OR of 0.55 would correspond to a 45% reduction in the odds of a heart attack. This is a reasonable way to communicate the result to a general audience.

However, be careful: this is **not** the same as the relative risk reduction or the absolute risk reduction. The odds ratio and the relative risk are different measures, and the distinction matters (see the next section). In particular, for rare outcomes, OR ≈ RR, but when events are more common the OR will exaggerate the relative risk.

**A quick numeric example to anchor this:**

Suppose in the placebo group, 10 out of 100 people had a heart attack. The odds in the placebo group are $10/90 \approx 0.111$.

If the OR for aspirin is 0.55, the odds in the aspirin group are $0.55 \times 0.111 = 0.061$, which corresponds to roughly $6/98$, or about a 6% event rate.

The **relative risk** (RR) would be $6\% / 10\% = 0.60$ --- close to, but not the same as, the OR of 0.55. For rare outcomes the two converge; for common ones, they diverge.

---

# Absolute difference vs. relative risk vs. odds ratio --- when do you use which?

> *"Absolute difference vs relative risk vs odds and interpreting them"*

This is one of the most important practical questions in biostatistics. Here is the core summary:

| Measure | What it captures | Scale | Interpretation |
|---|---|---|---|
| **Risk Difference (RD)** | Absolute gap in proportions | Additive | "X more events per 100 people" |
| **Relative Risk (RR)** | Ratio of proportions | Multiplicative | "X times as likely" |
| **Odds Ratio (OR)** | Ratio of odds | Multiplicative | "X times the odds" |

**Risk Difference** is the most clinically intuitive. An RD of 0.04 means the treatment group had 4 more (or fewer) events per 100 people. This directly tells you the **number needed to treat** (NNT = 1 / |RD|). It answers the question: "If I treat 100 patients, how many will benefit?"

**Relative Risk** is easier to compare across studies with different baseline rates. An RR of 0.55 means the treatment group had 55% of the event rate of the control group --- a 45% relative reduction. But an RR of 0.55 means something very different if the baseline rate is 50% (treatment rate = 27.5%) versus 1% (treatment rate = 0.55%). The absolute impact depends on the baseline.

**Odds Ratio** is commonly reported because it arises naturally from logistic regression and from case-control study designs (where RR cannot be directly estimated). It approximates the RR when outcomes are rare, but diverges from it as events become more common. ORs are less intuitively interpretable and tend to make effects sound larger than they are when events are common.

**Which to use?** In clinical trials, risk differences and relative risks are usually preferred for communication. Odds ratios appear most often in observational studies and logistic regression. When reading a paper, always check the baseline event rate --- a "dramatic" relative risk reduction can be clinically unimportant if the absolute risk difference is tiny.

---

# Do you need to memorize the formulas?

> *"I suppose the formulas were a little much but I guess they need not be memorized"*

You are right --- you do not need to memorize them. The goal of this course is **concept before calculation**. What matters is that you understand:

- *Why* a formula takes the shape it does (what problem is it solving?)
- *How* to interpret the result
- *When* to use a particular test or estimate

R does the arithmetic. The formula serves as a window into the concept. When you see the formula for the standard error of a proportion, $\sqrt{\hat{p}(1-\hat{p})/n}$, the important insight is that uncertainty increases with variability in the proportion and decreases with larger $n$ --- you do not need to reproduce that algebra on a test.

Exams in this course will focus on interpretation, reasoning, and correctly applying methods --- not formula recall.

---

# Continuity correction: what counts as a "large" sample? And what are the names of these tests?

> *"I may have missed this, but for the continuity correction what is considered a large sample size? Is it also n > 30? Also, in practice how do you refer to tests used? Example: confidence interval with continuity correction or confidence interval with Wilson score interval. Are there other names for the tests we've learned?"*

Great set of questions. Let me take these in order.

**What counts as "large" for the continuity correction?**

The commonly cited rule is that the normal approximation is appropriate when both $n\hat{p} \geq 10$ and $n(1 - \hat{p}) \geq 10$ --- that is, you expect at least 10 successes and at least 10 failures. The "$n > 30$" rule you have heard before applies to means (via the Central Limit Theorem) and is not the right criterion here. For proportions, the key is having *enough events in both categories*, not just a large overall $n$.

When those conditions are not met, the exact binomial test (via `binom.test()`) is preferred.

**Names of confidence interval methods for proportions:**

There are several methods for constructing a confidence interval for a single proportion, and they do have names:

| Method | How it works | When used |
|---|---|---|
| **Wald interval** | Uses $\hat{p} \pm z \cdot SE$ directly | Large samples; can perform poorly near 0 or 1 |
| **Wald with continuity correction** | Adds a small correction to improve normal approximation | Large samples, slight improvement over Wald |
| **Wilson score interval** | Based on inverting the score test | Works well even for small $n$ or extreme $p$ |
| **Clopper-Pearson ("exact") interval** | Based directly on the binomial distribution | Conservative; use for small $n$ |

In practice, the **Wilson score interval** is widely considered the best default for a single proportion --- it outperforms the Wald interval across a wide range of sample sizes and true proportions. R's `prop.test()` uses the Wilson interval with continuity correction by default.

**How do statisticians refer to tests in practice?**

In a methods section, you would typically write something like: *"A one-sample proportion test was used to compare the observed proportion to the null value of 0.5 (Wilson score interval with continuity correction)."* For two-sample comparisons: *"A two-sample test of proportions was conducted using `prop.test()` in R."* The exact naming convention varies by journal and audience --- the key is being specific enough that someone could reproduce your analysis.

---

# What does the continuity correction actually do?

> *"What does continuity correction actually do to the data?"*

The continuity correction does not change your data at all --- it adjusts the **calculation** to improve the approximation.

Here is the core idea. A proportion is inherently discrete: with $n = 20$ people, you can observe 0, 1, 2, ..., 20 successes. But when we use the normal distribution to approximate probabilities, we are using a continuous curve to approximate a staircase. The mismatch can cause meaningful errors, especially in small samples.

The continuity correction adds or subtracts $\frac{1}{2n}$ to account for this --- it "smears" each discrete value to cover the interval between adjacent values, which makes the normal approximation more accurate.

The practical effect: the corrected confidence interval is **slightly wider** than the uncorrected Wald interval, which is actually a good thing --- it makes the interval more accurate in small-to-moderate samples. The $p$-value from a corrected test is also slightly larger (more conservative), which reduces the chance of a spurious rejection.

In large samples, the correction makes very little difference. In small samples, it can matter meaningfully.

---

# Percent vs. percentage points --- what is the difference?

> *"I'm not super clear what the difference is between reporting percent and percentage points."*

This is an important distinction that trips up even experienced writers.

**Percentage points** describe an **absolute difference** between two proportions. If the event rate in one group is 40% and in another is 25%, the difference is **15 percentage points**.

**Percent** (or "percent change") describes a **relative difference**. A change from 25% to 40% is a 60% increase (because $\frac{40 - 25}{25} \times 100 = 60\%$).

These two framings of the same change produce very different numbers, which is why the distinction matters:

| Framing | Value | What it means |
|---|---|---|
| Absolute change | 15 percentage points | The raw gap between the two rates |
| Relative change | 60% increase | How large the change is relative to the starting value |

A classic example of why this matters: a drug that reduces the risk of a disease from 2% to 1% can be described as a "1 percentage point reduction" (modest-sounding) or a "50% relative risk reduction" (dramatic-sounding). Both are technically correct, which is exactly why you need to know the difference and report the baseline rate alongside any relative measure.

In this course, when you report a risk difference, always use "percentage points." Reserve "percent" for relative comparisons (RR or OR).

---

# What does "association" mean technically?

> *"I was a little unclear on what 'association' means on a technical level"*

In statistics, two variables are **associated** if knowing the value of one variable tells you something about the likely value of the other. More precisely, two variables are associated if their **joint distribution is not equal to the product of their marginal distributions** --- that is, they are not independent.

For categorical variables (like those in a 2×2 table), we say there is an association if the proportion of outcomes differs across groups. In the aspirin example: if the rate of heart attacks among aspirin users differs from the rate among placebo users, then group membership and heart attack status are associated.

Note that **association does not imply causation**. Two variables can be strongly associated because one causes the other, because both are caused by a third variable (a confounder), or because of chance. The word "association" is deliberately neutral on this point.

In practical terms:

- A chi-squared test tests whether there is an association between two categorical variables
- An odds ratio or relative risk *quantifies* the strength and direction of that association
- "No association" corresponds to OR = 1 or RR = 1, or a risk difference of 0

---

# `prop.test()` vs. `binom.test()` --- what is actually different?

> *"I'm still not clear on when you would want to use prop test vs binom except for small sample sizes and what it is that is actually different between them? Is the prop test not measuring an exact proportion, like the binomial?"*

Both tests evaluate a hypothesis about a proportion, but they use different mathematical approaches:

**`binom.test()`** uses the **exact binomial distribution**. It calculates the probability of observing your data (or something more extreme) directly from the binomial probability formula, without any approximation. This is exact in the sense that no distributional assumption beyond "the data are binomial" is required.

**`prop.test()`** uses a **normal approximation** (the score test). It approximates the binomial distribution with a normal distribution, which is appropriate when the sample is large enough ($n\hat{p} \geq 10$ and $n(1 - \hat{p}) \geq 10$). The confidence interval from `prop.test()` is the Wilson score interval (with continuity correction by default).

So to directly answer your question: `prop.test()` *is* measuring the same underlying proportion --- the difference is in how the probability calculation is done, not in what is being estimated.

**When to use which:**

| Situation | Preferred test |
|---|---|
| Small $n$ or extreme $\hat{p}$ (near 0 or 1) | `binom.test()` |
| Large $n$ with $n\hat{p} \geq 10$ and $n(1-\hat{p}) \geq 10$ | `prop.test()` |
| Two-sample comparison of proportions | `prop.test()` (binom.test is one-sample only) |
| Chi-squared test context | `prop.test()` |

In practice, `binom.test()` is generally safe in any situation --- being exact is never a liability. The normal approximation in `prop.test()` is a computational convenience that works well for large samples. For two-group comparisons, `prop.test()` is your go-to.

---

# Can you change your hypothesis after seeing the data?

> *"Is there a time where it is ok to change your hypothesis after you've seen the data? For example if you are testing a cancer drug and see that it increases survival in men but when the experiment is repeated in women it is actually worse than the no treatment control. Now you'd want to know if the treatment is significant worse than no treatment. This would have clinical relevance because this could make cancer worse in women and should not be prescribed."*

This is a genuinely important question, and your instinct about the clinical stakes is exactly right. Let me address both the statistical and practical sides.

**The statistical problem with changing hypotheses post-hoc**

The $p$-value framework is built on the assumption that the hypothesis was specified *before* looking at the data. When you change a hypothesis after seeing the data --- even with good intentions --- you inflate the Type I error rate. If you look at enough subgroups or outcomes, you will find something that looks significant by chance alone. This is sometimes called **HARKing** (Hypothesizing After Results are Known) and is a major contributor to the replication crisis in science.

In your example: if the original trial was powered and designed to test overall survival, the observation that women appear to do worse is an **exploratory finding**, not a confirmatory one. Running a $p$-value on it as though it were pre-specified overstates the evidence.

**What you should do instead**

This is precisely the situation that motivates **pre-specified subgroup analyses** and **adaptive trial designs**. Modern clinical trials increasingly pre-register their analysis plan --- including any planned subgroup analyses --- before data collection begins. If sex-based differences in treatment effect were anticipated, the trial would be designed and powered to test them formally.

When an unexpected finding like this emerges, the appropriate response is to:

1. Report the observation transparently, clearly labeling it as exploratory/hypothesis-generating
2. Quantify it with an effect size and confidence interval (not just a $p$-value)
3. Use it to motivate a **new, properly powered, pre-specified study**

**The clinical reality**

Your clinical instinct is sound: if a treatment is harming women, that is urgent information regardless of the $p$-value. Regulatory agencies (like the FDA) take exactly this approach --- unexpected safety signals trigger further investigation even if the $p$-value does not cross a formal threshold. The statistical caution about post-hoc hypotheses is not an argument for ignoring alarming data; it is an argument for being appropriately humble about the strength of evidence and investing in the confirmatory study needed to act decisively.

---

# Pace and general comments

The pace feedback was excellent this week (93% "about right"). The feedback about the aspirin example is encouraging --- connecting the statistical theory to a real clinical result with all three measures (RD, RR, OR) is exactly the kind of integration I want you to be able to do. Several of you also mentioned appreciating the time spent on R workflow details --- that is good to hear, and I will keep building on that.

---

# Summary

- **OR interpretation:** The reference group has OR = 1 by definition; an OR of 0.55 means the treatment group's odds are 55% of the reference group's odds, or a 45% reduction in odds
- **RD vs RR vs OR:** Risk difference is most clinically intuitive (enables NNT); relative risk is useful for comparing across studies; odds ratio arises from logistic regression and case-control designs
- **Formulas:** Understand the concept behind them; you do not need to memorize them
- **Large sample for proportions:** $n\hat{p} \geq 10$ and $n(1-\hat{p}) \geq 10$ --- not simply $n > 30$
- **CI methods:** Wilson score interval is the best default for a single proportion; Wald + continuity correction is also acceptable for large samples; exact (Clopper-Pearson) for small $n$
- **Continuity correction:** Adjusts the calculation to improve the normal approximation for discrete data; slightly widens the interval; does not change the data
- **Percentage points vs percent:** Percentage points = absolute difference; percent = relative change --- always report the baseline rate alongside any relative measure
- **Association:** Two variables are associated if knowing one tells you something about the other; association ≠ causation
- **`prop.test()` vs `binom.test()`:** Both estimate the same proportion; `binom.test()` uses exact binomial probabilities (good for small $n$); `prop.test()` uses the normal approximation (good for large $n$, required for two-sample comparisons)
- **Post-hoc hypotheses:** Changing hypotheses after seeing data inflates Type I error; unexpected findings should be labeled exploratory and used to motivate new, pre-specified studies
