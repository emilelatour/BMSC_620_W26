---
title: "Response to Muddy Points"
subtitle: "Lecture: Chi-Squared Tests and Fisher's Exact Test + R Module 1 (Week 8, Day 2)"
author: "Emile"
date: "2026-02-25"
date-format: long
format:
  html:
    toc: true
    toc-depth: 3
    toc-location: right
    html-math-method: mathjax
    embed-resources: true
    self-contained: true
execute:
  echo: true
  warning: false
  message: false
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(knitr)

library(lamisc)
library(laviz)

theme_set(laviz::theme_minimal_white(grid = "none", axis = FALSE, base_size = 14))
```

# Overview

Thank you again for the great feedback! The pace continues to land well — about 96% of you rated it "about right," and several of you called out the chi-squared material and the R code-along as highlights. That's exactly the balance I'm aiming for.

::: {.callout-note}
You do not need to read every section below. Each heading addresses a specific muddy point raised in class. Feel free to focus on the ones most relevant to you.
:::

---

# Can Fisher's exact test be used on a `tabyl` table?

> *"Can Fischer's exact test be used on a tibble table? When to use chi squared vs Fischer."*

Yes! In class we showed that `fisher.test()` works with several different input types. Here is a quick summary:

**Base R `table()` — always works:**

```{r}
#| eval: false
dep_pa_tab <- table(data$PA, data$Depression)
fisher.test(dep_pa_tab)
```

**`janitor::tabyl()` — also works directly:**

```{r}
#| eval: false
dep_pa_tab_janitor <- data %>%
  tabyl(PA, Depression)

fisher.test(dep_pa_tab_janitor)
```

**A `matrix` you build by hand — also works:**

```{r}
#| eval: false
my_mat <- matrix(c(5, 8, 1, 11), nrow = 2, byrow = TRUE)
fisher.test(my_mat)
```

Under the hood, `fisher.test()` converts whatever you give it into a matrix of counts. So whether you pass it a `table`, a `tabyl`, or a raw `matrix`, the result is the same. The same applies to `chisq.test()`.

**When to use which test** — the decision rule from the slides:

1. Build your contingency table
2. Check expected counts (using `chisq.test(your_table)$expected`)
3. If expected counts are large enough → `chisq.test()`
4. If expected counts are too small → `fisher.test()`

The specific thresholds:

- **2×2 tables:** Use Fisher's if any expected count < 10
- **Larger tables:** Use Fisher's if more than 20% of expected counts < 5, or if any expected count < 1

When in doubt, Fisher's exact test is always valid — it just requires more computing time for very large tables.

---

# Two-proportion test vs. chi-squared for a 2×2 table

> *"Could you go over when you'd use a two way proportion test versus a chi squared for a 2×2 contingency table? I was reading a source that said those two tests would come up with the same p value. Also what does the df = 1 chi-square distribution look like?"*

Great question — and your source is correct. For a **2×2 table**, the two-proportion $z$-test (from `prop.test()`) and the chi-squared test (from `chisq.test()`) are **mathematically equivalent**. Specifically, the chi-squared statistic is exactly the square of the $z$-statistic: $\chi^2 = z^2$ (when no continuity correction is applied). A chi-squared distribution with $df = 1$ is the same as the distribution of a squared standard normal. So the $p$-values will be identical.

Why do both exist? The two-proportion test is designed for the special case of comparing exactly two groups on a binary outcome. It gives you a familiar framework: a null hypothesis stated with symbols ($p_1 - p_2 = 0$), a $z$-statistic, and a confidence interval for the difference in proportions. The chi-squared test is the more general tool — it extends naturally to tables with more than two rows or columns, where a simple difference in proportions no longer captures the full picture.

**In practice:**

- If you have a **2×2 table** and you want a confidence interval for the difference in proportions → `prop.test()` is more informative
- If you have a **2×2 table** and you only need a $p$-value for association → either test works; they give the same answer
- If you have a **larger table** (3×2, 2×4, etc.) → `chisq.test()` is the appropriate tool

**What does the $df = 1$ chi-squared distribution look like?**

It is the orange curve from the plot in the slides — the one with the dramatically different shape. Here is why it looks so different from the others:

```{r}
#| label: fig-chisq-df1
#| echo: false
#| fig-height: 4.5
#| fig-width: 7
#| fig-cap: "Chi-squared distributions for df = 1 through df = 10"

tibble(x = seq(0.01, 20, length.out = 500)) %>%
  mutate(
    df1  = dchisq(x, df = 1),
    df2  = dchisq(x, df = 2),
    df4  = dchisq(x, df = 4),
    df6  = dchisq(x, df = 6),
    df10 = dchisq(x, df = 10)
  ) %>%
  pivot_longer(-x, names_to = "df", values_to = "density") %>%
  mutate(df = factor(df,
                     levels = c("df1", "df2", "df4", "df6", "df10"),
                     labels = c("df = 1", "df = 2", "df = 4", "df = 6", "df = 10"))) %>%
  ggplot(aes(x = x, y = density, color = df)) +
  geom_line(linewidth = 1.2) +
  scale_color_manual(values = c("#D55E00", "#E69F00", "#56B4E9", "#009E73", "#0072B2")) +
  labs(x = expression(chi^2), y = "Density", color = "Degrees of\nFreedom") +
  coord_cartesian(ylim = c(0, 0.5)) +
  theme(legend.position = "right")
```

The $df = 1$ curve shoots up near zero and drops off steeply — even more extreme than $df = 2$. This makes sense: with only 1 degree of freedom (a 2×2 table), most of the probability mass is concentrated near zero, meaning that under $H_0$, you expect very small $\chi^2$ values. Even a moderately large $\chi^2$ value is unusual, which is why the test can be quite sensitive with just a single degree of freedom.

---

# Why does the $df = 2$ curve look so different from the others?

> *"I don't understand graphing the degrees of freedom, why do the lower values (orange, df = 2) had the opposite curve as the rest of the numbers"*

This is a great observation, and it catches something real about how the chi-squared distribution changes shape.

For **$df = 1$ and $df = 2$**, the density is highest at (or very near) zero and decreases monotonically — the curve slopes *downward* from left to right. There is no "hump."

For **$df \geq 3$**, the distribution develops a mode (a peak) away from zero. As $df$ increases, that peak shifts further to the right and the distribution becomes more symmetric, eventually looking roughly bell-shaped.

**Why?** The chi-squared distribution with $df = k$ is the distribution of a sum of $k$ independent squared standard normal variables. When $k$ is small (1 or 2), the squared values are mostly small and clustered near zero — so the density piles up on the left. As you add more squared terms ($k = 4, 6, 10, \ldots$), the sum is increasingly unlikely to be near zero (because you are adding multiple non-negative quantities), which pushes the peak to the right and fills out the body of the distribution.

The conceptual takeaway for this course: degrees of freedom control the shape of the reference distribution. More $df$ means the distribution's center shifts rightward, so a given $\chi^2$ value is less extreme (less surprising) with higher $df$. You don't need to memorize the mathematical details — just understand that $df$ determines what counts as "unusual" under $H_0$.

---

# The R material at the end (R Module 1)

> *"The R stuff at the end about adding the code in your analysis"*

I know we moved through the R Module quickly at the end of class. The core idea is **reproducible reporting**: instead of manually typing numbers into your prose, you let R compute them and insert them automatically.

The three key skills from R Module 1:

**1. YAML header** — The block at the very top of your `.qmd` file between the `---` marks. This controls document-level settings. The most useful options for homework:

```yaml
---
title: "My Report"
author: "Your Name"
date: today
format:
  html:
    toc: true
    self-contained: true
execute:
  echo: true
  warning: false
  message: false
---
```

Setting `warning: false` and `message: false` at the document level means you don't have to repeat those options in every single chunk.

**2. Chunk options** — The `#|` lines at the top of each code chunk. The most important ones:

- `echo: true/false` — show the code?
- `eval: true/false` — run the code?
- `include: false` — run the code but hide everything (code and output) from the final document. This is what you use for your setup chunk.

**3. Inline R** — This is the key to reproducible numbers. Instead of writing "The mean BMI was 27.3," you write:

````{markdown}
The mean BMI was `r round(mean(data$BMI, na.rm = TRUE), 1)`.
````

Now if the data changes, the number updates automatically when you re-render. The best practice is to compute values in a hidden chunk first, then reference the saved objects inline — this keeps your prose clean and readable.

We will keep building on these skills throughout the rest of the course, and you will see them in action in future homework assignments.

---

# A note on global chunk options (HW 3 feedback)

> *"Review on Global chunk options is set to not echo code chunks... this was feedback from homework but on my side it rendered successfully so I'm not sure what to do differently in the future."*

**This one was on me, not you.** When I build the homework templates, I sometimes set a global chunk option that suppresses code output so I can render and check the document cleanly on my end. Before distributing the homework, I switch it back. For HW 3, I missed that step — so the template you received had a setting that hid code chunks in the rendered output even though your code ran fine.

This has been fixed for all homework assignments after HW 3, and **nobody was penalized for it.** You don't need to change anything about your workflow going forward. If your document rendered successfully on your end, you were doing everything right.

---

# Pace and general comments

The pace continues to look great — 96% "about right" with only one response at "slightly too fast" and none at "too slow." Several of you highlighted the chi-squared material, Fisher's test interpretation, and the R code-along as particularly clear, which is encouraging.

I'll continue to balance conceptual explanation with hands-on R practice. If the R Module material felt rushed, that's fair — we squeezed it into the remaining class time. The R Module slides and references are posted on the course website for you to review at your own pace.

---

# Summary

- **Fisher's test on a `tabyl`:** Yes — `fisher.test()` accepts `table`, `tabyl`, or `matrix` objects interchangeably
- **Two-proportion test vs. chi-squared for 2×2:** Mathematically equivalent ($\chi^2 = z^2$); use `prop.test()` when you want a CI for the difference, `chisq.test()` for the general association test
- **$df = 1$ chi-squared distribution:** Steeply decreasing from zero with no peak — most probability mass near zero
- **Why low-$df$ curves look different:** With $df \leq 2$, the distribution decreases monotonically from zero; with $df \geq 3$, a peak emerges and shifts right as $df$ increases
- **R Module 1 essentials:** YAML controls document settings; chunk options (`echo`, `eval`, `include`) control what appears in the output; inline R eliminates manual number-typing
- **Global chunk options (HW 3 feedback):** That was an instructor-side error in the template — it has been fixed, and no one was penalized
